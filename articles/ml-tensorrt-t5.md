---
title: "Hugging face t5-base-japaneseã‚’Tensor-RTã§é«˜é€ŸåŒ–ã—ã¦ã¿ãŸ" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ˜¸" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["æ©Ÿæ¢°å­¦ç¿’", "python"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: True # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
---

# ã¯ã˜ã‚ã«

ã“ã‚“ã«ã¡ã¯ã€ã‚ã£ã—ãƒ¼ã§ã™ã€‚
ç”»åƒãƒ»è‡ªç„¶è¨€èªãƒ»éŸ³å£°ã«é–¢ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ç ”ç©¶é–‹ç™ºã‚„MLOpsã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ç‰¹ã«æœ€è¿‘ã¯ã€éŸ³å£°é–¢é€£ã®ç ”ç©¶é–‹ç™ºã«åŠ›ã‚’å…¥ã‚Œã¦ã„ã¾ã™ã€‚ã‚‚ã—ã€æ©Ÿæ¢°å­¦ç¿’ã«é–¢ã—ã¦ã€ã”ç›¸è«‡ãŒã‚ã‚Œã°ã€[@kwashizzz](https://twitter.com/kwashizzz)ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¾ã§DMã—ã¦ãã ã•ã„ï¼
ã“ã‚Œã¾ã§ã®ã€[æ©Ÿæ¢°å­¦ç¿’è¨˜äº‹ã®ã¾ã¨ã‚](https://zenn.dev/kwashizzz/articles/my-ml-articles-summary)ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€Hugging faceã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€æ—¥æœ¬èªã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸT5 ([sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese))ã‚’FineTuningã—ãŸãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’[Tensor-RT](https://github.com/NVIDIA/TensorRT)ã§é«˜é€ŸåŒ–ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚


ç’°å¢ƒæ§‹ç¯‰ã«é–¢ã—ã¦ã¯ã€[å…¬å¼ã®è¨˜äº‹](https://developer.nvidia.com/blog/optimizing-t5-and-gpt-2-for-real-time-inference-with-tensorrt/)ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã«é–¢ã—ã¦ã¯ã€ä¸‹éƒ¨ã«è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚
Tensor-RTã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«é–¢ã—ã¦ã¯ã€æœ€æ–°ã®releaseã§ã‚ã‚‹ã€ver 8.2ãŒã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å¤‰æ›å¾Œã®æ¨è«–ãŒä¸Šæ‰‹ãã§ããªã‹ã£ãŸãŸã‚ã€22/6/1æ™‚ç‚¹ã§æœ€æ–°ã®mainãƒ–ãƒ©ãƒ³ãƒã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

åŸºæœ¬çš„ã«ã¯ã€[å…¬å¼t5å¤‰æ›notebook](https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/notebooks/t5.ipynb)ã«å‰‡ã£ã¦ã€Tensor-RTã«å¤‰æ›ã—ã¦ã„ãã¾ã™ã€‚

# çµæœ

ã©ã®ãã‚‰ã„æ¨è«–é€Ÿåº¦ãŒé€Ÿããªã‚‹ã‹ã¨ã„ã†ã“ã¨ã§ã™ãŒã€Encoderã®å‡¦ç† + decoderã®å‡¦ç†ã‚’99å›ãƒ«ãƒ¼ãƒ—ã•ã›ãŸéš›ã®å‡¦ç†é€Ÿåº¦ã¯ã€ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

|||
|-|-|
|T5 hugging face generate | 1.225 sec |
| Tensor RT | 0.261 sec |

Tensor RTã®æ–¹ãŒã€ç´„ 5å€é€Ÿããªã£ã¦ã„ã¾ã™ã€‚


# è©°ã¾ã£ãŸéƒ¨åˆ†ã®è§£èª¬

[å…¬å¼t5å¤‰æ›notebook](https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/notebooks/t5.ipynb) ã®å®Ÿè£…ã«å¯¾ã—ã¦ã€ä¿®æ­£ã—ãŸéƒ¨åˆ†ã‚’è§£èª¬ã—ã¾ã™ã€‚

1. T5_VARIANT = 't5-small' ã¯ã€'t5-base' ãªã©å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

2. TensorRT/demo/HuggingFace/T5/T5ModelConfig.pyã®T5ModelTRConfigã‚’ãƒ¢ãƒ‡ãƒ«ã®configãƒ•ã‚¡ã‚¤ãƒ«ã«åˆã‚ã›ã‚‹ã€‚
  ä»Šå›ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®VOCAB SIZEãŒ32128ã‹ã‚‰ã€32000ã¸å¤‰æ›´ã—ãŸã€‚

3. fp16ã®è¨­å®š

fp16ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã€false

```python
metadata=NetworkMetadata(variant=variant, precision=Precision(fp16=False), other=T5Metadata(kv_cache=False))
```

4. Tensor-RTã¸ã®å¤‰æ›æ™‚ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’FineTuningã—ãŸãƒ¢ãƒ‡ãƒ«ã®config.jsonã«åˆã‚ã›ã‚‹ã€‚

ä»¥ä¸‹ã€[sonoisa](https://zenn.dev/sonoisa)æ§˜ã‚ˆã‚Šã€ã‚³ãƒ¡ãƒ³ãƒˆã„ãŸã ãã¾ã—ãŸã€‚
> sonoisa/t5-base-japaneseãƒ¢ãƒ‡ãƒ«ï¼ˆåŸè«–æ–‡ã®è¨­å®šã®T5ãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã‚ã‚Œã°ã€tfm_config.d_ff = 3072ã¨tfm_config.feed_forward_proj = "relu"ã§ã™ã­ã€‚
> T5 v1.1ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ãˆã°ã€megagonlabs/t5-base-japanese-webï¼‰ã§ã‚ã‚Œã°ã€tfm_config.d_ff = 2048ã¨tfm_config.feed_forward_proj = "gated-gelu"ã«ãªã‚Šã¾ã™ã€‚

```python
from T5.trt import T5TRTEncoder, T5TRTDecoder

tfm_config = T5Config(
    use_cache=True,
    num_layers=T5ModelTRTConfig.NUMBER_OF_LAYERS["t5-base"]
)
tfm_config.d_ff = 2048
tfm_config.vocab_size = 32000 # tokenizer.vocab_sizeã¯32100ã ãŒã€configã«ã¯ã€32000ã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚
tfm_config.num_heads = 12
tfm_config.d_model = 768
tfm_config.feed_forward_proj = "gated-gelu"
print(tfm_config)
```

5. æ¨è«–å‡¦ç†ã‚’åˆ¥é€”å®Ÿè£…

ä»¥ä¸‹ã€æ¨è«–å‡¦ç†ã§ã™ã€‚
é€”ä¸­ã«å‡ºã¦ãã‚‹ã‚¯ãƒ©ã‚¹ã¯ã€notebookã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚

```python
T5_MODEL_DIR = "..."
t5_model = T5ForConditionalGeneration.from_pretrained(T5_MODEL_DIR)
tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_DIR)
config = T5Config(T5_MODEL_DIR)

text = "ã‚ã„ã†ãˆãŠ"
device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
inputs = tokenizer(text, return_tensors="pt")
input_ids = inputs.input_ids.to(device)
attention_mask = inputs.attention_mask.to(device)

input_ids = input_ids.unsqueeze(0) # batch x seqlen
attention_mask = attention_mask.unsqueeze(0) # batch x -1

max_length = 512

decoder_input_ids = torch.as_tensor(
    [[tokenizer.pad_token_id]], dtype=torch.long
).to(device)

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®è¨ˆç®—
encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))

s = time.time()
for step in range(max_length):
    # ãƒ‡ã‚³ãƒ¼ãƒ€ã®è¨ˆç®—
    outputs = t5_trt_decoder(
                input_ids=decoder_input_ids,
                encoder_hidden_states=encoder_last_hidden_state,
                output_hidden_states=True,
                return_dict=True,
                use_cache=True
            )
    logit_for_next_step = outputs.logits[:, -1, :]
    _decoder_input_ids = torch.argmax(logit_for_next_step , dim=-1)
    if _decoder_input_ids[0].item() == tokenizer.eos_token_id:
        break
    decoder_input_ids = torch.cat([decoder_input_ids, _decoder_input_ids[:, None]], dim=-1) # past keyãŒä½¿ãˆãªã„
    
    print(tokenizer.decode(decoder_input_ids[0]))
    print(step, time.time() - s)
print(tokenizer.decode(_decoder_input_ids, skip_special_tokens=True)) # æœ€çµ‚çš„ãªãƒ†ã‚­ã‚¹ãƒˆ
```



# ç’°å¢ƒæ§‹ç¯‰

Tensor-RTã®ç’°å¢ƒæ§‹ç¯‰ã¯é¢å€’ãªã®ã§ã€å…¬å¼ã®Docker(ubuntu 20.04ã®versoin)ã‚’ä¿®æ­£ã—ãŸã‚‚ã®ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
å¾Œã»ã©è¨˜è¼‰ã™ã‚‹ã€requirements.txtã«åŠ ãˆã¦ã€pytorchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã—ã¦ã„ã¾ã™ãŒã€é©å®œä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

```dockerfile
ARG CUDA_VERSION=11.4.2
ARG OS_VERSION=20.04

FROM nvidia/cuda:${CUDA_VERSION}-cudnn8-devel-ubuntu${OS_VERSION}
LABEL maintainer="NVIDIA CORPORATION"

ENV TRT_VERSION 8.2.5.1
SHELL ["/bin/bash", "-c"]

RUN mkdir -p /workspace

# Required to build Ubuntu 20.04 without user prompts with DLFW container
ENV DEBIAN_FRONTEND=noninteractive

# Update CUDA signing key
RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub

# Install requried libraries
RUN apt-get update && apt-get install -y software-properties-common
RUN add-apt-repository ppa:ubuntu-toolchain-r/test
RUN apt-get update && apt-get install -y --no-install-recommends \
    libcurl4-openssl-dev \
    wget \
    zlib1g-dev \
    git \
    pkg-config \
    sudo \
    ssh \
    libssl-dev \
    pbzip2 \
    pv \
    bzip2 \
    unzip \
    devscripts \
    lintian \
    fakeroot \
    dh-make \
    build-essential

# Install python3
RUN apt-get install -y --no-install-recommends \
      python3 \
      python3-pip \
      python3-dev \
      python3-wheel &&\
    cd /usr/local/bin &&\
    ln -s /usr/bin/python3 python &&\
    ln -s /usr/bin/pip3 pip;

# Install TensorRT
RUN if [ "${CUDA_VERSION}" = "10.2" ] ; then \
    v="${TRT_VERSION%.*}-1+cuda${CUDA_VERSION}" &&\
    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub &&\
    apt-get update &&\
    sudo apt-get install libnvinfer8=${v} libnvonnxparsers8=${v} libnvparsers8=${v} libnvinfer-plugin8=${v} \
        libnvinfer-dev=${v} libnvonnxparsers-dev=${v} libnvparsers-dev=${v} libnvinfer-plugin-dev=${v} \
        python3-libnvinfer=${v}; \
else \
    v="${TRT_VERSION%.*}-1+cuda${CUDA_VERSION%.*}" &&\
    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub &&\
    apt-get update &&\
    sudo apt-get install libnvinfer8=${v} libnvonnxparsers8=${v} libnvparsers8=${v} libnvinfer-plugin8=${v} \
        libnvinfer-dev=${v} libnvonnxparsers-dev=${v} libnvparsers-dev=${v} libnvinfer-plugin-dev=${v} \
        python3-libnvinfer=${v}; \
fi

# Install PyPI packages
RUN pip3 install --upgrade pip
RUN pip3 install setuptools>=41.0.0
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install -r /tmp/requirements.txt

RUN pip3 install torch==1.11.0+cu113 torchaudio==0.11.0+cu113 torchvision==0.12.0+cu113 --extra-index-url https://download.pytorch.org/whl/cu113

RUN pip3 install jupyter jupyterlab
# Workaround to remove numpy installed with tensorflow
RUN pip3 install --upgrade numpy

# Install Cmake
RUN cd /tmp && \
    wget https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4-Linux-x86_64.sh && \
    chmod +x cmake-3.14.4-Linux-x86_64.sh && \
    ./cmake-3.14.4-Linux-x86_64.sh --prefix=/usr/local --exclude-subdir --skip-license && \
    rm ./cmake-3.14.4-Linux-x86_64.sh

# Download NGC client
RUN cd /usr/local/bin && wget https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip && unzip ngccli_cat_linux.zip && chmod u+x ngc && rm ngccli_cat_linux.zip ngc.md5 && echo "no-apikey\nascii\n" | ngc config set

# Set environment and working directory
ENV TRT_LIBPATH /usr/lib/x86_64-linux-gnu
ENV TRT_OSSPATH /workspace/TensorRT
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${TRT_OSSPATH}/build/out:${TRT_LIBPATH}"
WORKDIR /workspace
```

ã“ã®Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’ç«‹ã¡ä¸Šã’ã‚‹docker-composeãƒ•ã‚¡ã‚¤ãƒ«ãŒä»¥ä¸‹ã§ã™ã€‚
Dockerfileã®ãƒ‘ã‚¹ãªã©ã¯é©å®œä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

```yaml
version: '3'

services:
  t5-ja-trt:
    build:
      context: .
      dockerfile: ./docker/Dockerfile
    container_name: t5-ja-trt
    image: t5-ja-trt-image
    shm_size: '24gb'
    tty: true
    volumes: 
      - $PWD:/workspace
    command: '/bin/bash'
    ports:
      - 18121-18130:18121-18130
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [gpu]

```

requirements.txtã¯ã€Tensor-RTã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã«åŠ ãˆã¦ã€t5-base-japaneseã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€transformesãªã©ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€Tensor-RTã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ãŒã€`polygraphy`ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚‚å¿…è¦ã¨ãªã‚Šã¾ã™ã€‚

```
transformers==4.11.3
sentencepiece==0.1.96
onnx==1.10.2
onnxruntime==1.8.1
-f https://download.pytorch.org/whl/cu113/torch_stable.html

pycuda<2021.1
pytest
--extra-index-url https://pypi.ngc.nvidia.com
onnx-graphsurgeon

polygraphy
```

ä¸Šè¨˜ã®`docker-compose.yml`ã‚’ç”¨ã„ã¦ã€`docker-compose up -d`ã§ã‚³ãƒ³ãƒ†ãƒŠã‚’ç«‹ã¡ä¸Šã’ãŸå¾Œã€ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã€TensorRTã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

```shell
git clone https://github.com/nvidia/TensorRT TensorRT
cd TensorRT
# git checkout release/8.2 # ä»Šå›ã¯mainã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚checkoutã—ãªã„
git submodule update --init --recursive

# TensorRTã®ãƒ“ãƒ«ãƒ‰
cd $TRT_OSSPATH
mkdir -p build && cd build
cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out
make -j$(nproc)
```

# ã¾ã¨ã‚

ä»¥ä¸Šã€t5-base-japaneseã®é«˜é€ŸåŒ–ã®æ–¹æ³•ã§ã—ãŸã€‚å®Ÿéš›ã€t5ã®å‡¦ç†é€Ÿåº¦ã§æ‚©ã‚€å ´é¢ã¯å¤šã„ã¨æ€ã†ã®ã§ã€ãœã²è©¦ã—ã¦è¦‹ã¦ãã ã•ã„ã€‚
