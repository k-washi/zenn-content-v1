---
title: "Meta開発のセグメンテーションモデル Segment Anything Model(SAM) の解説" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習", "論文解説", "SAM"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
publication_name: "fusic"
---

こんにちは！[Fusic](https://fusic.co.jp/) 機械学習チームの鷲崎です。機械学習モデルの開発から運用までなんでもしています。もし、機械学習で困っていることがあれば、気軽に[お問い合わせ](https://fusic.co.jp/contact/)ください。

23/4/6、新しい画像セグメンテーションのモデルである、Segment Anything Model(SAM)がMeta社から発表されました。

使ってみたところ、分割しすぎな気がしますが、胴体やサンドバックなどうまく分離できています。サンドバックなどデータにほとんど含まれてなさそうですが、すごいです。後ほど解説しますが、プロンプトエンジニアリング次第では、より精度がよくなるかもしれません。

![](https://storage.googleapis.com/zenn-user-upload/dbf5c560da40-20230406.png)

アブストより、SAMの特徴としては、
- 1100万枚の画像とそれに付随した10億以上のマスクからなる世界最大のデータセットを構築し、訓練した
- プロンプトを用いて新しい画像分布やタスクにZeroshotで対応できるようにした
- 多くのタスクでZero-Shot性能がよく、教師あり学習の結果と同等以上であることもある

とのことです。

個人的には、プロンプトエンジニアリング部分がおもしろいと思いました。例えば、物体検出結果のBBoxをプロンプトとして用いることで、インスタンスセグメンテーションを実行していました。また、CLIPのテキスト、画像エンコーダをうまく使って、テキストからマスクの生成をしていました。

本記事では、このSAMの論文である、[Segment Anything](https://ai.facebook.com/research/publications/segment-anything/)の解説を行います。

解説しませんが、コード([facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything))です。

※以下、解説記事の画像は、論文より引用しています。

# 概要

論文の一番上の画像が、まさに、SAMの概念を表しています。下図の(b)を見ると、画像に加えて、プロンプトを入力しています。そして、それぞれを情報をエンコーダで良い感じに解釈し、デコーダで混ぜ合わせることで、有効なセグメンテーションマスクを出力しています。このような設計により、(a)のように、様々なプロンプトに対して、セグメンテーションのタスクを追加学習なしで実行可能となっています。

また、ZeroShot性能を向上させるために大量のデータによる訓練が必要であるため、下図(c)のように、訓練とアノテーションの繰り返しにより、10億を超えるマスクを含むSA-1Bデータセットを構築しています。[Segment Anything](https://segment-anything.com/)からデータセットをダウンロードできるはずです。

![](https://storage.googleapis.com/zenn-user-upload/43a7337e2fb4-20230406.png)

# SAMについて

任意のセグメンテーション用のプロンプトを入力したら、有効なセグメンテーションマスクを返すことが、モデルの目標です。プロンプトとは、画像内の何をセグメンテーションするかを指定するもので、例えば、セグメンテーション対象を特定する空間情報やテキスト情報などがそれにあたります。有効な出力マスクの要件として、プロンプトが曖昧で複数の物体を対象としている可能性がある場合、出力はこれらの物体の少なくとも1つの妥当なマスクである必要があります。（例えば、シャツ上の指した点（プロンプト）はシャツまたはそれを着ている人のいずれかを示す）

この要件を盛り込んだモデル SAMは、以下のような構造です。Prompt Encoderが、点やボックス、テキストなどスパースな入力を受け付ける部分と、密なマスク情報を受け付ける部分に分かれている点が面白いです。また、上記のように曖昧性の課題があるため、3つのマスク出力を行うように工夫しています。(3つあれば、殆どのケースに対応可能とのこと)　


![](https://storage.googleapis.com/zenn-user-upload/6e0404331f59-20230407.png)

23のセグメンテーションデータセットに対して、下図(Figure 3)のように、1つの前景点（プロンプトとして前景部分に点を打つ）から高品質のマスクを生成した場合、手動注釈のGTをわずかに下回る程度とのことです。


![](https://storage.googleapis.com/zenn-user-upload/062421d47491-20230407.png)



# プロンプトエンジニアリングでいろんなタスクを行う


Edge Deteection、Instant Segmentatino、Text to Maskなど、様々な下流タスクにおいて、プロンプトエンジニアリングを行うことで、よい性能を獲得しています。

各タスクを簡単に説明します。

Edge Detectionは、下図(Figure 10の一部)のようにエッジを検出するタスクで、16x16の格子状に並べた前景点に対してSAMでマスクを予測し、出力された768(16x16x3)個のマスクに対して、NMSによるフィルタリングやソーベルフィルタによるエッジ強調などを行いエッジ画像を作成しています。

これからも分かる通り、プロンプトの拡張と生成されたマスクの後処理をうまくやることで、様々なタスクに適応させることが可能になっています。

![](https://storage.googleapis.com/zenn-user-upload/469e7ed14f29-20230407.png)

インスタンスセグメンテーションは、ViTDetなど物体検出モデルの出力であるBBoxをプロンプトとして入力することで実行可能です。

また、下図(Figure 12の一部)のようなテキストを用いたマスクの生成は、CLIPのテキストエンコーダの出力をテキストプロンプトとして入力することで、マスクを生成できます。CLIPの画像埋め込みは、テキスト埋め込みと一致するように学習されているため、学習中は、画像の埋め込みを使用し、推論時はテキスト埋め込みを使用するといった手法を用いてるそうです。

![](https://storage.googleapis.com/zenn-user-upload/d11d44ecf666-20230407.png)

# まとめ

初手でセグメンテーションタスクを試す手法として、選択肢に入ると思います。プロンプトエンジニアリングを工夫すると、ドメインにすら特化したものにできる可能性があり、期待感があります。これを、FineTuningとかできると、かなり面白いと思うので試してみたいです。

最後に宣伝になりますが、機械学習でビジネスの成長を加速するために、[Fusic](https://fusic.co.jp/)の機械学習チームがお手伝いしています。機械学習のPoCから運用まで、すべての場面でサポートした実績があります。もし、困っている方がいましたら、ぜひ[Fusic](https://fusic.co.jp/)にご相談ください。[お問い合わせ](https://fusic.co.jp/contact/)から気軽にご連絡いただけますが、[TwitterのDM](https://twitter.com/kwashizzz)からでも大歓迎です！