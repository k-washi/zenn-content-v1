---
title: "人の顔を入れ替えてみた！最新の顔すり替え手法 SimSwapの解説！" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
---

こんにちは、鷲崎です。最近、精巧な偽画像を作成するディープフェイクによる事件が摘発され、実社会の驚異となってきています。例えば、動画の顔をすり替えることで、偽の演説動画を作成したフェイクニュースや、ポルノ動画に写った人物の顔をアイドルの顔にすり替えるフェイクポルノなどの事件は話題になったかと思います。

このように犯罪用途で何かと注目されるAIによる顔すり替え技術ですが、非常に有用な技術でもあります。例えば、テレビ番組や映画制作では亡くなった役者を登場させたり、スタントマンの顔をすり替えるなど制作の強化に使われています。また、洋服の試着では、体型や顔をすり替えることで、仮想的な試着を可能にします。博物館などで、歴史上の偉人をリアルな映像で登場させるなどの応用例もあります。

しかし、従来の顔すり替え技術では、対象の顔を目的の顔にすり替えるためにチューニングする必要があったり、すり替え対象の顔における表情や視線などの属性をすり替え後に保持できていないなどの課題がありました。そこで、本記事で紹介する[SimSwap: An Efficient Framework For High Fidelity Face Swapping](https://arxiv.org/abs/2106.06340)という論文では、Simple Swap (SimSwap)と呼ばれる効率的で、任意の顔の属性を保持したまま、目的の顔にすり替えるフレームワークが提案されました。

まず、実際に試して、下図のような結果が得られました。一番左の列がすり替え対象の顔画像で、一番上の行がすり替える目的の顔です。眼鏡をかけている私ですが、眼鏡のデータが少ないためか、顔すり替え後の画像の目の部分に違和感があります。他の方々の顔すり替えは上手くいっているようです。輪郭や表情、目線は保持しつつ、顔すり替えができています。

![](https://storage.googleapis.com/zenn-user-upload/442b2b05e6eb3b9010ad507b.png)

本記事では、以降、すり替え対象となる顔(左側列）を対象の顔(対象画像)、すり替える目的の顔(一番上の行）をソースの顔 (ソース画像)と呼びます。

# SimSwapの立ち位置
SimSwapとは、ソース画像と対象画像が与えられた時、対象の顔の属性(表情など)を変更せずにソースの顔の個性を対象の顔に写すフレームワークです。

過去の研究として、顔すり替え手法には、画像レベルで対象の顔を扱うソース指向の手法と、特徴量レベルで対象の顔を扱う対象指向の手法があります。ソース指向の手法は、表情や姿勢などの属性を対象の顔からソースの顔へ転写し、その後、転写されたソースの顔を対象の顔にブレンドすることで顔をすり替えています。この手法は、ソース画像の姿勢や照明の影響を受けやすく、すり替え対象の表情を正確に再現することができないという欠点があります。対象指向の手法は、対象画像の特徴をソース画像に似るように修正するので、ソースの顔のバリエーションに上手く対応できないという欠点があります。他にもIDに特化した顔すり替え手法では、特定のID(ソース情報)に限定してすり替えを行うため汎化性にかけていたり、GANベースの手法ではソースの顔情報を残すため、対象画像の表情などの属性情報を保持できていない場合が多々あります。

そこで、論文の著者は、ID(ソース情報)に特化した顔すり替え手法のアーキテクチャを分析し、デコーダ部分にID情報が統合されているため汎化性が欠如していることを発見しました。SimSwapでは、この統合を避けるためID注入モジュールを提案しています。また、IDと属性の情報が特徴量レベルで結びついていており、特徴量レベルで変更した場合性能が劣化することから、それを緩和する弱特徴マッチング損失を提案しています。上記の手法を用いることで、SimSwapは、競争力のあるID(ソース情報）の転写性能を実現し、従来の手法より優れた属性保持能力を持つことができています。

# 提案手法

![simswap arc.](https://storage.googleapis.com/zenn-user-upload/155f02b84953c358b9eddf6a.png)

上図(論文 Figure 2)はSimSwapのアーキテクチャです。簡単に説明すると、
1. 対象画像$I_T$が与えられたら、Encoderを通して特徴量$Fea_T$を抽出 (Encoder)
2. $Fea_T$の属性情報部分は残し、ID情報の部分のみソース画像$I_S$の情報に入れ替える (IIM; ID Injection Module)
3. Decoderで結果画像$I_R$を再構成する

と、Descriminatorで構成されています。

# ID注入部 (IIM)とGenerator損失について

2のIIMの部分は、特徴量$Fea_T$において、属性情報とID情報が密結合しているので、ID情報のみのすり替えが課題となります。そこで、IIMでは、学習損失を利用して、ネットワークに$Fea_T$のどの部分を変更し、どの部分を修正すべきかを暗黙的に学習させることで、$Fea_T$全体の修正を行いソース画像のID注入を達成しています。

このIIMはID抽出部(ID Extractor)とID埋め込み部(ID Embedding)の2つで構成されています。ID抽出部は、顔認識ネットワークを用いて、ソース画像$I_S$からIDベクトル$v_S$を抽出しています。ID埋め込み部(ID-Block)では、Residual Blockの改良版で、Batch Normalizationの部分をAdaptive Instance Normalization (AdaIN)に変更しています。また、十分なID埋め込みを達成するため9個のID-Blockを使用しています。

AdaINは、以下の式で表されます。

$$
AdaIN(Fea, v_S) = \sigma_S \frac{Fea - \mu (Fea)}{\sigma (Fea)} + \mu_S

$$

$\mu (Fea$)と$\sigma (Fea)$は $Fea$に対するチャンネル単位の平均と標準分散です。$\mu_S$と$\sigma_S$は、$v_S$から全結合層を使用して生成される値です。

IIMのあとは、IIMを通過した特徴量からDecoderにより$I_R$を再構成し、ID抽出によりIDベクトル$v_R$を取得します。得られた情報を用い、このGenerator部分では、以下の2つ損失が計算できます。

1. Identity Loss 

$$
L_{ID} = 1 - \frac{v_R \cdot v_S}{||v_R||_2 ||v_S||_2}
$$

2. 再構成Loss

$$
L_{Recon} = ||I_R - I_T||_1
$$

# Discriminator

顔の入れ替えタスクでは、ID情報のみ修正し、対象の顔属性(表情、位置、照明など)の情報は変更しないことが良いとされます。しかし、上記のようにIIM部分で$Fea_T$を直接修正しているため、ID情報の埋め込みにより属性情報が影響をうける可能性は大きいです。そこで、Feature Matching Loss (FML)という学習損失を用いて、制約をかけることで、属性の不一致を防いでいます。オリジナルのFMLは、pix2pixHDという手法にて導入され、Discriminatorを用いて画像の特徴を抽出し、比較しています。SimSwapでは、少し修正し、最初のいくつかのレイヤーに関しては無視した形で特徴を抽出し、比較しており、これをWeak Feature Matching Loss(wFM)としています。式は以下の通りです。


$$
L_{wFM} (D) = \sum_{i=m}^{M} \frac{1}{N_i} ||D^{(i)}(I_R) - D^{(i)}(I_T)||_1
$$

ここで、$D^{(i)}$は、Discriminator ($D$)のi番目の特徴抽出レイヤーを示しています。$N_i$はそのレイヤーの要素数です。$M$は、使用するレイヤー数です。

Disciminatorでは、wFMに加え、一般的なGANの損失であるRealとFakeを見分ける[Hinge バージョンのAdversarial Loss](https://arxiv.org/abs/1802.05957) ($L_{Adv}$)が用いられています。

``` python
# Hinge Loss
lossD = F.relu(1.0 - y_real).mean() + F.relu(1.0 + y_fake).mean()
```

# 損失

最終的に損失は、それぞれの重み$\lambda$と掛け合わせて以下のようになります。学習の安定のため、[Gradient Penalty](https://qiita.com/triwave33/items/5c95db572b0e4d0df4f0)($L_{GP}$)も導入されています。
また、wFMでは、Multi Scale Descriminatorを導入し、高解像画像の学習を効率化しています。例えば、$D_1$, $D_2$では、1, 1/2倍スケールの画像を入力としたDiscriminatorとなる。

$$
L = \lambda_{ID} L_{ID} +  L_{Adb} + \lambda_{GP} L_{GP} + \lambda_{wFM}(L_{wFM}(D_1) + L_{wFM}(D_2))
$$

# まとめ

SimSwapの実験結果の例は、以下の図のようになりました。また、実験結果としてIDの注入が上手くできている指標が得られています。違和感なくすり替えれらており、恐ろしくも感じます。

SimSwapでは、IDをどう扱うかといることを損失の設計で解決しており、他のGANでも有効な手法な気がしています。また、Hinge LossやGPなど学習の安定化のための手法が、一般的に使われており、今後、GANの実装には積極的に取り入れていきたい所です。


![Sim Swap res](https://storage.googleapis.com/zenn-user-upload/b13b095c615d03705f0cef6f.png)