---
title: "SimCTG: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãŠã‘ã‚‹Contrastiveå­¦ç¿’æ¨è«–ã®è§£èª¬ãƒ»å®Ÿè£…" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ˜¸" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["æ©Ÿæ¢°å­¦ç¿’", "python"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: true # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
---

# ã¯ã˜ã‚ã«

ã“ã‚“ã«ã¡ã¯ã€ã‚ã£ã—ãƒ¼ã§ã™ã€‚
ç”»åƒãƒ»è‡ªç„¶è¨€èªãƒ»éŸ³å£°ã«é–¢ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ç ”ç©¶é–‹ç™ºã‚„MLOpsã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã€æ©Ÿæ¢°å­¦ç¿’ã«é–¢ã—ã¦ã€ã”ç›¸è«‡ãŒã‚ã‚Œã°ã€[@kwashizzz](https://twitter.com/kwashizzz)ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¾ã§DMã—ã¦ãã ã•ã„ï¼
ã“ã‚Œã¾ã§ã®ã€[æ©Ÿæ¢°å­¦ç¿’è¨˜äº‹ã®ã¾ã¨ã‚](https://zenn.dev/kwashizzz/articles/my-ml-articles-summary)ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€æ–‡ç« ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ä¸è‡ªç„¶ã•ã‚„ã€æœ›ã¾ã—ããªã„å˜èªã®ç¹°ã‚Šè¿”ã—ã‚’æŠ‘ãˆã‚‹æ‰‹æ³•ã¨ã—ã¦ã€[A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)ã§ææ¡ˆã•ã‚ŒãŸSimCTG(a **SIM**ple **C**ontrastive framework for neural **T**ext **G**eneration)ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

ã¾ãŸã€[è«–æ–‡ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰](https://github.com/yxuansu/SimCTG/tree/bb54480e5c43d62d5b660d5cdabaee7c7d7af442)ã‚’å‚è€ƒã«ã—ã€Encoder-Decoderå½¢å¼ã®æ–‡ç« ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹T5ã«SimCTGã‚’é©ç”¨ã—ã¦ã¿ãŸã®ã§ã€å®Ÿè£…æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

â€» [è«–æ–‡ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰](https://github.com/yxuansu/SimCTG/tree/bb54480e5c43d62d5b660d5cdabaee7c7d7af442)ã«ã¯ã€GPT-2ã«é©ç”¨ã—ãŸå®Ÿè£…ãŒã‚ã‚‹ã®ã§ã€GPT-2ã‚’ä½¿ã„ãŸã„äººã¯ã€[ã“ã¡ã‚‰](https://github.com/yxuansu/SimCTG/tree/bb54480e5c43d62d5b660d5cdabaee7c7d7af442)ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

---

22/5/31 - æ¨è«–å‡¦ç†ã‚’hugging faceã®generateãƒ¡ã‚½ãƒƒãƒ‰likeã®å‡¦ç†ã«ä¿®æ­£ã—ã¾ã—ãŸã€‚(æ¨è«–é€Ÿåº¦ãŒæ”¹å–„ã—ã¦ã„ã¾ã™ã€‚)

# ãªãœSimCTGãŒå¿…è¦ãªã®ã‹?

> æ˜¨ä»Šã®Decoderã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å…¥åŠ›æ–‡ï¼‹éå»ã«ç”Ÿæˆã—ãŸå˜èªåˆ—ã‹ã‚‰æ¬¡ã®å˜èªåˆ—ã‚’äºˆæ¸¬ã™ã‚‹Auto-Regressiveãªç”ŸæˆãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®éš›ã«ã€è¨ˆç®—é‡ã®é–¢ä¿‚ã‹ã‚‰ã€Gready Searchã‚„Beam Searchãªã©ãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

å‚è€ƒ: [ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãŠã‘ã‚‹ decoding ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯: Greedy search, Beam search, Top-K, Top-p
](https://zenn.dev/hellorusk/articles/1c0bef15057b1d)

ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®æ¢ç´¢æ‰‹æ³•ã‚’ç”¨ã„ãŸå ´åˆã€åŒã˜å˜èªã‚’ç¹°ã‚Šè¿”ã™ãªã©ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚ãã“ã§ã€`huggingface`ã®æ–‡ç« ç”Ÿæˆé–¢æ•°`generete`ã§ã¯ã€`no_repeat_ngram_size`ã‚„`repetition_penalty`ã¨ã„ã†å¼•æ•°ãŒã‚ã‚Šã€ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ç¹°ã‚Šè¿”ã—ã‚’æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ã¯ã€åŒã˜å˜èªåˆ—ãŒå…¨ãå‡ºãªããªã‚Šä¸è‡ªç„¶ãªç”Ÿæˆã«ãªã£ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ä¸‹å›³(è«–æ–‡ä¸­Fig.1)ã®(a)ã¯ã€GPT-2ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸå„ãƒˆãƒ¼ã‚¯ãƒ³ã«ãŠã‘ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«(Transformeræœ€çµ‚å±¤)ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã§ã™ã€‚è¦‹ã¦ã®é€šã‚Šã€æ–‡ä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é¡ä¼¼åº¦ãŒ0.95ä»¥ä¸Šã«ãªã£ã¦ãŠã‚Šã€äº’ã„ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒè¿‘ã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã§ç¹°ã‚Šè¿”ã—ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ã¦ã—ã¾ã†åŸå› ã®ä¸€ã¤ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚è«–æ–‡ä¸­ã§ã¯ã€ã“ã®ã‚ˆã†ã«ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒåã£ã¦ã„ã‚‹ã“ã¨ã‚’ç•°æ–¹æ€§ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚

![Fig.1](https://storage.googleapis.com/zenn-user-upload/c51f3beef16d-20220313.png)

ç†æƒ³çš„ã«ã¯ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«é–“ã‚’è­˜åˆ¥å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ä¸Šå›³(b)ã®ã‚ˆã†ã«ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é¡ä¼¼åº¦ãŒä½ããªã‚‹ã‚ˆã†ã«ã™ã¹ãã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã¤ã¾ã‚Šã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯åã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ï¼ˆç­‰æ–¹æ€§ã«ã™ã¹ãã€‚ï¼‰

ãã“ã§ã€æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã§è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã„ãã¨ã„ã†å¾“æ¥æ‰‹æ³•ã«å¯¾ã—ã¦ã€è­˜åˆ¥å¯èƒ½ã§ç­‰æ–¹çš„ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®å­¦ç¿’ã‚’ä¿ƒã™SimCTGæå¤±ã‚’ææ¡ˆã—ã€åŠ ãˆã¦ã€SimCTGã«ã‚ˆã‚‹å­¦ç¿’ã‚’è£œå®Œã™ã‚‹æ–°ã—ã„å¾©å·åŒ–æ‰‹æ³•ã§ã‚ã‚‹Contrastive Search (å¯¾ç…§æ¢ç´¢)ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

# SimCTG

SimCTGã¯ã€æœ€å°¤æ¨å®šã®æå¤±ï¼ˆã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±)ã¨å¯¾ç…§æå¤±(Constrastive Loss)ã‹ã‚‰ãªã‚Šã¾ã™ã€‚

å˜èªåˆ—ã‚’$x$ã¨ã—ãŸå ´åˆã€æœ€å°¤æ¨å®šã®æå¤±ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ä¸€ã¤å‰ã¾ã§ã®å˜èªåˆ—($x_{ < i}$)ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹å˜èªãŒ$x_i$ã§ã‚ã‚‹ç¢ºç‡åˆ†å¸ƒ $p_{\theta}(x)$ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªæå¤±$\mathcal{L}_{MLE}$ã§ã™ã€‚

$$
\begin{equation}
\mathcal{L}_{MLE} = - \frac{1}{|x|} \sum_{i=1}^{|x|} \log p_{\theta} (x_i | x_{ < i})
\tag{1}
\end{equation}
$$

æ¬¡ã«å¯¾ç…§æå¤±$\mathcal{L}_{CL}$ã¯ã€é¡ä¼¼åº¦é–¢æ•°$s$ã‚’ç”¨ã„ã¦ã€ä»¥ä¸‹ã®å¼ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€åŒã˜å˜èªã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’é«˜ãã—ã€ç•°ãªã‚‹å˜èªã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’å°ã•ãã™ã‚‹ã‚ˆã†ãªæå¤±ã§ã™ã€‚

$$
\begin{equation}

\mathcal{L}_{CL} = \frac{1}{|x| \times (|x| - 1)} \sum_{i=1}^{|x|} \sum_{j=1, j \neq i}^{|x|} 
\max \{ 0, \rho - s(h_{x_i}, h_{x_i}) + s(h_{x_i}, h_{x_j}) \}

\tag{2}
\end{equation}
$$

ãƒãƒ¼ã‚¸ãƒ³ $\rho \in [-1, 1]$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€$h_{x_i}$ãŒãƒˆãƒ¼ã‚¯ãƒ³($x_i$)ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã§ã™ã€‚é¡ä¼¼åº¦é–¢æ•°ã¯ã€

$$
\begin{equation}

s(h_{x_i}, h_{x_j}) = \frac{h_{x_i}^Th_{x_j}}{||h_{x_i}|| \cdot || h_{x_j} ||}

\tag{3}
\end{equation}
$$

ã§è¨ˆç®—ã§ãã¾ã™ã€‚

ä¸Šè¨˜ã®æå¤±ã‚’ç”¨ã„ã¦SimCTGæå¤±ã¯ã€

$$
\begin{equation}

\mathcal{L}_{CTG} = \mathcal{L}_{MLE} + \mathcal{L}_{CL}

\tag{4}
\end{equation}
$$

ã¨ãªã‚Šã¾ã™ã€‚

ã“ã®SimCTGæå¤±ã‚’ç”¨ã„ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯è¿‘ãã«ãªã‚Šã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯é ããªã‚Šã¾ã™ã€‚

# Contrastive Search

å¯¾ç…§æå¤±ã‚’ç”¨ã„ãŸSimCTGã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹å ´åˆã€ä¸€ã¤å‰ã¾ã§ã®å˜èªåˆ—ã¨ã¯ç•°ãªã‚‹å˜èªã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚ãã®çŸ¥è­˜ã‚’å–ã‚Šå…¥ã‚Œã€å¾“æ¥æ‰‹æ³•ã¨åŒæ§˜ã«ç¢ºç‡ã®é«˜ã„å˜èªã‚’æ¢ç´¢ã™ã‚‹ã“ã¨ã«åŠ ãˆã€ä¸€ã¤å‰ã¾ã§ã®å˜èªåˆ—ã¨é¡ä¼¼åº¦ãŒä½ã„å˜èªã‚’æ¢ç´¢ã™ã‚‹Contrastive SearchãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

äºˆæ¸¬ç¢ºç‡ãŒé«˜ã„$k$å€‹ã®å˜èªé›†åˆã‚’$V^k$ã¨ã—ãŸå ´åˆã®Contastive Searchã«ã‚ˆã‚‹å˜èªé¸æŠã¯ã€

$$
\begin{equation}

x_t = \mathrm{argmax}_{v \in V^k} \{ (1 - \alpha) \times p_{\theta}(v | x_{ < t}) 
- \alpha \times (\mathrm{max} \{ s(h_v, h_{x_j}) : 1 \leq j \leq t - 1 \}) \}

\tag{5}
\end{equation}
$$

ã§è¡Œã‚ã‚Œã¾ã™ã€‚

# çµæœ

è«–æ–‡ã®Table.1ã‚’è¦‹ã¦ãã ã•ã„ã€‚è«–æ–‡é€šã‚Šã®çµæœã ã¨ã€SimCTGã«ã€Contrastive Searchã‚’ä½¿ã£ãŸéš›ã€æœ€å¼·ã§ã™ã€‚
å€‹äººçš„ã«ä½¿ç”¨ã—ã¦è¦‹ãŸã¨ã“ã‚ã€æ™®é€šã®å­¦ç¿’ã«ã‚ˆã‚Šä½œæˆã—ãŸT5ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã€logitsã®Top1ã¨2ã«é–‹ããŒã‚ã‚Šã¾ã—ãŸã€‚

# T5ã«ãŠã‘ã‚‹SimCTGã®å­¦ç¿’ã®å®Ÿè£…

ä»Šå›ã¯ã€[ã€æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ä»˜ãã€‘2021å¹´ã«è‡ªç„¶è¨€èªå‡¦ç†ã‚’ã™ã‚‹äººã«ãŠå‹§ã‚ã—ãŸã„äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«](https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44)ã§æä¾›ã•ã‚Œã¦ã„ã‚‹T5ã®è»¢ç§»å­¦ç¿’ä¾‹ã®[ã‚³ãƒ¼ãƒ‰](https://github.com/sonoisa/t5-japanese)ã‚’ãƒ™ãƒ¼ã‚¹ã«æ‹¡å¼µã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚[ã‚³ãƒ¼ãƒ‰](https://github.com/sonoisa/t5-japanese)ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆä¸€ç¨®ã®æ–‡ç« è¦ç´„ï¼‰ã®ãƒªãƒ³ã‚¯ã‹ã‚‰Google Colabã§ã®è¨“ç·´ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã‚Œã¾ã™ã€‚ã¾ãŸã€æœ¬è¨˜äº‹ã§ã¯ã€ç°¡å˜ã®ãŸã‚ã€ä¿®æ­£ã—ãŸSimCTGã®é‡è¦ãªéƒ¨åˆ†ã®ã¿è¨˜è¼‰ã—ã¾ã™ã€‚

ã¾ãšã€é¡ä¼¼åº¦è¨ˆç®—ã®éƒ¨åˆ†ã§ã™ã€‚ã“ã“ã§ã¯ã€T5ã®ãƒ‡ã‚³ãƒ¼ãƒ€ã®æœ€çµ‚å±¤ã‚’ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒãƒƒãƒ, å˜èªåˆ—æ•°, ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ãªã‚‹è¡Œåˆ—ãªã®ã§ã€å„å˜èªã”ã¨ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€ãƒãƒƒãƒæ•°Ã—å˜èªåˆ—æ•°Ã—å˜èªåˆ—æ•°ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

```python
def t5_cosine_simirality(outputs):
    last_hidden_state = outputs.decoder_hidden_states[-1] # torch.Size([3, 64, 768])
    norm_rep = last_hidden_state / last_hidden_state.norm(dim=2, keepdim=True)
    cosine_scores = torch.matmul(norm_rep, norm_rep.transpose(1,2)) # torch.Size([3, 64, 64])
    return cosine_scores
```

äº‹å‰å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã®é¡ä¼¼åº¦è¡Œåˆ—ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚(é•·ã„ã®ã§æœ€åˆã®5è¡Œ5åˆ—éƒ¨åˆ†ã®ã¿ã§ã™ã€‚)
å®Ÿéš›ã€å¯¾è§’éƒ¨åˆ†ä»¥å¤–ã®é¡ä¼¼åº¦ã‚‚é«˜ã„ã“ã¨ãŒè¦‹ã¦å–ã‚Œã¾ã™ã€‚
```
[0.9999, 0.9807, 0.9654, 0.9587, 0.9528, 
[0.9807, 0.9999, 0.9805, 0.9755, 0.9687,
[0.9654, 0.9805, 0.9999, 0.9816, 0.9712,
[0.9587, 0.9755, 0.9816, 0.9999, 0.9878, 
[0.9528, 0.9687, 0.9712, 0.9878, 0.9999,
```

æ¬¡ã«ã€å¯¾ç…§æå¤±ã®éƒ¨åˆ†ã§ã™ã€‚ã“ã‚Œã¯ã€[è«–æ–‡ã®ã‚³ãƒ¼ãƒ‰](https://github.com/yxuansu/SimCTG/blob/bb54480e5c43d62d5b660d5cdabaee7c7d7af442/pretraining/simctg.py#L50)ã¨åŒã˜ã§ã™ã€‚è«–æ–‡ã‚³ãƒ¼ãƒ‰ã®ã‚¹ã‚¿ãƒ¼ã‚’ãƒãƒƒãƒã£ã¨æŠ¼ã—ã¾ã—ã‚‡ã†ï¼

```python
def compute_contrastive_loss(score_matrix, margin):
    '''
        margin: predefined margin to push similarity score away
        score_matrix: bsz x seqlen x seqlen; cosine similarity matrix
        input_ids: bsz x seqlen
    '''
    bsz, seqlen, _ = score_matrix.size()
    gold_score = torch.diagonal(score_matrix, offset=0, dim1=1, dim2=2) #å¯¾è§’æˆåˆ†ã‚’å–ã‚Šå‡ºã™ã€€bsz x seqlen
    gold_score = torch.unsqueeze(gold_score, -1) 
    assert gold_score.size() == torch.Size([bsz, seqlen, 1])

    difference_matrix = gold_score - score_matrix
    assert difference_matrix.size() == torch.Size([bsz, seqlen, seqlen])
    loss_matrix = margin - difference_matrix # bsz x seqlen x seqlen
    loss_matrix = torch.nn.functional.relu(loss_matrix)
    cl_loss = torch.mean(loss_matrix)
    return cl_loss
```

æ¬¡ã«å­¦ç¿’ã™ã‚‹éƒ¨åˆ†ã§ã™ã€‚å‚è€ƒã«ã•ã›ã¦ã„ãŸã ã„ãŸè»¢ç§»å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã¯Pytorch Lightningã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ãã®ãƒ¢ãƒ‡ãƒ«éƒ¨åˆ†ã®æ‹¡å¼µã§ã™ã€‚
T5ã®éš ã‚Œå±¤ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã«ã€`output_hidden_states=True `ã‚’è¨­å®šã—ã¦ã„ã¾ã™ã€‚`_step`éƒ¨åˆ†ã§æå¤±ã‚’è¿½åŠ ã§è¨ˆç®—ã—ã¦ã„ã¾ã™ã€‚

```python
class T5FineTuner(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        # æœ€æ–°ç‰ˆã®plã§ã¯ã€hparamsã®updateæ–¹æ³•ãŒå¤‰æ›´ã•ã‚ŒãŸã€‚
        self.save_hyperparameters(hparams)
        # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path, is_fast=True)

    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, 
                decoder_attention_mask=None, labels=None):
        """é †ä¼æ¬"""
        return self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            labels=labels,
            output_hidden_states=True # decoderã®éš ã‚Œå±¤ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚Trueã«è¨­å®šã€‚
        )

    def _step(self, batch):
        """ãƒ­ã‚¹è¨ˆç®—"""
        labels = batch["target_ids"]
        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100

        outputs = self(
            input_ids=batch["source_ids"],
            attention_mask=batch["source_mask"],
            decoder_attention_mask=batch['target_mask'],
            labels=labels,
        )

        mle_loss = outputs.loss
        cosine_scores = t5_cosine_simirality(outputs) # é¡ä¼¼åº¦è¨ˆç®—
        margin = 0.5 # margin
        cl_loss = compute_contrastive_loss(cosine_scores, margin) # å¯¾ç…§æå¤±è¨ˆç®—

        loss = mle_loss + cl_loss # SimCTG æå¤±
        loss = loss.mean()
        return loss
```

ä¸Šè¨˜ã®å¤‰æ›´ã‚’è¡Œãªã£ãŸã‚³ãƒ¼ãƒ‰ã§å­¦ç¿’ã—ãŸçµæœã€ä»¥ä¸‹ã®é¡ä¼¼åº¦ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚(åŒæ§˜ã«ã€é•·ã„ã®ã§æœ€åˆã®5è¡Œ5åˆ—éƒ¨åˆ†ã®ã¿ã§ã™ã€‚)
å¯¾è§’æˆåˆ†ã®é¡ä¼¼åº¦ãŒé«˜ãã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é¡ä¼¼åº¦ã§ã‚ã‚‹å¯¾è§’æˆåˆ†ä»¥å¤–ã¯ã€é¡ä¼¼åº¦ãŒå°ã•ããªã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

```
[0.9999, 0.2982, -0.5032, -0.3650, -0.5413,
[0.2982, 0.9999, 0.2020, 0.2578, 0.2829,
[-0.5032, 0.2020, 0.9999, 0.6498, 0.6361,
[-0.3650, 0.2578, 0.6498, 1.0, 0.6117, 0.7059,
[-0.5413, 0.2829, 0.6361, 0.6117, 0.9999
```

# T5ã«ãŠã‘ã‚‹Contrastive Searchã®å®Ÿè£…

ã¾ãšã€æ¨è«–ã®å¤§ã¾ã‹ãªæµã‚Œã§ã™ã€‚è«–æ–‡ã®å®Ÿè£…ã§é‡è¦ãªéƒ¨åˆ†ã¯ã€å‰ã®å˜èªåˆ—ã‹ã‚‰æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹`ContrastiveDecodingOneStepFast`ã¨ã„ã†é–¢æ•°ã§ã™ãŒã€githubã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹é–¢æ•°ã§æ¨è«–ã™ã‚‹ã¨æ¨è«–é€Ÿåº¦ãŒé…ã„ãŸã‚ã€huggingfaceã®generateãƒ¡ã‚½ãƒƒãƒ‰ã«åˆã‚ã›ãŸå½¢å¼ã§ä½œæˆã—ãŸã€`T5SimCTGGenerate`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚


```python
device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

decoding_len = 512 # æ–‡ç« ã®æœ€å¤§é•·ã•
beam_width = 3 # å®Ÿè£…ã§ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®çµæœã«Contrastive Searchã‚’è¡Œãªã£ã¦ã„ãŸã€‚
alpha = 0.5 # (1 - a) * ç¢ºç‡æœ€å¤§ + a * é¡ä¼¼åº¦
model = T5SimCTGGenerate(T5_MODEL_DIR, max_length=decoding_len, num_beams=beam_width, simctg_alpha=alpha) # T5_MODEL_DIRã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãªã©ãŒå…¥ã£ã¦ã„ã‚‹ã¨ä»®å®š

text_list = [
    "text1 ......"
]

output = model(text_list)
print(output)
```

æ¬¡ã«ã€`T5SimCTGGenerate`ã®å®Ÿè£…ã§ã™ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®æ¨è«–çµæœã‚’ä¿æŒã—ãŸã¾ã¾ã€ãã‚Œã¨ä¸€ã¤å‰ã®ãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã«ã‚ˆã‚‹æ¨è«–ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ã¾ãŸã€GPUä½¿ç”¨ã—ã¦ã„ãŸéš›ã«ã€ãƒ¡ãƒ¢ãƒªãƒ¼ã‚¨ãƒ©ãƒ¼ã«ãªã£ã¦ã„ãŸã®ã§ã€ãã®å¯¾å‡¦ã‚’å…¥ã‚Œã¦ã„ã¾ã™ã€‚[è«–æ–‡å®Ÿè£…ã¯ã“ã®éƒ¨åˆ†](https://github.com/yxuansu/SimCTG/blob/bb54480e5c43d62d5b660d5cdabaee7c7d7af442/document_generation/utlis.py#L126)ã«ãªã‚Šã¾ã™ã€‚


```python
class T5SimCTGGenerate:
    def __init__(self, model_dir, gpu_id=0, max_length=512, num_beams=3, simctg_alpha=0.5):
        self.is_cuda = torch.cuda.is_available()
        if gpu_id > -1:
            self.device = torch.device(f'cuda:{gpu_id}') if self.is_cuda else torch.device('cpu')
        else:
            self.device = torch.device('cpu')
        
        self.max_length = max_length
        self.num_beams = num_beams
        self.simctg_alpha = simctg_alpha
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.tokenizer = T5Tokenizer.from_pretrained(model_dir, is_fast=True)
        self.model = T5ForConditionalGeneration.from_pretrained(model_dir)
        for param in self.model.parameters():
            param.grad = None
        self.model.to(self.device)
        self.model.eval()

        self.eos_token_tensor = torch.as_tensor(self.tokenizer.eos_token_id, dtype=torch.long, device=self.device)

    def __call__(self, text_list):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒåŒ–+ãƒˆã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œã€T5ã«ã‚ˆã‚‹æ¨è«–ã‚’è¡Œã„ã€çµæœã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚
        """
        batch_input_ids, batch_attention_mask = self.__create_batch(text_list) # ãƒãƒƒãƒåŒ–+ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        generated = self.forward(batch_input_ids, batch_attention_mask) # t5ã®æ¨è«–
        return [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False) for t in generated]
    
    @torch.no_grad()
    def forward(self, batch_input_ids, batch_attention_mask):
        self.model.eval()
        batch_size, _ = batch_input_ids.size()

        no_pad_length = 1
        for bid in range(batch_size):
            _no_pad_length = torch.sum(batch_input_ids[bid] != self.tokenizer.pad_token_id).item()
            no_pad_length = max(no_pad_length, _no_pad_length)
        
        # ç”Ÿæˆçµæœã‚’ä¿å­˜ã™ã‚‹é…åˆ—
        generated = torch.zeros((batch_size, self.max_length), device=self.device, dtype=torch.long) #[[] for _ in range(batch_size)] # ãƒãƒƒãƒã”ã¨ã®ç”Ÿæˆæ–‡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã§ã™ã€‚
        is_eos = [False for _ in range(batch_size)] # ãƒãƒƒãƒã”ã¨ã®æ–‡ç« ç”Ÿæˆçµ‚äº†(EOS)åˆ¤å®šãƒªã‚¹ãƒˆã§ã™ã€‚

        past_key_values = None
        last_hidden_states = None
        logits = None

        batch_input_ids = batch_input_ids.to(self.device)
        batch_attention_mask = batch_attention_mask.to(self.device)

        selected_batch_indexes = torch.LongTensor([bs for bs in range(batch_size)]).to(self.device)
        _selected_idx = torch.zeros((batch_size), dtype=torch.long).to(self.device)
        _next_ids = torch.zeros((batch_size), dtype=torch.long).to(self.device)
        
        model_kwargs = {}
        model_kwargs["use_cache"] = True

        _kewargs = ['decoder_input_ids', 'past_key_values', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask']
        for _kewarg in _kewargs:
            model_kwargs[_kewarg] = None
            
            
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®äº‹å‰è¨ˆç®—
        # 2. prepare encoder args and encoder kwargs from model kwargs
        irrelevant_prefix = ["decoder_", "cross_attn", "use_cache"]
        encoder_kwargs = {
            argument: value
            for argument, value in model_kwargs.items()
            if not any(argument.startswith(p) for p in irrelevant_prefix)
        }
        encoder_kwargs["return_dict"] = True
        encoder_kwargs["input_ids"] = batch_input_ids
        encoder_kwargs["attention_mask"] = batch_attention_mask
        model_kwargs["encoder_outputs"] = self.model.encoder(**encoder_kwargs)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ã«ã‚ˆã‚‹å‡¦ç†éƒ¨åˆ†
        for _, step in enumerate(range(self.max_length)):
            if step == 0:
                # T5ã§ã¯ã€æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿pad tokenã‚’ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ã¨ã™ã‚‹ã€‚
                with torch.no_grad():
                    decoder_inputs = torch.as_tensor([[self.tokenizer.pad_token_id] for _ in range(batch_size)], device=self.device) # pad_token_idã§åŸ‹ã‚ãŸé…åˆ—ãŒæœ€åˆã®å…¥åŠ›ã¨ãªã‚‹
                    model_kwargs["attention_mask"] = self.model._prepare_attention_mask_for_generation(    
                        batch_input_ids, self.tokenizer.pad_token_id, self.tokenizer.eos_token_id
                    )
                    # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›å½¢æˆã¯ã€huggingfaceã®T5å®Ÿè£…ã®ã‚‚ã®ã‚’ä½¿ç”¨
                    model_inputs = self.model.prepare_inputs_for_generation(decoder_inputs, **model_kwargs)
                    output = self.model(
                        **model_inputs, 
                        output_hidden_states=True,
                        output_attentions=False,
                        return_dict=True
                    )
                    last_hidden_states = output.decoder_hidden_states[-1] #.cpu()    # [B, S, E]
                    # ãƒãƒƒãƒæ•° => ãƒãƒƒãƒæ•°xãƒ“ãƒ¼ãƒ æ•°ã«å¤‰æ›´
                    model_kwargs["past"] = enlarge_past_key_values( output.past_key_values, self.num_beams)
                    logit_for_next_step = output.logits[:, -1, :] #.cpu()    # [B, V]
                    expanded_return_idx = (
                        torch.arange(decoder_inputs.shape[0]).view(-1, 1).repeat(1, self.num_beams).view(-1).to(decoder_inputs.device)
                    )
                    model_kwargs["attention_mask"] = model_kwargs["attention_mask"].index_select(0, expanded_return_idx)
                    model_kwargs["encoder_outputs"]["last_hidden_state"] = model_kwargs["encoder_outputs"].last_hidden_state.index_select(
                        0, expanded_return_idx.to(model_kwargs["encoder_outputs"].last_hidden_state.device)
                    )
                    
                    
            _, seqlen, embed_dim = last_hidden_states.size() #.cpu()    # [B, S, E]

            next_probs = F.softmax(logit_for_next_step, dim=-1)

            # æœ€å¤§ç¢ºç‡ã®kå€™è£œã‚’å–å¾—
            _, top_k_ids = torch.topk(logit_for_next_step, dim=-1, k=self.num_beams) # [B, V:38000ãã‚‰ã„] -> [B, K:beam_width]
            top_k_probs = torch.gather(next_probs, dim=1, index=top_k_ids) # å€™è£œã®ç¢ºç‡ã‚’å–å¾— [B, K] 
            
            # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã®past_keyã‚’ãƒãƒƒãƒxå€™è£œã®ã‚¿ãƒ—ãƒ«ã«ä¿®æ­£
            model_inputs = self.model.prepare_inputs_for_generation(top_k_ids.contiguous().view(-1, 1), **model_kwargs) 

            # æ¬¡ã®å˜èªã‚’äºˆæ¸¬
            with torch.inference_mode():
                output = self.model(
                    **model_inputs,
                    output_hidden_states=True,
                    return_dict=True
                )
                model_kwargs["past"] = output.past_key_values
                    
            logits = output.logits[:, -1, :] #.cpu()    # [B*K, V]
            next_hidden = output.decoder_hidden_states[-1] #.cpu()    # [B*K, 1, E]
            context_hidden = last_hidden_states.unsqueeze(1).expand(-1, self.num_beams, -1, -1).reshape(batch_size*self.num_beams, seqlen, embed_dim)    # [B*K, S, E]

            # ãƒãƒƒãƒã”ã¨ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã®å˜èªã‚’é¸æŠ
            # å®Ÿéš›ã®å¼ã®éƒ¨åˆ†ã‚’è¨ˆç®— (ä¸‹ã®é–¢æ•°)
            selected_idx = ranking_fast(
                context_hidden, 
                next_hidden, 
                top_k_probs,
                self.simctg_alpha,
                self.num_beams,
            )  # [B]
            
            for bs in range(batch_size):
                _selected_idx[bs] = torch.add(selected_idx[bs], bs * self.num_beams)
            _next_ids[:] = top_k_ids[selected_batch_indexes, selected_idx] #.unsqueeze(-1)    # [B, 1]
            next_hidden = next_hidden[_selected_idx, :]
            last_hidden_states = torch.cat([last_hidden_states, next_hidden], dim=1)    # [B, S, E]
            logits = logits[_selected_idx, :]
            logit_for_next_step = logits

            for idx in range(batch_size): #ãƒãƒƒãƒæ•°åˆ†
                # EOSã®å ´åˆã‚¹ã‚­ãƒƒãƒ—
                if is_eos[idx]:
                    continue
                
                # EOSã®ãƒã‚§ãƒƒã‚¯
                if torch.eq(_next_ids[idx], self.eos_token_tensor).all():
                    is_eos[idx] = True
                    continue
                
                generated[idx, step] = _next_ids[idx]
            if False not in is_eos:
                break

        # ä¸è¦ãªã‚‚ã®ã‚’GPUã‹ã‚‰é™¤å»
        if self.device != torch.device("cpu"):
            del batch_input_ids, batch_attention_mask, top_k_ids, top_k_probs, last_hidden_states, past_key_values,  next_probs, _next_ids, _selected_idx, selected_idx, next_hidden, logits, output, context_hidden, model_kwargs
            torch.cuda.empty_cache()
        return generated
    
    def preprocess(self, text):
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        text = text.lower() # æœ¬å½“ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ãŒå¿…è¦
        text_token = self.tokenizer.batch_encode_plus(
            [text], max_length=self.max_length, truncation=True, padding="max_length", return_tensors="pt"

        )
        return text_token

    def __create_batch(self, text_list):
        # ãƒãƒƒãƒã®ä½œæˆ
        batch_input_ids = []
        batch_attention_mask = []
        for text in text_list:
            tokens = self.preprocess(text)
            input_ids = tokens["input_ids"].squeeze()
            attention_mask = tokens["attention_mask"].squeeze()
            batch_input_ids.append(input_ids)
            batch_attention_mask.append(attention_mask)
        batch_input_ids = torch.stack(batch_input_ids)
        batch_attention_mask = torch.stack(batch_attention_mask)
        return batch_input_ids, batch_attention_mask
        
def enlarge_past_key_values(past_key_values, beam_width):
    # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã®past_keyã‚’ãƒãƒƒãƒxå€™è£œã®ã‚¿ãƒ—ãƒ«ã«ä¿®æ­£
    # from [B, num_head, seq_len, esz] to [B*K, num_head, seq_len, esz]
    new_key_values = []
    for layer in past_key_values:
        items = []
        for item in layer:
            # item is the key and value matrix
            bsz, num_head, seq_len, esz = item.size()
            item = item.unsqueeze(1).expand(-1, beam_width, -1, -1, -1).reshape(bsz*beam_width, num_head, seq_len, esz)    # [bsz*beam, num_head, seq_len, esz]
            items.append(item)
        new_key_values.append(items)
    return new_key_values

def select_past_key_values(past_key_values, beam_width, selected_idx):
    '''ãƒãƒƒãƒxå€™è£œã‹ã‚‰æœ€å¤§ã‚¹ã‚³ã‚¢ã®past_keyã‚’é¸æŠ
    select_idx: [B]
    '''
    new_key_values = []
    for layer in past_key_values:
        items = []
        for item in layer:
            bsz_and_beam, num_head, seq_len, esz = item.size()
            bsz = int(bsz_and_beam//beam_width)
            item = torch.stack(torch.split(item, beam_width, dim=0))    # [B, K, num_head, seq_len, esz] 
            item = item[range(bsz), selected_idx, :, :, :]   # [B, num_head, seq_len, esz]
            items.append(item)
        new_key_values.append(items)
    return new_key_values
```

ä»¥ä¸Šã€T5ã®Consrastive Searchå®Ÿè£…ã«ãªã‚Šã¾ã™ã€‚

# ã¾ã¨ã‚

æ–‡ç« ç”Ÿæˆã®æ”¹å–„æ‰‹æ³•ã‚’æ¢ã—ã¦ã„ã¦ã€å¶ç„¶è¦‹ã¤ã‘ãŸè«–æ–‡ã§ã™ãŒã€ç°¡å˜ã«å®Ÿè£…ã§ãã€åŠ¹æœãŒé«˜ãã†ãªæ‰‹æ³•ãªæ°—ãŒã—ã¾ã™ã€‚å†ç¾å®Ÿé¨“ã¾ã§ã‚„ã£ãŸã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ä»Šå¾Œã€ã©ã‚“ã©ã‚“ä½¿ã£ã¦ã„ããŸã„æ‰‹æ³•ã§ã™ã€‚

å€‹äººçš„ã«ã€Contrastive Learningã¯ã€å¥½ããªåˆ†é‡ã§ã€ç”»åƒç³»ã§ã™ãŒSimSiamã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã„ã¾ã™ã€‚ä»Šå¾Œã‚‚ã€ã“ã®åˆ†é‡ã«ã¯ç€ç›®ã—ã¦ã„ããŸã„ã§ã™ã­ã€‚

