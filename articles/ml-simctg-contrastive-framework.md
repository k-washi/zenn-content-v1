---
title: "SimCTG: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãŠã‘ã‚‹Contrastiveå­¦ç¿’æ¨è«–ã®è§£èª¬ãƒ»å®Ÿè£…" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ˜¸" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["æ©Ÿæ¢°å­¦ç¿’", "python"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: false # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
---

# ã¯ã˜ã‚ã«

ã“ã‚“ã«ã¡ã¯ã€ã‚ã£ã—ãƒ¼ã§ã™ã€‚
ç”»åƒãƒ»è‡ªç„¶è¨€èªãƒ»éŸ³å£°ã«é–¢ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ç ”ç©¶é–‹ç™ºã‚„MLOpsã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã€æ©Ÿæ¢°å­¦ç¿’ã«é–¢ã—ã¦ã€ã”ç›¸è«‡ãŒã‚ã‚Œã°ã€[@kwashizzz](https://twitter.com/kwashizzz)ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¾ã§DMã—ã¦ãã ã•ã„ï¼
ã“ã‚Œã¾ã§ã®ã€[æ©Ÿæ¢°å­¦ç¿’è¨˜äº‹ã®ã¾ã¨ã‚](https://zenn.dev/kwashizzz/articles/my-ml-articles-summary)ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€æ–‡ç« ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ä¸è‡ªç„¶ã•ã‚„ã€æœ›ã¾ã—ããªã„å˜èªã®ç¹°ã‚Šè¿”ã—ã‚’æŠ‘ãˆã‚‹æ‰‹æ³•ã¨ã—ã¦ã€[A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)ã§ææ¡ˆã•ã‚ŒãŸSimCTG(a **SIM**ple **C**ontrastive framework for neural **T**ext **G**eneration)ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

ã¾ãŸã€[è«–æ–‡ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰](https://github.com/yxuansu/SimCTG/tree/bb54480e5c43d62d5b660d5cdabaee7c7d7af442)ã‚’å‚è€ƒã«ã—ã€Encoder-Decoderå½¢å¼ã®æ–‡ç« ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹T5ã«SimCTGã‚’é©ç”¨ã—ã¦ã¿ãŸã®ã§ã€å®Ÿè£…æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

â€» [è«–æ–‡ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰](https://github.com/yxuansu/SimCTG/tree/bb54480e5c43d62d5b660d5cdabaee7c7d7af442)ã«ã¯ã€GPT-2ã«é©ç”¨ã—ãŸå®Ÿè£…ãŒã‚ã‚‹ã®ã§ã€GPT-2ã‚’ä½¿ã„ãŸã„äººã¯ã€[ã“ã¡ã‚‰](https://github.com/yxuansu/SimCTG/tree/bb54480e5c43d62d5b660d5cdabaee7c7d7af442)ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

# ãªãœSimCTGãŒå¿…è¦ãªã®ã‹?

> æ˜¨ä»Šã®Decoderã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å…¥åŠ›æ–‡ï¼‹éå»ã«ç”Ÿæˆã—ãŸå˜èªåˆ—ã‹ã‚‰æ¬¡ã®å˜èªåˆ—ã‚’äºˆæ¸¬ã™ã‚‹Auto-Regressiveãªç”ŸæˆãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®éš›ã«ã€è¨ˆç®—é‡ã®é–¢ä¿‚ã‹ã‚‰ã€Gready Searchã‚„Beam Searchãªã©ãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

å‚è€ƒ: [ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãŠã‘ã‚‹ decoding ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯: Greedy search, Beam search, Top-K, Top-p
](https://zenn.dev/hellorusk/articles/1c0bef15057b1d)

ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®æ¢ç´¢æ‰‹æ³•ã‚’ç”¨ã„ãŸå ´åˆã€åŒã˜å˜èªã‚’ç¹°ã‚Šè¿”ã™ãªã©ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚ãã“ã§ã€`huggingface`ã®æ–‡ç« ç”Ÿæˆé–¢æ•°`generete`ã§ã¯ã€`no_repeat_ngram_size`ã‚„`repetition_penalty`ã¨ã„ã†å¼•æ•°ãŒã‚ã‚Šã€ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ç¹°ã‚Šè¿”ã—ã‚’æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ã¯ã€åŒã˜å˜èªåˆ—ãŒå…¨ãå‡ºãªããªã‚Šä¸è‡ªç„¶ãªç”Ÿæˆã«ãªã£ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ä¸‹å›³(è«–æ–‡ä¸­Fig.1)ã®(a)ã¯ã€GPT-2ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸå„ãƒˆãƒ¼ã‚¯ãƒ³ã«ãŠã‘ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«(Transformeræœ€çµ‚å±¤)ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã§ã™ã€‚è¦‹ã¦ã®é€šã‚Šã€æ–‡ä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é¡ä¼¼åº¦ãŒ0.95ä»¥ä¸Šã«ãªã£ã¦ãŠã‚Šã€äº’ã„ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒè¿‘ã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã§ç¹°ã‚Šè¿”ã—ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ã¦ã—ã¾ã†åŸå› ã®ä¸€ã¤ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚è«–æ–‡ä¸­ã§ã¯ã€ã“ã®ã‚ˆã†ã«ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒåã£ã¦ã„ã‚‹ã“ã¨ã‚’ç•°æ–¹æ€§ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚

![Fig.1](https://storage.googleapis.com/zenn-user-upload/c51f3beef16d-20220313.png)

ç†æƒ³çš„ã«ã¯ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«é–“ã‚’è­˜åˆ¥å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ä¸Šå›³(b)ã®ã‚ˆã†ã«ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é¡ä¼¼åº¦ãŒä½ããªã‚‹ã‚ˆã†ã«ã™ã¹ãã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã¤ã¾ã‚Šã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯åã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ï¼ˆç­‰æ–¹æ€§ã«ã™ã¹ãã€‚ï¼‰

ãã“ã§ã€æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã§è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã„ãã¨ã„ã†å¾“æ¥æ‰‹æ³•ã«å¯¾ã—ã¦ã€è­˜åˆ¥å¯èƒ½ã§ç­‰æ–¹çš„ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®å­¦ç¿’ã‚’ä¿ƒã™SimCTGæå¤±ã‚’ææ¡ˆã—ã€åŠ ãˆã¦ã€SimCTGã«ã‚ˆã‚‹å­¦ç¿’ã‚’è£œå®Œã™ã‚‹æ–°ã—ã„å¾©å·åŒ–æ‰‹æ³•ã§ã‚ã‚‹Contrastive Search (å¯¾ç…§æ¢ç´¢)ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

# SimCTG

SimCTGã¯ã€æœ€å°¤æ¨å®šã®æå¤±ï¼ˆã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±)ã¨å¯¾ç…§æå¤±(Constrastive Loss)ã‹ã‚‰ãªã‚Šã¾ã™ã€‚

å˜èªåˆ—ã‚’$x$ã¨ã—ãŸå ´åˆã€æœ€å°¤æ¨å®šã®æå¤±ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ä¸€ã¤å‰ã¾ã§ã®å˜èªåˆ—($x_{ < i}$)ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹å˜èªãŒ$x_i$ã§ã‚ã‚‹ç¢ºç‡åˆ†å¸ƒ $p_{\theta}(x)$ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªæå¤±$\mathcal{L}_{MLE}$ã§ã™ã€‚

$$
\begin{equation}
\mathcal{L}_{MLE} = - \frac{1}{|x|} \sum_{i=1}^{|x|} \log p_{\theta} (x_i | x_{ < i})
\tag{1}
\end{equation}
$$

æ¬¡ã«å¯¾ç…§æå¤±$\mathcal{L}_{CL}$ã¯ã€é¡ä¼¼åº¦é–¢æ•°$s$ã‚’ç”¨ã„ã¦ã€ä»¥ä¸‹ã®å¼ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€åŒã˜å˜èªã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’é«˜ãã—ã€ç•°ãªã‚‹å˜èªã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’å°ã•ãã™ã‚‹ã‚ˆã†ãªæå¤±ã§ã™ã€‚

$$
\begin{equation}

\mathcal{L}_{CL} = \frac{1}{|x| \times (|x| - 1)} \sum_{i=1}^{|x|} \sum_{j=1, j \neq i}^{|x|} 
\max \{ 0, \rho - s(h_{x_i}, h_{x_i}) + s(h_{x_i}, h_{x_j}) \}

\tag{2}
\end{equation}
$$

ãƒãƒ¼ã‚¸ãƒ³ $\rho \in [-1, 1]$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€$h_{x_i}$ãŒãƒˆãƒ¼ã‚¯ãƒ³($x_i$)ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã§ã™ã€‚é¡ä¼¼åº¦é–¢æ•°ã¯ã€

$$
\begin{equation}

s(h_{x_i}, h_{x_j}) = \frac{h_{x_i}^Th_{x_j}}{||h_{x_i}|| \cdot || h_{x_j} ||}

\tag{3}
\end{equation}
$$

ã§è¨ˆç®—ã§ãã¾ã™ã€‚

ä¸Šè¨˜ã®æå¤±ã‚’ç”¨ã„ã¦SimCTGæå¤±ã¯ã€

$$
\begin{equation}

\mathcal{L}_{CTG} = \mathcal{L}_{MLE} + \mathcal{L}_{CL}

\tag{4}
\end{equation}
$$

ã¨ãªã‚Šã¾ã™ã€‚

ã“ã®SimCTGæå¤±ã‚’ç”¨ã„ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯è¿‘ãã«ãªã‚Šã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯é ããªã‚Šã¾ã™ã€‚

# Contrastive Search

å¯¾ç…§æå¤±ã‚’ç”¨ã„ãŸSimCTGã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹å ´åˆã€ä¸€ã¤å‰ã¾ã§ã®å˜èªåˆ—ã¨ã¯ç•°ãªã‚‹å˜èªã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚ãã®çŸ¥è­˜ã‚’å–ã‚Šå…¥ã‚Œã€å¾“æ¥æ‰‹æ³•ã¨åŒæ§˜ã«ç¢ºç‡ã®é«˜ã„å˜èªã‚’æ¢ç´¢ã™ã‚‹ã“ã¨ã«åŠ ãˆã€ä¸€ã¤å‰ã¾ã§ã®å˜èªåˆ—ã¨é¡ä¼¼åº¦ãŒä½ã„å˜èªã‚’æ¢ç´¢ã™ã‚‹Contrastive SearchãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

äºˆæ¸¬ç¢ºç‡ãŒé«˜ã„$k$å€‹ã®å˜èªé›†åˆã‚’$V^k$ã¨ã—ãŸå ´åˆã®Contastive Searchã«ã‚ˆã‚‹å˜èªé¸æŠã¯ã€

$$
\begin{equation}

x_t = \mathrm{argmax}_{v \in V^k} \{ (1 - \alpha) \times p_{\theta}(v | x_{ < t}) 
- \alpha \times (\mathrm{max} \{ s(h_v, h_{x_j}) : 1 \leq j \leq t - 1 \}) \}

\tag{5}
\end{equation}
$$

ã§è¡Œã‚ã‚Œã¾ã™ã€‚

# çµæœ

è«–æ–‡ã®Table.1ã‚’è¦‹ã¦ãã ã•ã„ã€‚è«–æ–‡é€šã‚Šã®çµæœã ã¨ã€SimCTGã«ã€Contrastive Searchã‚’ä½¿ã£ãŸéš›ã€æœ€å¼·ã§ã™ã€‚ä¸€æ–¹ã€ä¸¡æ‰‹æ³•ã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã¨ã‚‚è¦‹ã¦å–ã‚Œã¾ã™ã€‚

# T5ã«ãŠã‘ã‚‹SimCTGã®å­¦ç¿’ã®å®Ÿè£…

ä»Šå›ã¯ã€[ã€æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ä»˜ãã€‘2021å¹´ã«è‡ªç„¶è¨€èªå‡¦ç†ã‚’ã™ã‚‹äººã«ãŠå‹§ã‚ã—ãŸã„äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«](https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44)ã§æä¾›ã•ã‚Œã¦ã„ã‚‹T5ã®è»¢ç§»å­¦ç¿’ä¾‹ã®[ã‚³ãƒ¼ãƒ‰](https://github.com/sonoisa/t5-japanese)ã‚’ãƒ™ãƒ¼ã‚¹ã«æ‹¡å¼µã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚[ã‚³ãƒ¼ãƒ‰](https://github.com/sonoisa/t5-japanese)ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆä¸€ç¨®ã®æ–‡ç« è¦ç´„ï¼‰ã®ãƒªãƒ³ã‚¯ã‹ã‚‰Google Colabã§ã®è¨“ç·´ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã‚Œã¾ã™ã€‚ã¾ãŸã€æœ¬è¨˜äº‹ã§ã¯ã€ç°¡å˜ã®ãŸã‚ã€ä¿®æ­£ã—ãŸSimCTGã®é‡è¦ãªéƒ¨åˆ†ã®ã¿è¨˜è¼‰ã—ã¾ã™ã€‚

ã¾ãšã€é¡ä¼¼åº¦è¨ˆç®—ã®éƒ¨åˆ†ã§ã™ã€‚ã“ã“ã§ã¯ã€T5ã®ãƒ‡ã‚³ãƒ¼ãƒ€ã®æœ€çµ‚å±¤ã‚’ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒãƒƒãƒ, å˜èªåˆ—æ•°, ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ãªã‚‹è¡Œåˆ—ãªã®ã§ã€å„å˜èªã”ã¨ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€ãƒãƒƒãƒæ•°Ã—å˜èªåˆ—æ•°Ã—å˜èªåˆ—æ•°ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

```python
def t5_cosine_simirality(outputs):
    last_hidden_state = outputs.decoder_hidden_states[-1] # torch.Size([3, 64, 768])
    norm_rep = last_hidden_state / last_hidden_state.norm(dim=2, keepdim=True)
    cosine_scores = torch.matmul(norm_rep, norm_rep.transpose(1,2)) # torch.Size([3, 64, 64])
    return cosine_scores
```

äº‹å‰å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã®é¡ä¼¼åº¦è¡Œåˆ—ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚(é•·ã„ã®ã§æœ€åˆã®5è¡Œ5åˆ—éƒ¨åˆ†ã®ã¿ã§ã™ã€‚)
å®Ÿéš›ã€å¯¾è§’éƒ¨åˆ†ä»¥å¤–ã®é¡ä¼¼åº¦ã‚‚é«˜ã„ã“ã¨ãŒè¦‹ã¦å–ã‚Œã¾ã™ã€‚
```
[0.9999, 0.9807, 0.9654, 0.9587, 0.9528, 
[0.9807, 0.9999, 0.9805, 0.9755, 0.9687,
[0.9654, 0.9805, 0.9999, 0.9816, 0.9712,
[0.9587, 0.9755, 0.9816, 0.9999, 0.9878, 
[0.9528, 0.9687, 0.9712, 0.9878, 0.9999,
```

æ¬¡ã«ã€å¯¾ç…§æå¤±ã®éƒ¨åˆ†ã§ã™ã€‚ã“ã‚Œã¯ã€[è«–æ–‡ã®ã‚³ãƒ¼ãƒ‰](https://github.com/yxuansu/SimCTG/blob/bb54480e5c43d62d5b660d5cdabaee7c7d7af442/pretraining/simctg.py#L50)ã¨åŒã˜ã§ã™ã€‚è«–æ–‡ã‚³ãƒ¼ãƒ‰ã®ã‚¹ã‚¿ãƒ¼ã‚’ãƒãƒƒãƒã£ã¨æŠ¼ã—ã¾ã—ã‚‡ã†ï¼

```python
def compute_contrastive_loss(score_matrix, margin):
    '''
        margin: predefined margin to push similarity score away
        score_matrix: bsz x seqlen x seqlen; cosine similarity matrix
        input_ids: bsz x seqlen
    '''
    bsz, seqlen, _ = score_matrix.size()
    gold_score = torch.diagonal(score_matrix, offset=0, dim1=1, dim2=2) #å¯¾è§’æˆåˆ†ã‚’å–ã‚Šå‡ºã™ã€€bsz x seqlen
    gold_score = torch.unsqueeze(gold_score, -1) 
    assert gold_score.size() == torch.Size([bsz, seqlen, 1])

    difference_matrix = gold_score - score_matrix
    assert difference_matrix.size() == torch.Size([bsz, seqlen, seqlen])
    loss_matrix = margin - difference_matrix # bsz x seqlen x seqlen
    loss_matrix = torch.nn.functional.relu(loss_matrix)
    cl_loss = torch.mean(loss_matrix)
    return cl_loss
```

æ¬¡ã«å­¦ç¿’ã™ã‚‹éƒ¨åˆ†ã§ã™ã€‚å‚è€ƒã«ã•ã›ã¦ã„ãŸã ã„ãŸè»¢ç§»å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã¯Pytorch Lightningã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ãã®ãƒ¢ãƒ‡ãƒ«éƒ¨åˆ†ã®æ‹¡å¼µã§ã™ã€‚
T5ã®éš ã‚Œå±¤ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã«ã€`output_hidden_states=True `ã‚’è¨­å®šã—ã¦ã„ã¾ã™ã€‚`_step`éƒ¨åˆ†ã§æå¤±ã‚’è¿½åŠ ã§è¨ˆç®—ã—ã¦ã„ã¾ã™ã€‚

```python
class T5FineTuner(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        # æœ€æ–°ç‰ˆã®plã§ã¯ã€hparamsã®updateæ–¹æ³•ãŒå¤‰æ›´ã•ã‚ŒãŸã€‚
        self.save_hyperparameters(hparams)
        # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path, is_fast=True)

    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, 
                decoder_attention_mask=None, labels=None):
        """é †ä¼æ¬"""
        return self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            labels=labels,
            output_hidden_states=True # decoderã®éš ã‚Œå±¤ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚Trueã«è¨­å®šã€‚
        )

    def _step(self, batch):
        """ãƒ­ã‚¹è¨ˆç®—"""
        labels = batch["target_ids"]
        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100

        outputs = self(
            input_ids=batch["source_ids"],
            attention_mask=batch["source_mask"],
            decoder_attention_mask=batch['target_mask'],
            labels=labels,
        )

        mle_loss = outputs.loss
        cosine_scores = t5_cosine_simirality(outputs) # é¡ä¼¼åº¦è¨ˆç®—
        margin = 0.5 # margin
        cl_loss = compute_contrastive_loss(cosine_scores, margin) # å¯¾ç…§æå¤±è¨ˆç®—

        loss = mle_loss + cl_loss # SimCTG æå¤±
        loss = loss.mean()
        return loss
```

ä¸Šè¨˜ã®å¤‰æ›´ã‚’è¡Œãªã£ãŸã‚³ãƒ¼ãƒ‰ã§å­¦ç¿’ã—ãŸçµæœã€ä»¥ä¸‹ã®é¡ä¼¼åº¦ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚(åŒæ§˜ã«ã€é•·ã„ã®ã§æœ€åˆã®5è¡Œ5åˆ—éƒ¨åˆ†ã®ã¿ã§ã™ã€‚)
å¯¾è§’æˆåˆ†ã¯ã€é¡ä¼¼åº¦ãŒé«˜ãã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é¡ä¼¼åº¦ã§ã‚ã‚‹å¯¾è§’æˆåˆ†ä»¥å¤–ã¯ã€é¡ä¼¼åº¦ãŒå°ã•ããªã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

```
[0.9999, 0.2982, -0.5032, -0.3650, -0.5413,
[0.2982, 0.9999, 0.2020, 0.2578, 0.2829,
[-0.5032, 0.2020, 0.9999, 0.6498, 0.6361,
[-0.3650, 0.2578, 0.6498, 1.0, 0.6117, 0.7059,
[-0.5413, 0.2829, 0.6361, 0.6117, 0.9999
```

# T5ã«ãŠã‘ã‚‹Contrastive Searchã®å®Ÿè£…

ã¾ãšã€æ¨è«–ã®å¤§ã¾ã‹ãªæµã‚Œã§ã™ã€‚é‡è¦ãªéƒ¨åˆ†ã¯ã€å‰ã®å˜èªåˆ—ã‹ã‚‰æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹`ContrastiveDecodingOneStepFast`ã¨ã„ã†é–¢æ•°ã§ã€å¾Œã§è§£èª¬ã—ã¾ã™ã€‚


```python
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
input_ids = next(iter(data_loader))["source_ids"] # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‹ã‚‰èª­ã¿è¾¼ã¿
batch_size, seqlen = input_ids.size()

generated = [[] for _ in range(batch_size)] # ãƒãƒƒãƒã”ã¨ã®ç”Ÿæˆæ–‡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã§ã™ã€‚
is_eos = [False for _ in range(batch_size)] # ãƒãƒƒãƒã”ã¨ã®æ–‡ç« ç”Ÿæˆçµ‚äº†(EOS)åˆ¤å®šãƒªã‚¹ãƒˆã§ã™ã€‚
# past_key_valuesã‚’ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‚’ä¸€ã¤å‰ã®å˜èªã®ã¿ã«ã§ãã‚‹(huggingfaceã®å®Ÿè£…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦‹ã¦ãã ã•ã„ã€‚)
past_key_values = None 
last_hidden_states = None
logits = None

decoding_len = 512 # æ–‡ç« ã®æœ€å¤§é•·ã•
beam_width = 3 # å®Ÿè£…ã§ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®çµæœã«Contrastive Searchã‚’è¡Œãªã£ã¦ã„ãŸã€‚
alpha = 0.5 # (1 - a) * ç¢ºç‡æœ€å¤§ + a * é¡ä¼¼åº¦

input_ids.to(device)
trained_model.eval()
for step in range(decoding_len):
    next_ids, past_key_values, last_hidden_states, logits = ContrastiveDecodingOneStepFast(
        trained_model,
        input_ids,
        beam_width,
        alpha,
        past_key_values,
        last_hidden_states,
        tokenizer,
        logits,
        device,
        first_step=step == 0, #æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿æ‰±ã„é•ã†
    )
    tokens = next_ids.squeeze(dim=-1).tolist()
    for idx, t in enumerate(tokens): #ãƒãƒƒãƒæ•°åˆ†
        # EOSã®å ´åˆã‚¹ã‚­ãƒƒãƒ—
        if is_eos[idx]:
            continue
        if t == tokenizer.eos_token_id:
            is_eos[idx] = True
            continue
        generated[idx].append(t)
```

æ¬¡ã«ã€`ContrastiveDecodingOneStepFast`ã®å®Ÿè£…ã§ã™ã€‚è«–æ–‡ã®å®Ÿè£…ã¯ã€GPT-2ç‰ˆã§ã™ãŒã€T5ã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¢ãƒ‡ãƒ«ãªã®ã§ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã¾ãŸã€GPUä½¿ç”¨ã—ã¦ã„ãŸéš›ã«ã€ãƒ¡ãƒ¢ãƒªãƒ¼ã‚¨ãƒ©ãƒ¼ã«ãªã£ã¦ã„ãŸã®ã§ã€ãã®å¯¾å‡¦ã‚’å…¥ã‚Œã¦ã„ã¾ã™ã€‚[è«–æ–‡å®Ÿè£…ã¯ã“ã®éƒ¨åˆ†](https://github.com/yxuansu/SimCTG/blob/bb54480e5c43d62d5b660d5cdabaee7c7d7af442/document_generation/utlis.py#L126)ã«ãªã‚Šã¾ã™ã€‚ã»ã¨ã‚“ã©è«–æ–‡ã®å®Ÿè£…ã¨åŒã˜ã§ã™ã€‚


```python
def ContrastiveDecodingOneStepFast(
    model, 
    ids, 
    beam_width, 
    alpha, 
    past_key_values,
    last_hidden_states,
    vocab,
    logit_for_next_step,
    device,
    first_step=False,
    
    ):
    # input_ids: [B, S]
    model.eval()
    if first_step:
        # T5ã§ã¯ã€æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿pad tokenã‚’ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ã¨ã™ã‚‹ã€‚
        with torch.no_grad():
            bsz, _ = ids.size()
            ids = ids.to(device)
            decoder_inputs = torch.tensor([[0] for _ in range(bsz)]).to(device) # pad_token_idã§start
            output = model(
                input_ids=ids, 
                decoder_input_ids=decoder_inputs,
                past_key_values=past_key_values,
                use_cache=True,
                output_hidden_states=True
            )
            del decoder_inputs
        past_key_values = output.past_key_values
        last_hidden_states = output.decoder_hidden_states[-1].cpu()    # [B, S, E]
        logit_for_next_step = output.logits[:, -1, :].cpu()    # [B, V]
    bsz, seqlen, embed_dim = last_hidden_states.size()
    p = random.uniform(0, 1)

    next_probs = F.softmax(logit_for_next_step, dim=-1)

    # æœ€å¤§ç¢ºç‡ã®kå€™è£œã‚’å–å¾—
    _, top_k_ids = torch.topk(logit_for_next_step, dim=-1, k=beam_width) # [B, V:38000ãã‚‰ã„] -> [B, K:beam_width]
    top_k_probs = torch.gather(next_probs, dim=1, index=top_k_ids) # å€™è£œã®ç¢ºç‡ã‚’å–å¾— [B, K] 
    
    # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã®past_keyã‚’ãƒãƒƒãƒxå€™è£œã®ã‚¿ãƒ—ãƒ«ã«ä¿®æ­£
    past_key_values = enlarge_past_key_values(past_key_values, beam_width)

    # æ¬¡ã®å˜èªã‚’äºˆæ¸¬
    input_ids = top_k_ids.view(-1, 1).to(device) # [B*K , 1]
    with torch.no_grad():
        output = model(
            input_ids=input_ids, 
            decoder_input_ids=input_ids,
            past_key_values=past_key_values,
            output_hidden_states=True,
            use_cache=True,
        )
    past_key_values = output.past_key_values
    logits = output.logits[:, -1, :].cpu()    # [B*K, V]
    next_hidden = output.decoder_hidden_states[-1].cpu()    # [B*K, 1, E]
    context_hidden = last_hidden_states.unsqueeze(1).expand(-1, beam_width, -1, -1).reshape(bsz*beam_width, seqlen, embed_dim)    # [B*K, S, E]

    # ãƒãƒƒãƒã”ã¨ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã®å˜èªã‚’é¸æŠ
    # å®Ÿéš›ã®å¼ã®éƒ¨åˆ†ã‚’è¨ˆç®— (ä¸‹ã®é–¢æ•°)
    selected_idx = ranking_fast(
        context_hidden, 
        next_hidden, 
        top_k_probs,
        alpha,
        beam_width,
    )  # [B]

    # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®æº–å‚™
    next_id = top_k_ids[range(len(top_k_ids)), selected_idx].unsqueeze(-1)    # [B, 1]
    next_hidden = torch.stack(torch.split(next_hidden.squeeze(dim=1), beam_width))    # [B, K, E]
    next_hidden = next_hidden[range(bsz), selected_idx, :]    # [B, E]
    last_hidden_states = torch.cat([last_hidden_states, next_hidden.unsqueeze(1)], dim=1)    # [B, S, E]
    past_key_values = select_past_key_values(past_key_values, beam_width, selected_idx)
    logits = torch.stack(torch.split(logits, beam_width))[range(bsz), selected_idx, :]    # [B, V]
    
    # GPUã®ãƒ¡ãƒ¢ãƒªè§£æ”¾
    if device != torch.device("cpu"):
        del model, input_ids, top_k_ids, top_k_probs
        torch.cuda.empty_cache()
    return next_id, past_key_values, last_hidden_states, logits 


def ranking_fast(context_hidden, next_hidden, next_top_k_probs, alpha, beam_width):
    '''ãƒãƒƒãƒã”ã¨ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã®å˜èªã‚’é¸æŠ
        context_hidden: bsz*beam x seqlen x embed_dim
        next_hidden: bsz*beam x 1 x embed_dim
        next_top_k_probs: bsz x beam
    '''
    _, context_len, embed_dim = context_hidden.size()
    norm_context_hidden = context_hidden / context_hidden.norm(dim=2, keepdim=True)
    norm_next_hidden = next_hidden / next_hidden.norm(dim=2, keepdim=True)
    cosine_matrix = torch.matmul(norm_context_hidden, norm_next_hidden.transpose(1,2)).squeeze(-1)    # [B*K, S]
    scores, _ = torch.max(cosine_matrix, dim=-1)    # [B*K]
    next_top_k_probs = next_top_k_probs.view(-1)    # [B*K]
    # å¼ã®éƒ¨åˆ†
    scores = (1.0 - alpha) * next_top_k_probs - alpha * scores #
    scores = torch.stack(torch.split(scores, beam_width)) # ãƒãƒƒãƒã”ã¨ã«æˆ»ã™ [B, K]
    selected_idx = scores.max(dim=-1)[1] # [B]
    return selected_idx

def enlarge_past_key_values(past_key_values, beam_width):
    # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã®past_keyã‚’ãƒãƒƒãƒxå€™è£œã®ã‚¿ãƒ—ãƒ«ã«ä¿®æ­£
    # from [B, num_head, seq_len, esz] to [B*K, num_head, seq_len, esz]
    new_key_values = []
    for layer in past_key_values:
        items = []
        for item in layer:
            # item is the key and value matrix
            bsz, num_head, seq_len, esz = item.size()
            item = item.unsqueeze(1).expand(-1, beam_width, -1, -1, -1).reshape(bsz*beam_width, num_head, seq_len, esz)    # [bsz*beam, num_head, seq_len, esz]
            items.append(item)
        new_key_values.append(items)
    return new_key_values

def select_past_key_values(past_key_values, beam_width, selected_idx):
    '''ãƒãƒƒãƒxå€™è£œã‹ã‚‰æœ€å¤§ã‚¹ã‚³ã‚¢ã®past_keyã‚’é¸æŠ
    select_idx: [B]
    '''
    new_key_values = []
    for layer in past_key_values:
        items = []
        for item in layer:
            bsz_and_beam, num_head, seq_len, esz = item.size()
            bsz = int(bsz_and_beam//beam_width)
            item = torch.stack(torch.split(item, beam_width, dim=0))    # [B, K, num_head, seq_len, esz] 
            item = item[range(bsz), selected_idx, :, :, :]   # [B, num_head, seq_len, esz]
            items.append(item)
        new_key_values.append(items)
    return new_key_values
```

ä»¥ä¸Šã€T5ã®Consrastive Searchå®Ÿè£…ã«ãªã‚Šã¾ã™ã€‚

# ã¾ã¨ã‚

æ–‡ç« ç”Ÿæˆã®æ”¹å–„æ‰‹æ³•ã‚’æ¢ã—ã¦ã„ã¦ã€å¶ç„¶è¦‹ã¤ã‘ãŸè«–æ–‡ã§ã™ãŒã€ç°¡å˜ã«å®Ÿè£…ã§ãã€åŠ¹æœãŒé«˜ãã†ãªæ‰‹æ³•ãªæ°—ãŒã—ã¾ã™ã€‚
å†ç¾å®Ÿé¨“ã¾ã§ã‚„ã£ãŸã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ä»Šå¾Œã€ã©ã‚“ã©ã‚“ä½¿ã£ã¦ã„ããŸã„æ‰‹æ³•ã§ã™ã€‚

å€‹äººçš„ã«ã€Contrastive Learningã¯ã€å¥½ããªåˆ†é‡ã§ã€ç”»åƒç³»ã§ã™ãŒSimSiamã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã„ã¾ã™ã€‚ä»Šå¾Œã‚‚ã€ã“ã®åˆ†é‡ã«ã¯ç€ç›®ã—ã¦ã„ããŸã„ã§ã™ã­ã€‚

