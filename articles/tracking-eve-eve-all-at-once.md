---
title: "OmniMotion - Tracking Everything Everywhere All at Once の解説" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習", "論文解説", "NeRF"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
publication_name: "fusic"
---

こんにちは！[Fusic](https://fusic.co.jp/) 機械学習チームの鷲崎です。機械学習モデルの開発から運用までなんでもしています。もし、機械学習で困っていることがあれば、気軽に[DM](https://twitter.com/kwashizzz)ください。

本記事では、[Tracking Everything Everywhere All at Once](https://arxiv.org/abs/2306.05422) という論文で提案された、オプティカルフローのように、動画における各ピクセル間の軌跡を推定する新しい手法であるOmniMotoinについて解説します。
まず、動画を見たほうがイメージがつくと思います。

https://www.youtube.com/watch?v=KHoAG3gA024

タスクとしては、画像中のピクセルの長距離（長いフレーム間）の追跡というモーション推定です。

ぱっとみ、すごい点として、

- オクルージョンをうまく回避している
- グローバルな一貫性を保った追跡ができている: 例えば、人が動作によって見え方が変わった場合でも、一貫性を保って追跡できている

があると思います。


# 課題と提案内容

モーション推定の手法は、スパース特徴追跡と、Dense（密）なオプティカルフロー推定の2つがあります。スパース特徴追跡は、画像中の特徴点を追跡するので、長距離の追跡が可能ですが、剛体に限定することが多く、また、追跡対象は、物体の特徴点に限定されます。オプティカルフローのイメージは、[映像の動き推定のためのオプティカルフロー](https://jp.mathworks.com/discovery/optical-flow.html)でわかるかと思います。オプティカルフローは、特徴点に限らず密なピクセル追跡が可能ですが、オクルージョン時に追跡が困難になったり、長距離の追跡を行った場合に、ドリフトが発生するという欠点があります。

それらの問題を事後的に解決する手法として、テスト時に動画像全体に対して最適化を行うOmniMotionが提案されました。入力値は、画像フレームの集合と、オプティカルフローなどによるノイジーなモーション推定結果で、これらを用いて、動画全体に対するグローバルに一貫したmotion representation (運動表現)を獲得しています。結果として、オクルージョンなどが発生した場合などでも、参照しているピクセルのコンテンツが消えたのか認識し、オクルージョンを通して点を追跡可能なものとなっているそうです。


# OmniMotion Reqresentatoin

Ommni Motionを説明する上で重要な概念が下図 (論文中 Figure. 2)のOmnimotion Reqresentationです。ぱっとみですが、Canonical 3D Volumeという時空間表現を介して、異なるフレームにおいて表現される座標空間をマッピング（3D bijections）している事がわかります。


![](https://storage.googleapis.com/zenn-user-upload/f373b87633b4-20230612.png)


ここで疑問なのが、なぜ`Canonical`なのかです。現実世界の動的なシーンを時空間表現するためには、シーンの形状、カメラの姿勢、動的な情報を分離する必要があります。しかし、OmniMotionでは、カメラの動きとシーンの動的情報の明示的な分離を行わず、正確な3Dシーンの表現ではないが、動的なシーン表現の曖昧さを回避した時間に依存しない空間を表現しています。この正確ではない点が`Canonical`とされている理由です。

しかし、正確な3Dシーン表現ではなくとも、NeRFと同様に、座標ネットワーク$F_{\theta}$を用いて、Canonical 3D空間上の座標$u$における密度$\sigma$と色$c$に対応させています。


また、3D bijectionsという、時間が異なるフレーム間のローカル3D座標を、Canonical 3D Volumeを介した双射写像を行っています。 ローカル座標の3次元点($x_i$)をCanonical 3D volumeに写像する、可逆な写像モデル$\Tau$をもちいて、$x_j = \Tau^{-1}_j \circ \Tau_i (x_i)$ のように、異なる時間フレームの3次元空間上の同一点を結びつけています。


3D bijectionsが可能な場合、Canonical 3D Volumeに格納された色情報$c$を用いて、最適化時に、photometric lossを計算できる。また、オクルージョンの関係は、Canonical 3D Volumeに格納された密度$\sigma＄より得られる表面位置の情報を駆使することで推論が可能となる。

上記より、異なるフレーム間のローカル3次元空間上は、変換できることがわかる。しかし、上図(a)のように、画像中の1ピクセルに対して、ローカル3次元上の点はRay(直線)上に無数に存在する。そこで、上図(b)のように、Ray上の点を別フレームの点に変換し、別フレーム上のあるピクセルのRay上の3次元点を重ね合わせている。そして、これらの結果を用いて、2Dの点に射影している。この重ね合わせ処理は、NeRFにおけるサンプル点の色情報の集計方法とどうようであり、以下の式で計算できる。簡単に言えば、密度によって、重ね合わせの強さを変えているだけである。

$\hat{x_j} = \sum_{k=1}^K \Tau_k \alpha_k x_j^k$  
$, where \  \Tau_k = \prod_{l=1}^{k-1} \rm{exp}(-\sigma_l)$


# 最適化方法

ここでは、上記の空間表現に変換するモデルの最適化に関して説明する。

まずは、損失関数を、ざっと説明する。

OmniMotion Representationで述べた、写像で得られる点から計算できるフロー$\hat{f}_{i \rightarrow l} = \hat{p}_j - p_i$と、入力であるモーション推定結果フロー$f_{i \rightarrow l}$ の平均絶対誤差$L_{flo}$をフロー損失としている。
加えて、予測した色と実際の色のphotometric loss $L_{pho}$、３次元空間上の軌跡を時間的に平滑化するための正則化項として、$i+1$と$i-1$における3次元点$x_{i+1}, x_{i-1}$と$i$番目のフレームにおける3次元点$x_i$を比較した3次元空間上の加速度を最小化する損失$L_{leg}$を用いる。


$L_{leg} = \sum_{(i, p) \in \Omega_p} ||x_{i+1} + x_{i-1} - 2 x_i ||$

しかし、これらの損失で学習したとしても、学習しやすい背景の動きを学習してしまい、動的な物体などの不確かさを含む物体の学習は、無視される事になってしまう。この問題に対処するため、学習時に困難な例をマイニングする、学習戦略として、予測フローと入力フローのユークリッド距離の差が大きい領域程、頻繁にサンプリングするようにしていた。

# 評価

評価用に長い動画の点追跡の性能評価用のベンチマーク[TAP-Vid: A Benchmark for Tracking Any Point in a Video](https://github.com/deepmind/tapnet) が用いられていた。とりあえず、ベンチマークに含まれる実動画、合成動画いずれにおいても、SoTAだった。

下図（論文中Figure.3)からも、オクルージョンやズームが発生する長時間のピクセルの追跡を達成できていることがわかる。

![](https://storage.googleapis.com/zenn-user-upload/6d7f54f552be-20230612.png)

一方で、論文の結論に書かれているが、急速な非剛体的なモーションに苦戦していたとのこと。（これは、他の手法も同様 + そもそも、入力となるフロー情報が信頼できない）また、計算コストがかなりかかっているという欠点があると指摘されている。