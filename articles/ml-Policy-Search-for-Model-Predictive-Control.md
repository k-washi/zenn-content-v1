---
title: "【論文読み】Policy Search for Model Predictive Control with Application to Agile Drone Flight" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["ml"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
---

ポリシーサーチとモデル予測制御（MPC）は、ロボット制御のための2つの異なるパラダイムです。

ポリシーサーチは、経験豊富なデータを用いて複雑なポリシーを自動的に学習するという強みがあり、MPCはモデルと軌道最適化を用いて最適な制御性能を提供することができます。

本研究では、MPCのための高レベルの決定変数を自動的に選択するためにポリシーサーチを使用することで、その答えを提供し、新しいMPC用のポリシーサーチ・フレームワークを導き出します。具体的には，MPCをパラメータ化されたコントローラとして定式化し，最適化が困難な決定変数を高レベルのポリシーとして表現します．

このような定式化により、自己監視型の方法でポリシーを最適化することができます。このフレームワークを、高速で移動するゲートを通過するようにクアドロターを飛行させるという、機動的なドローン飛行における困難な問題に焦点を当てて検証しました。実験によると、本研究で開発したコントローラは、シミュレーションと実世界の両方において、ロバストでリアルタイムな制御性能を達成している。提案されたフレームワークは、学習と制御を融合するための新たな視点を提供します。

[youtube](https://www.youtube.com/watch?v=Qei7oGiEIxY) より、実際の道を見ることができます。

[![youtube](https://img.youtube.com/vi/Qei7oGiEIxY/0.jpg)](https://www.youtube.com/watch?v=Qei7oGiEIxY)

モバイルロボットは、ダイナミックな世界で活動しています。特にクアッドローターは、人間が近づけないような非常に複雑でダイナミックな環境でも、高速で移動することができる俊敏なロボットです。

しかし、動的な障害物のような急激な環境変化は、車両の制御に根本的な問題を引き起こす可能性があります。


動的環境下でドローンを俊敏に飛行させるためには、環境の変化に応じて車両の軌道を迅速に適応させることが不可欠です。最新のモデルベースのアプローチは，静的環境と動的環境の両方でクアドローターを制御するのに効果的であることが示されている[1]-[10]．例えば，ドローンレース[11]-[13]では，ドローンは（小さな外乱を受けた）静的なゲートの連続を超高速で飛行する必要があります．

モデル予測制御（MPC）[14]は，複雑なクアドローター制御問題を解決するための強力なモデルベースのアプローチであることが示されています[3]，[15]-[18]．例えば，知覚を考慮したMPC [15]は，計画と知覚の両方の目的を統一するフレームワークです．MPCは，複雑な非線形力学系を同時に扱うことができ，さまざまな状態や入力制約を満たすことができるため，多くのロボット分野で人気が高まっています．

このような成功を収めているにもかかわらず、多くのMPCアプリケーションでは、正確な数学モデルが必要であることや、小規模なコンピュータの限られた計算能力で軌道最適化問題をオンラインで解く必要があることなど、大きな課題があります。実際には，特定のタスクに対するMPCの閉ループ性能は，コスト関数の定式化，ハイパーパラメータ，予測水平線など，いくつかの設計上の選択に影響されます．そのため、一連の近似、ヒューリスティック、パラメータチューニングが行われ、最適ではないソリューションが生み出されています。

一方，強化学習（RL）[19]手法は，政策探索と同様に，タスクに関する最小限の事前知識で連続制御問題を解くことができる．RLの主なアイデアは、試行錯誤によってポリシーを自動的に学習し、与えられた報酬関数で測定されるタスクのパフォーマンスを最大化することである。RLは広範なロボット制御タスクを解決する上で素晴らしい結果を達成しているが[20]-[23]、RLを用いて訓練されたエンド・ツー・エンドのコントローラの解釈可能性が欠如していることが、制御コミュニティの大きな懸念となっている[24]。

理想的な制御フレームワークは、MPCのようなモデルベースコントローラが、ダイナミックモデリングと最適化において確立された知識を用いて物理的なロボットを安全に制御する能力と、経験豊富なデータを用いて複雑なポリシーを自動的に学習するRLの能力という、両方の手法の利点を組み合わせることができるものでなければならない。そのため、大規模な入力に対応し、人間によるループ設計やチューニングを削減し、最終的には適応的で最適な制御性能を実現することができます。

しかし、このような利点があるにもかかわらず、このようなシステムを設計することは非常に困難です。そのため、学習分野では、モデルプライアーを用いたデータ効率の良いポリシー探索手法の開発に焦点を当てた研究が行われている。例えば、ガイド付きポリシー探索（GPS）アルゴリズム[25]-[27]は、RLを教師付き学習問題に変換することを選択する。GPSでは、軌道最適化アルゴリズムを用いて学習データを収集し、教師付き学習によってニューラルネットワークを学習することが重要なアイデアとなっている。

しかし、これらの手法では、ブラックボックス化した制御ポリシーを学習するため、一般化が困難である。第2のトレンドは，学習ベースのMPC [28]-[31]に関するもので，実世界のデータを活用してダイナミクスモデリングを改善し，最適化にモデル予測経路積分制御（MPPI） [32]を使用することができます．このようなアルゴリズムは一般的に確率最適制御にルーツがあり，最適化のために大量のデータをリアルタイムでサンプリングする必要があるため，計算コストが高くなります．

## contribution

本研究では，学習と制御を融合させるための新しいパラダイムとして，ポリシーサーチを用いたモデル予測制御のためのハイレベルなポリシーの学習を提案する．

我々のアプローチの概要を図1に示します。具体的には，MPCをパラメータ化されたコントローラとみなし，MPCのハイレベルな決定変数の探索を確率的なポリシー探索問題として定式化する．

まず，高レベルの決定変数をモデル化するために2つの一般的なガウス政策を使用し，政策の更新が閉形式解を持つことを示す．

次に、ニューラルネットワークの政策を学習するための自己教師付き学習法を提案する。我々の重要な洞察は、政策探索がMPCのハイレベルな決定を行うのに有効であり、最適化が困難なパラメータを自動的に学習し適応させることができるということである。

実験面では、ダイナミックな環境下でのドローンの自律的な飛行に向けた困難な問題に取り組むことで、我々のアプローチを評価しました。標準的なMPC方式と比較した場合の本アプローチの主な利点は、他の状態変数と同時に最適化することが困難な所望の移動時間をオフラインで学習し、実行時に適応的に選択できることです。その結果、学習したニューラルネットワークポリシーとMPCで構成される制御装置は、物理的なドローンのリアルタイム制御性能を実現しました。

実際の実験の様子を図2に示します。本研究は，我々が以前に発表した学会論文[33]を発展させたものです．以前の論文では，ガウス型およびニューラルネットワーク型の政策を学習することを提案し，動的なゲートを通過してクアドロトールを飛行させるための単一の時間変数を学習することをシミュレーションで実証した．

本論文では、さらに、
1）ガウス線形ポリシーを学習するための新しいアルゴリズムを紹介し、
2）我々のアプローチが、単一の変数だけでなく、多次元の決定変数を学習できる一般的なフレームワークであることを示し、
3）我々のコントローラがシミュレーションで複数のゲートを通過するようにドローンを制御でき、標準的なMPCや軌道サンプリング法よりも優れていることを示し、4）物理的なドローンにアルゴリズムを展開し、学習されたニューラルネットワークのハイレベルポリシーが微調整なしで実世界に移行できること
を示した。

### policy search

ポリシーサーチ[19]は，強化学習の中心的な分野であり，サンプリングされた軌道の期待リターンを最大化することで，最適なパラメトリックポリシーを見つける方法に関するものである．確率的な軌道生成のための探索戦略によって，政策探索法はステップベースの方法とエピソードベースの方法に分類される[19]，[34]．政策探索法のほとんどのバリエーションは、各制御時間ステップで行動空間に異なる探索ノイズを追加することで、ステップベースの探索戦略を利用している。ステップベースの政策探索アルゴリズム[35]-[38]は，脚式ロボットの俊敏な運動スキルの学習[39]から，シミュレートされたレーシングカーの摩擦限界の制御[22]まで，連続的な制御タスクに広く用いられている．これらの手法では，観測値を制御コマンドに直接対応させることができるエンドツーエンドのブラックボックス制御ポリシーを学習する．これに対して，エピソードベースの政策探索法[34]，[40]-[42]は，政策のパラメータ空間に探索ノイズをエピソードの最初にのみ加える．特に、エピソードベースの手法は、ロボットの制御ポリシーのコンパクトなパラメータ化である動作プリミティブ[43]-[45]の学習に広く用いられている。例えば，タスク・パラメータ化されたダイナミック・モーター・プリミティブ（DMP）[43]，[46]は，ロボット工学において人気のあるコンパクトなポリシー表現である．DMPのパラメータを調整することで，ロボットは新しいスキルを素早く学習し，Baseball[47]，Ball-in-the-cup[48]，Table Tennis[49]のような多くの困難なロボット制御問題を解決することができる．エピソー ドに基づく方針探索法は，人間の専門家がモデル化するのが容易ではないコンパクトなスキル表現を学習するのに役立つ．



