---
title: "CIDER: åˆ†å¸ƒå¤–(OOD)ã«é©ã—ãŸåŸ‹ã‚è¾¼ã¿ã¨ã¯ï¼Ÿ" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ˜¸" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["æ©Ÿæ¢°å­¦ç¿’", "è«–æ–‡è§£èª¬", "OOD"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: true # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
publication_name: "fusic"
---

ã“ã‚“ã«ã¡ã¯ï¼[Fusic](https://fusic.co.jp/) æ©Ÿæ¢°å­¦ç¿’ãƒãƒ¼ãƒ ã®é·²å´ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‹ã‚‰é‹ç”¨ã¾ã§ãªã‚“ã§ã‚‚ã—ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã€æ©Ÿæ¢°å­¦ç¿’ã§å›°ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€æ°—è»½ã«[DM](https://twitter.com/kwashizzz)ãã ã•ã„ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€[CIDER: Exploiting Hyperspherical Embeddings for Out-of-Distribution Detection](https://ai.papers.bar/paper/fde80d32a02311ecbb8c3d25c114d5ed) ã¨ã„ã†ã€Out-of-Distribution (OOD)ã®æ¤œå‡ºã¨ã„ã†ã‚¿ã‚¹ã‚¯ã§ã€hyperspherical space(è¶…çƒå½¢ç©ºé–“)ã¸ã®åŸ‹ã‚è¾¼ã¿ã‚’åˆ©ç”¨ã™ã‚‹æ–°ã—ã„è¡¨ç¾å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦è§£èª¬ã—ã¾ã™ã€‚

OODã¨ã„ã†ã‚¿ã‚¹ã‚¯ã¯ã€ä¾‹ãˆã°ã€ç”»åƒåˆ†é¡ã«ãŠã„ã¦ã€è¨“ç·´ã«ä½¿ç”¨ã—ã¦ã„ãªã„ã‚¯ãƒ©ã‚¹ãŒæ¨è«–ã«å…¥åŠ›ã•ã‚ŒãŸéš›ã«ã€åˆ†å¸ƒå¤–ã®å…¥åŠ›ã¨ã—ã¦æ¤œçŸ¥ã™ã‚‹ã¨ã„ã†ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚åŸºæœ¬çš„ã«ã¯ã€è·é›¢ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®æ€§èƒ½ãŒã‚ˆãã€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå¾—ã‚‰ã‚Œã‚‹ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã‚’åˆ©ç”¨ã—ã€IDãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’æ™‚ã«ä½¿ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ã€OODã®ã‚µãƒ³ãƒ—ãƒ«ãŒæ¯”è¼ƒçš„é›¢ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ä»®å®šã—ã¦OODãŒè¡Œã‚ã‚Œã¾ã™ã€‚ã“ã®è«–æ–‡ã§è«–ã˜ã¦ã„ã‚‹è·é›¢ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®ã¿ã§ã¯ã€ã©ã®ãã‚‰ã„ã®é–¾å€¤ã‚’è¨­å®šã™ã‚Œã°ã‚ˆã„ã®ã‹ãªã©ã€è·é›¢ã®æ­£è¦åŒ–ãŒå›°é›£ã§ã‚ã£ãŸã‚Šã™ã‚‹ãŸã‚ã€[VOS: Learning What You Don't Know by Virtual Outlier Synthesis](https://openreview.net/forum?id=TW7d65uYu5M)ã®ã‚ˆã†ã«ã€IDãƒ‡ãƒ¼ã‚¿ã®å„ã‚¯ãƒ©ã‚¹ã®ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã‚’æ­£è¦åˆ†å¸ƒã¨ä»®å®šã—ã€OODã®é–¾å€¤è¨­å®šã‚’å®¹æ˜“ã«ã™ã‚‹æ–¹æ³•ãªã©ã‚‚ã‚ã‚Šã¾ã™ã€‚

[VOS: Learning What You Don't Know by Virtual Outlier Synthesis ã®è§£èª¬ãƒ»å®Ÿè£…](https://zenn.dev/kwashizzz/articles/ml-vos-ood-det) ã®è¨˜äº‹ã§è§£èª¬ã—ã¦ã„ã¾ã™ãŒã€ã“ã®æ–¹æ³•ã¯ã€è«–æ–‡ã®æœ¬ç­‹ã¨ã¯é–¢ä¿‚ã‚ã‚Šã¾ã›ã‚“...

ã“ã®è«–æ–‡ã§è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹èª²é¡Œã¯ã€æ—¢å­˜ã®æ‰‹æ³•(Contrastive Lossã‚’ç”¨ã„ãŸæ‰‹æ³•ãªã©)ã¯ã€IDãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡ã«ã¯é©ã—ã¦ã„ã‚‹ãŒã€OODã«ã¯æœ€é©ã§ã¯ãªã„åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ãŒæŒ™ã’ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

> ä¾‹ãˆã°ã€SupConæå¤±ã‚’ç”¨ã„ã¦CIFAR-10ã‚’å­¦ç¿’ã—ãŸå ´åˆã€IDã¨OODãƒ‡ãƒ¼ã‚¿ã®å¹³å‡è§’åº¦è·é›¢ã€€ã¯ã€åŸ‹ã‚è¾¼ã¿ç©ºé–“ã«ãŠã„ã¦29.86åº¦ã—ã‹ãªãã€åŠ¹æœçš„ãªIDã¨OODã®åˆ†é›¢ã‚’è¡Œã†ã«ã¯å°ã•ã™ãã‚‹

ã¨ã®ã“ã¨ã§ã™ã€‚

ãã“ã§ã€**OODã«æœ€é©ãªè¡¨ç¾å­¦ç¿’æ–¹æ³•ã¯ãªã«ã‹ï¼Ÿ** ã¨ã„ã†ã“ã¨ç–‘å•ã«å¯¾ã—ã€**C**ompactness (ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆæ€§)ã¨ã€**D**isp**E**rsoin **R**egularized å­¦ç¿’ã®OODç”¨ã«è¨­è¨ˆã•ã‚Œã¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹CIDEãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã—ãŸã€‚CIDERã§ã¯ã€è¶…çƒå½¢åŸ‹ã‚è¾¼ã¿ã‚’ã€å˜ä½ãƒãƒ«ãƒ ã®çƒçŠ¶ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«ä¼¼ã¦ã„ã‚‹von Mises-Fisher(vMF)åˆ†å¸ƒã«è¿‘ã¥ããªã‚‹ã‚ˆã†ã«æå¤±é–¢æ•°ã‚’è¨­è¨ˆã—ã¦ã„ã¾ã™ã€‚

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€

1. ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆæå¤±: ã‚µãƒ³ãƒ—ãƒ«ãŒä¸æ­£ã‚¯ãƒ©ã‚¹ã¨æ¯”è¼ƒã—ã¦æ­£ã—ã„ã‚¯ãƒ©ã‚¹ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ç¢ºç‡ãŒé«˜ã„

```python: https://github.com/deeplearning-wisc/cider/blob/34501dfcaa65820ed2e7021dd2678b2aba90ed72/utils/losses.py#L153
class CompLoss(nn.Module):
    '''
    Compactness Loss with class-conditional prototypes
    '''
    def __init__(self, args, temperature=0.07, base_temperature=0.07):
        super(CompLoss, self).__init__()
        self.args = args
        self.temperature = temperature
        self.base_temperature = base_temperature

    def forward(self, features, prototypes, labels):
        # ã‚¯ãƒ©ã‚¹ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—(ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿å¹³å‡)
        prototypes = F.normalize(prototypes, dim=1) 
        proxy_labels = torch.arange(0, self.args.n_cls).cuda()
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, proxy_labels.T).float().cuda() #bz, cls

        # compute logitsã€€(z^T * u / t)
        feat_dot_prototype = torch.div(
            torch.matmul(features, prototypes.T),
            self.temperature)
        # for numerical stability
        logits_max, _ = torch.max(feat_dot_prototype, dim=1, keepdim=True)
        logits = feat_dot_prototype - logits_max.detach()

        # compute log_prob
        exp_logits = torch.exp(logits) 
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # compute mean of log-likelihood over positive
        mean_log_prob_pos = (mask * log_prob).sum(1)

        # loss
        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos.mean()
        return loss
```


2. Dispersion loss (åˆ†æ•£æå¤±): ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹å¹³å‡é–“ã®è§’åº¦è·é›¢ã‚’æœ€å¤§åŒ–ã™ã‚‹

```python: https://github.com/deeplearning-wisc/cider/blob/34501dfcaa65820ed2e7021dd2678b2aba90ed72/utils/losses.py#L244
class DisLoss(nn.Module):
    '''
    Dispersion Loss with EMA prototypes
    '''
    def __init__(self, args, model, loader, temperature= 0.1, base_temperature=0.1):
        super(DisLoss, self).__init__()
        self.args = args
        self.temperature = temperature
        self.base_temperature = base_temperature
        self.register_buffer("prototypes", torch.zeros(self.args.n_cls,self.args.feat_dim))
        self.model = model
        self.loader = loader
        self.init_class_prototypes()

    def forward(self, features, labels):    

        prototypes = self.prototypes
        num_cls = self.args.n_cls

        # æŒ‡æ•°ç§»å‹•å¹³å‡(EMA) ã§ã‚¯ãƒ©ã‚¹ã”ã¨ã®åŸ‹ã‚è¾¼ã¿å¹³å‡ã‚’æ›´æ–° (å„ã‚¯ãƒ©ã‚¹ã®å…¨ã‚¯ãƒ©ã‚¹ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ˆã‚‹æ›´æ–°ã ã¨è¨ˆç®—é‡ãŒå¤šã™ãã‚‹ãŸã‚)
        for j in range(len(features)):
            prototypes[labels[j].item()] = F.normalize(prototypes[labels[j].item()] *self.args.proto_m + features[j]*(1-self.args.proto_m), dim=0)
        self.prototypes = prototypes.detach()
        labels = torch.arange(0, num_cls).cuda()
        labels = labels.contiguous().view(-1, 1)

        mask = (1- torch.eq(labels, labels.T).float()).cuda()


        logits = torch.div(
            torch.matmul(prototypes, prototypes.T),
            self.temperature)

        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(num_cls).view(-1, 1).cuda(),
            0
        )
        mask = mask * logits_mask
        mean_prob_neg = torch.log((mask * torch.exp(logits)).sum(1) / mask.sum(1))
        mean_prob_neg = mean_prob_neg[~torch.isnan(mean_prob_neg)]
        loss = self.temperature / self.base_temperature * mean_prob_neg.mean()
        return loss

    def init_class_prototypes(self):
        """Initialize class prototypes"""
        self.model.eval()
        start = time.time()
        prototype_counts = [0]*self.args.n_cls
        with torch.no_grad():
            prototypes = torch.zeros(self.args.n_cls,self.args.feat_dim).cuda()
            for i, (input, target) in enumerate(self.loader):
                input, target = input.cuda(), target.cuda()
                features = self.model(input)
                for j, feature in enumerate(features):
                    prototypes[target[j].item()] += feature
                    prototype_counts[target[j].item()] += 1
            for cls in range(self.args.n_cls):
                prototypes[cls] /=  prototype_counts[cls] 
            # measure elapsed time
            duration = time.time() - start
            print(f'Time to initialize prototypes: {duration:.3f}')
            prototypes = F.normalize(prototypes, dim=1)
            self.prototypes = prototypes
```

ã®æå¤±ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯ã€[How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?](https://github.com/deeplearning-wisc/cider/tree/master)

ã“ã®ã‚¯ãƒ©ã‚¹é–“ã®è·é›¢ã‚’å¤§ããã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã®ãŒSupConãªã©ã®æ—¢å­˜æ‰‹æ³•ã«å¯¾ã™ã‚‹æ–°è¦éƒ¨åˆ†?ã ã¨æ€ã„ã¾ã™ã€‚vMFåˆ†å¸ƒã¨ã©ã†é–¢ä¿‚ã™ã‚‹ã‹ã¨ã„ã†ã¨ã€æ­£ç›´ã‚ã¾ã‚Šã‚ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æå¤±é–¢æ•°ã‚’è¦‹ã¦ã‚ã‹ã‚‹ã‚ˆã†ã«ã€ä¸€å¿œã€è§’åº¦ã¨ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã¾ã™ã€‚

çµæœã¨ã—ã¦ã¯ã€ã“ã‚Œã‚‰ã®æå¤±ã‚’è¿½åŠ ã™ã‚‹ã¨ã€OODã®æ€§èƒ½ãŒã‚ãŒã‚Šã€åŠ ãˆã¦ã€åˆ†é¡æ€§èƒ½ã‚‚ä¸ŠãŒã£ã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ã€è«–æ–‡ã‚’è¦‹ã¦ã¿ã¦ãã ã•ã„ã€‚

# ã¾ã¨ã‚

ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãƒã‚¹æ€§ + ã‚¯ãƒ©ã‚¹é–“è·é›¢ã®æœ€å¤§åŒ–ã§OODæ€§èƒ½ã‚’ä¸Šã’ãŸè«–æ–‡ã§ã—ãŸã€‚
å®Ÿè£…ãŒç°¡å˜ãªã®ã§ã€ã‚¯ãƒ©ã‚¹é–“è·é›¢ã®æå¤±ã‚’è¿½åŠ ã™ã‚‹ã®ã¯ã€ãœã‚“ãœã‚“ã‚ã‚Šã ã¨æ€ã„ã¾ã—ãŸã€‚

