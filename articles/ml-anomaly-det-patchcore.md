---
title: "ã€CVPR2022ã€‘ç”»åƒç•°å¸¸æ¤œçŸ¥ PatchCoreã®å®Ÿè£…è§£èª¬" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ˜¸" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["python", "ml"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: false # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
---

# ã¯ã˜ã‚ã«

ã“ã‚“ã«ã¡ã¯ã€[ã‚ã£ã—ãƒ¼](https://twitter.com/kwashizzz)ã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€CVPR2022ã§ç™ºè¡¨ã•ã‚ŒãŸç”»åƒç•°å¸¸æ¤œçŸ¥æ‰‹æ³•ã§ã‚ã‚‹[PatchCore](https://openaccess.thecvf.com/content/CVPR2022/papers/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.pdf)ã®å®Ÿè£…ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚

ã¾ãšã¯ã€å®Ÿéš›ã«è©¦ã—ãŸçµæœã§ã™ã€‚ä¸‹å›³ã®ä¸Šã¯æ­£å¸¸ç”»åƒã€ä¸‹ã¯ç•°å¸¸ç”»åƒã§ã™ã€‚ç•°å¸¸éƒ¨åˆ†ãŒèµ¤ããªã£ã¦ãŠã‚Šã€è£½å“ãŒæ¬ æã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

![](https://storage.googleapis.com/zenn-user-upload/08dabbd0fbe0-20220827.png)

PatchCoreã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[å¤–è¦³æ¤œæŸ»å‘ã‘ç•°å¸¸æ¤œçŸ¥æ‰‹æ³•ã«é–¢ã™ã‚‹è«–æ–‡ç´¹ä»‹](https://techblog.leapmind.io/blog/20220701-introduction_to_patchcore/)ã®è¨˜äº‹ãŒã‚ã‹ã‚Šã‚„ã™ã„ã§ã™ã€‚

PatchCoreã®åˆ©ç‚¹ã¯ã€**ImageNetãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã•ã‚ŒãŸäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‚’ç”¨ã„ã‚‹ãŸã‚æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã®å¿…è¦ãªã„**ã“ã¨ã§ã™ã€‚
æ‰‹æ³•ã¨ã—ã¦ã¯ã€

1. æ­£å¸¸ãªç”»åƒç¾¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã«ãŠã‘ã‚‹å±€æ‰€çš„ãªéƒ¨åˆ†ã‚’ãƒ‘ãƒƒãƒç‰¹å¾´é‡ã¨ã—ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ã«ä¿å­˜ã™ã‚‹
2. é«˜é€ŸåŒ–ã®ãŸã‚ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ¬¡å…ƒå‰Šé™¤ã—ãŸç‰¹å¾´é‡ã«å¯¾ã—ã¦Greedyæ³•ã‚’ç”¨ã„ã€ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯å†…ã®ãƒ‘ãƒƒãƒç‰¹å¾´é‡ã®æ•°ã‚’å‰Šæ¸›
3. ãƒ†ã‚¹ãƒˆç”»åƒã®å„ä½ç½®ã®ç‰¹å¾´é‡ã«å¯¾ã—ã¦ã€è¿‘å‚æ³•ã§ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯å†…ã®æœ€ã‚‚è¿‘ã„ç‰¹å¾´é‡ã‚’é¸æŠã—ã€ãã®è·é›¢ã‚’ç•°å¸¸åº¦ã®ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹

ã§ã™ã€‚é›£ã—ãã†ã«æ€ãˆã¾ã™ãŒã€ä»¥ä¸‹ã®å®Ÿè£…ã‚’è¦‹ãªãŒã‚‰ã ã¨ã€ç†è§£ã§ãã‚‹ã¨æ€ã„ã¾ã™ã€‚

# å®Ÿè£…

æ¬¡ã«å®Ÿè£…ã®è©³ç´°ã‚’è§£èª¬ã—ã¾ã™ã€‚[k-washi/anomaly_detection_exp_v1](https://github.com/k-washi/anomaly_detection_exp_v1/blob/main/src/model/patchcore.py)ã«ã€å®Ÿè£…ã‚’è¼‰ã›ã¦ã„ã¾ã™(ç°¡å˜ã®ãŸã‚è§£èª¬ã§ã¯ã€ä¿®æ­£ã—ã¦ã„ã‚‹éƒ¨åˆ†ãŒã‚ã‚Šã¾ã™)ã€‚åŸºæœ¬çš„ã«pytorch lightningã«å‰‡ã£ãŸå®Ÿè£…ã‚’ã—ã¦ã„ã¾ã™ã€‚

PatchCoreã¯ã€Resnetãªã©ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸­é–“å±¤ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€`hook`ã‚’ç”¨ã„ã¦å–å¾—ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
class PatchCoreModelModule(LightningModule):
    def __init__(self, cfg: Config) -> None:
        super(PatchCoreModelModule, self).__init__()

        ...

        self.model = torch.hub.load(
            'pytorch/vision:v0.9.0',
            'wide_resnet50_2',
            pretrained=True
        )
        
        # wide resnet50ã®layer2ã¨layer3ã®å‡ºåŠ›ã‚’Forward Hookã‚’ä½¿ç”¨ã—ã¦å–å¾—
        self.features = []
        def hook_t(module, input, output):
            self.features.append(output)
        
        self.model.layer2[-1].register_forward_hook(hook_t)
        self.model.layer3[-1].register_forward_hook(hook_t)

    def forward(self, x_t):
        # ç‰¹å¾´é‡ã‚’å‡ºåŠ›
        # length:2 
        # y[0]:torch.Size([b, 512, 32, 32]) 
        # y[1]torch.Size([b, 1024, 16, 16])
        
        self.features = []
        _ = self.model(x_t)
        return self.features
```

ã¾ãšã¯ã€å…¨æ­£å¸¸ç”»åƒã«å¯¾ã™ã‚‹ç‰¹å¾´é‡ã®å–å¾—ã§ã™ã€‚`training_step`ã§ã“ã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚

```python

class PatchCoreModelModule(LightningModule):
    
    ...

    def training_step(self, batch, batch_idx): # save locally aware patch features
            x, _, _, _ = batch
            features = self(x)
            embeddings = []
            for feature in features:
                # Average Poolingã‚’è¡Œã†ã“ã¨ã§ã€ç‰¹å¾´ãƒãƒƒãƒ—ã‚’å±€æ‰€çš„ã«ã¼ã‚„ã‹ã™ã®ã¨åŒç­‰ã®å‡¦ç†ã‚’ã—ã¦ã„ã‚‹ã€‚
                # åŒã˜å¤§ãã•ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‚’è¿”ã™ ex. torch.Size([b, 512, 32, 32]) => torch.Size([b, 512, 32, 32])
                m = torch.nn.AvgPool2d(3, 1, 1)
                embeddings.append(m(feature.cpu()))
            
            # 2ã¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‚’çµåˆ
            embedding = embedding_concat(embeddings[0], embeddings[1])
            
            # ä½ç½®ã”ã¨ã®ç‰¹å¾´é‡ã‚’æ ¼ç´(1ã¤ã®ç‰¹å¾´é‡ã¯ãƒãƒ£ãƒ³ãƒãƒ«æ–¹å‘ã®ç‰¹å¾´)
            self.embedding_list.extend(reshape_embedding(np.array(embedding)))

# ä¸Šè¨˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã€é–¢æ•°ã§ã™
def embedding_concat(x, y):
    """
    yã‚’xã«åˆã‚ã›ã¦ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    Args:
        x (_type_): torch.Size([b, 512, 32, 32])
        y (_type_): torch.Size([b, 1024, 16, 16])

    Returns:
        torch.Size([b, 1536, 32, 32])
    """
    B, C1, H1, W1 = x.size()
    _, C2, H2, W2 = y.size()
    s = int(H1 / H2) # 2
    y = F.interpolate(y, scale_factor=s, mode="bilinear")
    z = torch.cat([x, y], dim=1)
    return z
    

def reshape_embedding(embedding):
    """batchx32(xä½ç½®åº§æ¨™)x32(yä½ç½®åº§æ¨™) å€‹ã®ãƒãƒ£ãƒ³ãƒãƒ«æ–¹å‘ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ

    Args:
        embedding (_type_): torch.Size([b, 1536, 32, 32])

    Returns:
        List[torch.Size([1536]), ....]
    """
    embedding_list = []
    for k in range(embedding.shape[0]):
        for i in range(embedding.shape[2]):
            for j in range(embedding.shape[3]):
                embedding_list.append(embedding[k, :, i, j])
    return embedding_list


```

å…¨ã¦ã®æ­£å¸¸ç”»åƒã«å¯¾ã™ã‚‹ç‰¹å¾´é‡ã‚’å–å¾—ã—ãŸå¾Œã¯ã€ç‰¹å¾´é‡ã®æ¬¡å…ƒå‰Šé™¤ã¨Greedyæ³•ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠã§ã™ã€‚

```python
from sklearn.random_projection import SparseRandomProjection

class PatchCoreModelModule(LightningModule):
    
    ...
    def training_epoch_end(self, outputs): 
        total_embeddings = np.array(self.embedding_list) # List[torch.Tensor((1536,)), ...] å„ä½ç½®ã®ç‰¹å¾´ã®ãƒªã‚¹ãƒˆ
        # Random projection
        # é«˜é€ŸåŒ–ã®ãŸã‚ã€ä½¿ç”¨ã™ã‚‹æ¬¡å…ƒã‚’ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§é¸ã³æ¬¡å…ƒå‰Šæ¸›ã™ã‚‹
        # Johnson-Lindenstrauss lemmaã«å‰‡ã£ã¦ä½æ¬¡å…ƒã«å°„å½±ã™ã‚‹ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œåˆ—ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹
        # Johnson-Lindenstrauss lemma: é«˜æ¬¡å…ƒã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“å†…ã®è¦ç´ ã‚’ãã‚Œãã‚Œã®è¦ç´ é–“ã®è·é›¢ã‚’ã‚ã‚‹ç¨‹åº¦ä¿ã£ãŸã¾ã¾ã€åˆ¥ã®(ä½æ¬¡å…ƒã®)ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã¸ç·šå‹å†™åƒã§ç§»ã›ã‚‹ã¨ã„ã†è£œé¡Œ
        self.randomprojector = SparseRandomProjection(n_components='auto', eps=0.9) # 'auto' => Johnson-Lindenstrauss lemma
        self.randomprojector.fit(total_embeddings) # å°„å½±ã™ã‚‹è¡Œåˆ—ã‚’å­¦ç¿’

        # Greedyæ³•ã«ã‚ˆã‚Šã€ç‰¹å¾´é‡ã®æ•°ã‚’Nå€‹é¸æŠã™ã‚‹
        print(total_embeddings.shape)
        selector = kCenterGreedy(total_embeddings,0,0)
        selected_idx = selector.select_batch(model=self.randomprojector, N=int(total_embeddings.shape[0]*0.001))
        self.embedding_coreset = total_embeddings[selected_idx]
        
        print('initial embedding size : ', total_embeddings.shape) #  (245760, 1536)
        print('final embedding size : ', self.embedding_coreset.shape) # (245, 1536) ã“ã“ã¾ã§ç‰¹å¾´é‡ã®æ•°ã‚’æ¸›ã‚‰ã›ã‚‹

        # ç‰¹å¾´é‡ã®ä¿å­˜
        with open(self.embedding_dir / 'embedding.pickle', 'wb') as f:
            pickle.dump(self.embedding_coreset, f)


```

Greedyæ³•ã‚’å®Ÿè£…ã—ãŸã€`kCenterGreedy`ã‚¯ãƒ©ã‚¹ã¯ä»¥ä¸‹ã«å®Ÿè£…ã‚’è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚

```python
import numpy as np
from sklearn.metrics import pairwise_distances

from src.model.utils.sampling_methods.sampling_der import SamplingMethod

class kCenterGreedy(SamplingMethod):

    def __init__(self, X, y, seed, metric='euclidean'):
        self.X = X
        self.y = y
        self.flat_X = self.flatten_X()
        self.name = 'kcenter'
        self.features = self.flat_X
        self.metric = metric
        self.min_distances = None
        self.n_obs = self.X.shape[0]
        self.already_selected = []

    def select_batch(self, model, N):
        """
        Greedyæ³•ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
        """

        # ç‰¹å¾´é‡ã®æ¬¡å…ƒå‰Šé™¤, (feat_num, 1536) => (feat_num, 306)
        self.features = model.transform(self.X)
        
        # Nå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ
        new_batch = []
        for _ in range(N):
            if self.already_selected is None:
                #åˆæœŸåŒ–: feat_num(Xã®ç‰¹å¾´æ•°)ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸€ã¤indexã‚’é¸æŠ
                ind = np.random.choice(np.arange(self.n_obs))
            else:
                # ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ç‰¹å¾´é‡ã¨ã®è·é›¢ãŒæœ€å¤§ã®ç‚¹(ç‰¹å¾´)ã‚’é¸æŠ
                ind = np.argmax(self.min_distances)

            x = self.features[[ind]] # ç‰¹å¾´é‡å–å¾—
            dist = pairwise_distances(self.features, x, metric=self.metric) #kcenteræŒ‡æ¨™ã§indã§é¸æŠã•ã‚ŒãŸç‰¹å¾´æ–™xã¨ã®è·é›¢ã‚’è¨ˆç®—
            
            # å„ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹æœ€ã‚‚å°ã•ãªè·é›¢ã‚’æ®‹ã™
            if self.min_distances is None:
                self.min_distances = np.min(dist, axis=1).reshape(-1,1) # (feat_num, 1)
            else:
                # ä»¥å‰è¨ˆç®—ã•ã‚ŒãŸè·é›¢ã¨ç¾æ™‚ç‚¹ã§è¨ˆç®—ã—ãŸè·é›¢ã®å°ã•ã„æ–¹æ³•æ®‹ã™
                self.min_distances = np.minimum(self.min_distances, dist) #  (feat_num, 1)
            new_batch.append(ind)
        print('Maximum distance from cluster centers is %0.2f'% max(self.min_distances))

        return new_batch
```

æœ€å¾Œã«ã€æ–°ã—ã„ç”»åƒã«å¯¾ã™ã‚‹ç•°å¸¸æ¤œçŸ¥ã§ã™ã€‚æ­£å¸¸ç”»åƒã«ã‚ˆã‚‹é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã«å¯¾ã™ã‚‹ç•°å¸¸å€¤ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

```python

from sklearn.neighbors import NearestNeighbors

class PatchCoreModelModule(LightningModule):
    def test_step(self, batch, batch_idx): # Nearest Neighbour Search
        # ã“ã“ã§ã¯batch_size=1ã‚’æƒ³å®š
        x, gt, label, x_type = batch

        # æ­£å¸¸ç”»åƒã®ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ã¾ã™
        self.embedding_coreset = pickle.load(open(self.embedding_dir / 'embedding.pickle', 'rb'))
        
        # ãƒ†ã‚¹ãƒˆç”»åƒã®ç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã¾ã™ (train_stepã¨åŒã˜ã§ã™)
        features = self(x)
        embeddings = []
        for feature in features:
            m = torch.nn.AvgPool2d(3, 1, 1)
            embeddings.append(m(feature))
        embedding_ = embedding_concat(embeddings[0], embeddings[1])
        embedding_test = np.array(reshape_embedding(np.array(embedding_.cpu())))


        # kè¿‘å‚æ³•
        # æœ€ã‚‚è¿‘ã„ç‰¹å¾´é‡ã‚’n_neighboreså€‹æ¢ç´¢ã—ã¾ã™
        nbrs = NearestNeighbors(n_neighbors=9, algorithm='ball_tree', metric='minkowski', p=2).fit(self.embedding_coreset)
        score_patches, _ = nbrs.kneighbors(embedding_test) # æ­£è§£ç‰¹å¾´é‡ã¨ã®è·é›¢ (1024, 9) : (ç‰¹å¾´ãƒãƒƒãƒ—32x32, è¿‘å‚ç‰¹å¾´é‡9å€‹)
        anomaly_map = score_patches[:,0].reshape((32, 32)) # æœ€ã‚‚è¿‘å‚ãªç‰¹å¾´é‡ã¨ã®è·é›¢ã‚’1åˆ—ã‹ã‚‰ç‰¹å¾´ãƒãƒƒãƒ—ã®å½¢å¼ã«reshape
        
        anomaly_map_resized = cv2.resize(anomaly_map, (254, 254)) # å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«resize
        anomaly_map_resized_blur = gaussian_filter(anomaly_map_resized, sigma=4) # çµæœãŒã‚·ãƒ£ãƒ¼ãƒ—ã™ãã‚‹ã®ã§å°‘ã—ã¼ã‹ã™
```


# æœ€å¾Œã«

ä»¥ä¸Šã€ç°¡å˜ã«å‡¦ç†ã®æµã‚Œã‚’è§£èª¬ã—ã¾ã—ãŸã€‚ã‹ãªã‚Šç²¾åº¦è‰¯ãç•°å¸¸éƒ¨åˆ†ã‚’æ¤œå‡ºã—ã¦ãã‚Œã¦ã„ã¾ã™ãŒã€ä¸€æ–¹ã§ã€è£½å“ã”ã¨ã«ã€ç•°å¸¸å€¤ã®ã‚¹ã‚³ã‚¢ãŒç•°ãªã£ã¦ã„ãŸã‚Šã¨é–¾å€¤ã®è¨­å®šãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

ã¾ã ã¾ã ã€ã“ã‚Œã®ã¿ã«å¤–è¦³æ¤œæŸ»ã‚’ä»»ã›ã‚‰ã‚Œã‚‹ç²¾åº¦ã§ã¯ãªã„æ°—ãŒã—ã¾ã™ã€‚é–¾å€¤ã®è¨­å®šã‚’ä½ãã‚ã«è¨­å®šã—ã€äººãŒç›®è¦–ã™ã‚‹é‡ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ä½¿ã†ãªã©ã‚³ã‚¹ãƒˆå‰Šæ¸›ãªã©ã«ã¯ä½¿ãˆã‚‹ã‹ã‚‚ã—ã¾ã›ã‚“ã€‚

---

ç”»åƒãƒ»è‡ªç„¶è¨€èªãƒ»éŸ³å£°ã«é–¢ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ç ”ç©¶é–‹ç™ºã‚„MLOpsã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã€æ©Ÿæ¢°å­¦ç¿’ã«é–¢ã—ã¦ã€ã”ç›¸è«‡ãŒã‚ã‚Œã°ã€[@kwashizzz](https://twitter.com/kwashizzz)ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¾ã§DMã—ã¦ãã ã•ã„ï¼
ã“ã‚Œã¾ã§ã®ã€[æ©Ÿæ¢°å­¦ç¿’è¨˜äº‹ã®ã¾ã¨ã‚](https://zenn.dev/kwashizzz/articles/my-ml-articles-summary)ã§ã™ã€‚