---
title: "【機械学習】動画にない視点の画像を作成してみた! NeRFを時間方向に拡張したNSFF:Nural Scene Flow Fieldの解説" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
---

こんにちは、鷲崎です。2020年、新しい3次元空間の表現手法として、[Neural Radiance Fields(NeRF)](https://arxiv.org/abs/2003.08934)が登場しました。この手法は、以下画像のように複数の画像からNeRFを学習したモデルにより、任意の視点(位置と角度)から見えるである画像を取得できます。

![nerf overview](images/nsff_nerf_overview.png)

この手法の面白いところは、学習に使用した画像の中に存在しない新しい視点の画像が生成できることです。そのため、以下の動画([論文公式ページより参照](https://www.matthewtancik.com/nerf))のように滑らかな視点変化の作成などもできます。

![nerf example](./images/nerf_ex.gif)

また、NeRFは、3D空間をニューラルネットワークで表現できるので、より研究が進めば、SLAMなどロボット研究の進歩につながるのではと期待しています。

# NeRF

実際には、NeRFは、画像を表現しているわけではありません。Radiance Fieldsと言う、ある点の物体の色と密度をニューラルネットワークで表現しており、これを基に画像が生成されています。

NeRFの詳細に関しては、[弊社機械学習チームの濱野が書いた記事](https://tech.fusic.co.jp/posts/2020-03-29-read-nelf-paper/)や、[【論文読解】NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://qiita.com/takoroy/items/53e62d303b9743b06801)など、わかりやす記事が多く書かれているので参考にしてください。

# NeRFの欠点

一般的に、インターネット上に公開されている動画は、人間や動物、車などの多様な動的なコンテンツを含んでいます。しかし、NeRFを表現する手法の多くは、学習するシーンが静的でることを仮定しています。そのため、下図の右端のように動的な物体に対しては、不確かなNeRFを獲得してしまい、動的なコンテンツを含む動画に対しては、適用できません。

そこで、空間に加え時間方向も表現した[Neural Scene Flow Fields (NSFF)](https://www.cs.cornell.edu/~zl548/NSFF/)が提案されました。

![nerf prop](images/nsff_nerf_prop.png)

# NSFFを実際に試してみました！

空間に加え時間方向も表現したNSFFの詳細な解説の前に、どんな手法であるかイメージを持ってもらうため、実際に試してみた結果を紹介したいと思います！

学習に使用した動画は、以下のものになります。  
![origin](images/nsff_wd_origin.gif)

この動画から、10FPSで画像を切り出し、それを学習データとして使用しました。

学習には、この動画の他に、カメラパラメータと言われる撮影条件のようなものが必要になります。これは、[COLMAP](https://colmap.github.io/)という、画像処理により複数の画像から、3Dモデルを作成する三次元再構成ライブラリを用いて取得します。ついでに作成した今回の動画から得られる3Dモデルは、以下のようなものになりました。

机の上の静的な物体は、3Dモデルになっていますが、人物やモニターなど動いているものは、あまり結果がよくありませんでした。

![colmap 3d](images/nerf_3d_post.gif)

さて、以上より取得した、複数枚の画像とカメラパラメータを用いて、[オフィシャルのコード](https://github.com/zhengqili/Neural-Scene-Flow-Fields)を参考にしつつ、NSFFを学習させてみました。学習には、GeForce RTX 3090を使用したのですが、4日ほどかかりました。結果は、以下のようになり、かなり良い結果が得られていると思います。

上の動画は、10フレームと同じ視点に固定し、時間方向だけ変化させた結果です。動画内で動的な手前の人物のみ移動しています。下の動画は、0フレームの時間において、視点を変化させています。一番下に、参考となる画像(左0, 右10フレーム)を貼っています。

参考画像を見ると0~10フレームでかなり視点が変わっていますが、上の動画では、視点を固定したにもかかわらず、手前の動的な人物を描画できています。

![loccam](images/nsff_wd_frame_10_47000_lockcam.gif)

下の動画では、0フレーム目の時間を対象としており、画像の左側に関する情報が動画内に存在しません。そのため、左側はボケています。しかし、そのような視点においても、NSFFは上手く機能していることが見てとれます。


![00](images/nerf_wd_frame_00_470001.gif)

![ref img](images/wd_ref_img.png)


# NSFFとは？
