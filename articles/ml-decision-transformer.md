---
title: "【論文読み】Decision Transformer: ReinforcementLearning via Sequence Modeling の解説" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
---

### Deadly Triadとは  

Deep Q Learningの失敗の原因は、学習にもちいる以下の3つに起因することを[Deadly Triad](https://arxiv.org/abs/1812.02648)と言われています。

1. Bootstrapping  
   他の状態/行動の価値を使って、ある状態の価値を学習することを、強化学習ではブートストラップと呼ばれています。この時、他の状態/行動に推定量を含んでおり、誤算が蓄積されてしまいます。

   解決法として、Monte Carlo法を使えば良いが、エピソード終了まで学習できないため、計算効率が悪く、膨大なメモリが必要となる。

2. Function approximation (関数近似)  
   昨今の強化学習では、入力が画像であったりと、考慮すべきパターンが膨大になってきたため、関数近似器(特にDNN)を用いて方策・価値関数などを近似することが多いです。学習時に関数近似器を更新していきますが、ある情報の価値を更新する際に、他の状態の価値も更新されてしまします。
　　
　　大規模な問題を解くためには必須となります。

3. Off-policy
   学習に使用するデータセットを蓄積し、学習することが多く、現在の方策とは異なる方策で得た遷移を用いて学習することになります。サンプルの取り方(偏るなど）によっては、学習する方策が悪い方に学習されてしまうことがあります。On-policyにしたいが、学習時後に収集したデータを捨てることになるため、サンプル効率が悪いです。よりOn-Oplicyに近く、多様なサンプルの利用が性能改善のこつらしい。

なぜ、DQLでは上手くいっているのかというと、以下の理由があります。
|原因|　理由|
|-|-|
|Bootstrapping |Multi step learning, Target networkを用いたDouble DQNで、発散しづらくなり、報酬も大きくなる|
|Function approximation| 関数近似器のネットワークを大きくすると、発散しやすくなるが、報酬が大きくなる|
|Off-policy| Replay Bufferからのサンプリングに優先度付け (優先度付き経験再生)の度合いが強いと、発散しやすい|




[Deep Reinforcement Learning and the Deadly Triad](https://arxiv.org/abs/1812.02648)を参考にしました。

# Conservative Q-Leaning(CQL)

[ゼロから始めてオフライン強化学習とConservative Q-Learningを理解する](https://qiita.com/aiueola/items/90f635200d808f904daf)が、とても参考になりました。

オフライン強化学習の課題  
1. 選ばれた行動以外のフィードバックが得られない．
2. データ分布がデータ収集時の方策に依存したバイアスのかかったものになっている

結果として、分布シフトが発生し、unlearning effectが発生してしまいます。つまり、どれだけデータ数を増やしても、性能が向上しなくなります。

オフライン強化学習では、実環境からのフィードバックが得られない。


# 

本研究では、従来のRLアルゴリズムにおけるコンポーネントのアーキテクチャとしてトランスフォーマーを用いた先行研究[4,5]とは対照的に、生成的な軌道モデリング、すなわち、状態、行動、報酬のシーケンスの共同分布のモデリングが、従来のRLアルゴリズムの置き換えとして機能するかどうかを研究する。これにより、長期的なクレジット割り当てのためのブートストラップの必要性を回避することができ、RLを不安定にすることが知られている「デッドリー・トライアッド」[6]の1つを回避することができます。

割引報酬が必要なく、直接学習可能。トランスフォーマーは、報酬がまばらであったり、気が散ったりする場合でも効果的に機能することができます。この仮説は、最適でないデータからポリシーを学習し、固定された限られた経験から最大限に効果的な行動を生み出すというタスクをエージェントに課す、オフラインRLを検討することで明らかになりました。

図より、データセットに対して最適な経路を学習するシーケンスモデリングを目的としており、誤差の伝搬や値の課題評価などの課題の解決法となる。(ベルマン方程式に則っていないから？)

ランダムウォークのデータのみを用いてトレーニングを行い、専門家によるデモンストレーションを行わずに、可能な限り高いリターンを得るための事前処理を追加することで、テスト時に最適な軌道を生成することができます