---
title: "OpenAIã®éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ« Whisperã®è§£èª¬ / Fine Tuning æ–¹æ³•" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ˜¸" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["æ©Ÿæ¢°å­¦ç¿’"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: false # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
---

OpenAIã‹ã‚‰ã€ã‹ãªã‚Šã™ã”ã„éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ« [Whisper](https://github.com/openai/whisper)ãŒç™ºè¡¨ã•ã‚Œã¾ã—ãŸã€‚ç‰¹å‡ºã™ã¹ãç‚¹ã¯ã€68ä¸‡æ™‚é–“ã¨ã„ã†ã€ã‹ãªã‚Šãƒ¤ãƒã‚ã®ãƒ‡ãƒ¼ã‚¿é‡ã§è¨“ç·´ã—ã¦ãŠã‚Šã€è‹±èªã§ã¯å•†ç”¨ã®éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚„äººé–“ã®æ›¸ãèµ·ã“ã—ã«åŒ¹æ•µã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã§ã™ã€‚

ç¤¾å†…ã§ã‚‚æ—¥æœ¬èªã€ãƒ–ãƒ«ã‚¬ãƒªã‚¢èªã€éŸ“å›½èªã§è©¦ã—ã¦ã¿ã¾ã—ãŸãŒã€ã™ã”ã„ç²¾åº¦ã§ã—ãŸã€‚æ—¥æœ¬èªã®å ´åˆã€æ¼¢å­—ã®é–“é•ã„ãŒå¤šã€…ã‚ã‚Šã¾ã—ãŸãŒã€ç™ºéŸ³ã¯å¤§ä½“ã‚ã£ã¦ãã†ã§ã—ãŸã€‚ãƒ–ãƒ«ã‚¬ãƒªã‚¢èªã¯ã€ãƒ­ã‚·ã‚¢èªã§èªè­˜ã•ã‚Œã¦ã„ã¾ã—ãŸã€‚éŸ“å›½èªã¯ã€å®Œç’§ã§ã—ãŸã€‚

ã—ã‹ã—ã€[Github](https://github.com/openai/whisper)ã«å…¬é–‹ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã¿ã‚‹ã¨ã€è¨“ç·´ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ãŠã‚‰ãšã€å…¬é–‹ã®äºˆå®šã‚‚ãªã„ãã†ã§ã™ã€‚ãã“ã§ã€æœ¬è¨˜äº‹ã§ã¯ã€Whisperã®è§£èª¬ã«åŠ ãˆã¦ã€è©¦ã—ã«ä½œæˆã—ã¦ã¿ãŸFine Tuningã®æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚

â€» Fine TungingãŒä¸Šæ‰‹ãã„ã£ã¦ãã†ã«è¦‹ãˆã‚‹ã®ã§ã™ãŒã€æ­£ç¢ºãªã‚³ãƒ¼ãƒ‰ã§ã¯ãªã„ã¨æ€ã„ã¾ã™ã€‚æ°—ä»˜ã„ãŸç‚¹ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€ã‚³ãƒ¡ãƒ³ãƒˆãã ã•ã„ã€‚

å…¨ã¦ã®ã‚³ãƒ¼ãƒ‰ã¯ã€[Whisper](https://github.com/openai/whisper)ã®[Discussion: Finetuning/Training code ?](https://github.com/openai/whisper/discussions/64#discussioncomment-3765117)ã‚’ã¿ã¦ãã ã•ã„ã€‚

# Whisperã¨ã¯

1. è‹±èªã®ã¿ãªã‚‰ãšã€æ—¥æœ¬èªã‚’å«ã‚€å¤šè¨€èªã§SoTAã«åŒ¹æ•µã™ã‚‹æ€§èƒ½
2. éŸ³å£°ã®è¨€èªèªè­˜ã€éŸ³å£°åŒºé–“æ¤œå‡ºã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å‡ºåŠ›ãŒã§ãã‚‹ã‚‰ã—ã„ (è©¦ã—ã¦ã„ãªã„...)
3. MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ï¼

ã¨ã„ã†ã™ã”ãã€ã‚ã‚ŠãŒãŸã„éŸ³å£°èªè­˜æ‰‹æ³•ã§ã™ã€‚

ã“ã‚Œã¾ã§ã®éŸ³å£°èªè­˜ã®æµã‚Œã¯ã€[wav2vec2.0](https://arxiv.org/abs/2006.11477)ãªã©æ•™å¸«ãªã—ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’ãŒãƒ¡ã‚¤ãƒ³ã§ã—ãŸã€‚ä¸€æ–¹ã§ã€Whisperã¯ã€æ•™å¸«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’68ä¸‡æ™‚é–“(æ—¥æœ¬èª:7054æ™‚é–“)ã«ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚‹ã“ã¨ã§ã€å¤šãã®è¨€èª/ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè»¢ç§»ã‚’å¯èƒ½ã«ã—ã¾ã—ãŸã€‚

LibriSpeechã§å­¦ç¿’ã—ãŸwav2vec2.0ã¨æ¯”è¼ƒã—ãŸçµæœãŒã€è«–æ–‡ä¸­ã®Table. 2ã®ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã™ã€‚å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹WER(word error rate)ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

![](https://storage.googleapis.com/zenn-user-upload/e94a891042cd-20220930.png)

wav2vec2.0ã®å­¦ç¿’å¯¾è±¡ã§ã‚ã‚‹LibriSpeechã«ãŠã„ã¦ã€Whisperã¯wav2vec2.0ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã—ã¦ãŠã‚Šã€ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€èª¤ã‚Šç‡ã‚’ã‹ãªã‚Šä½æ¸›ã§ãã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã¨ã¦ã‚‚ã™ã”ã„ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ã§ã™ï¼


ã¾ãŸã€è‹±èªã«ãŠã‘ã‚‹äººé–“ã«ã‚ˆã‚‹æ›¸ãèµ·ã“ã—ã¨ã®æ¯”è¼ƒã§ã™ãŒã€ä»¥ä¸‹ã®å›³ã«ãªã‚Šã¾ã™ã€‚å·¦å´ãŒã€éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã€å³å´ãŒäººé–“ã«ã‚ˆã‚‹æ›¸ãèµ·ã“ã—ã®WERã§ã™ã€‚ã“ã‚Œã‚‚ã™ã”ã„çµæœã§ã€äººé–“ã¨Whisperã¯ã»ã¨ã‚“ã©ç²¾åº¦å·®ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

![](https://storage.googleapis.com/zenn-user-upload/7179ee64bd6a-20220930.png)

ä»–ã«ã‚‚ã€å¤šè¨€èªã«ãŠã‘ã‚‹è€ƒå¯Ÿãªã©[è«–æ–‡](https://cdn.openai.com/papers/whisper.pdf)ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€è©³ç´°ã‚’çŸ¥ã‚ŠãŸã„æ–¹ã¯ã€è¦ãƒã‚§ãƒƒã‚¯ã§ã™ã€‚

# æ‰‹æ³•ã®æ¦‚è¦

ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ä»¥ä¸‹ã®å›³ã®ã‚ˆã†ã«å˜ç´”ã§ã€éŸ³å£°ã‚’å…¥åŠ›ã¨ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€ãƒ‡ã‚³ãƒ¼ãƒ€å½¢å¼ã®Transformerã§ã™ã€‚
ãƒ‡ã‚³ãƒ¼ãƒ€ã§ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§æŠ½å‡ºã—ãŸéŸ³å£°ç‰¹å¾´é‡ã‚’ã‚¯ãƒ­ã‚¹ã‚¢ãƒƒãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å…¥åŠ›ã¨ã—ã¦ã„ã¾ã™ã€‚ãã—ã¦ã€SOT(Start of transcript)ã‚’æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’ç¹°ã‚Šè¿”ã™ã ã‘ã§ã™ã€‚

å®Ÿéš›ã€éŸ³å£°èªè­˜ã‚’ã™ã‚‹éš›ã«ã¯ã€ä»¥ä¸‹ã®å›³ã®ã‚ˆã†ã«SOTã€Langage Tagã€Transcribeã€No Timestampã‚’çµ„ã¿åˆã‚ã›ã§ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚Œã°è‰¯ã„ã§ã™ã€‚

ä¾‹ãˆã°ã€ã€Œã“ã‚“ã«ã¡ã¯ã€ã¨ã„ã†æ—¥æœ¬èªã®å ´åˆã¯ã€(SOT, Ja, Transcribe, No Timestamp)ã‚’ã¯ã˜ã‚ã®ãƒ‡ã‚³ãƒ¼ãƒ€å…¥åŠ›ã¨ã—ã€çµæœã¨ã—ã¦ã€(Ja, Transcribe, No Timestamp, ã“)ãŒæ¨å®šã•ã‚Œã€ãã®å¾Œã€(SOT, Ja, Transcribe, No Timestamp, ã“)ã‚’å…¥åŠ›ã¨ã—ã€(Ja, Transcribe, No Timestamp, ã“, ã‚“)ãŒæ¨å®šã•ã‚Œã¦ã„ãã¿ãŸã„ãªæ„Ÿã˜ã§ã™ã€‚ã€Œã“ã€ã€ã€Œã‚“ã€ãªã©ã®å˜èªã¯ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã§å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’æ¨å®šã•ã‚Œã‚‹ã®ã§ã€ãã®æœ€å¤§å€¤ã®Indexã‚’ã¨ã‚Šã€ãã‚Œã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§æ–‡å­—ã‚„å˜èªã«ç›´ã™ã¨ã„ã†æ‰‹é †ã§å–å¾—ã§ãã¾ã™ã€‚

![](https://storage.googleapis.com/zenn-user-upload/89b506b237a9-20220930.png)


# Fine Tuning

æ¬¡ã«ã€Whisperã®Fine Tuningæ–¹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚ä»Šå›ã¯ã€ç°¡å˜ã®ãŸã‚[JVSã‚³ãƒ¼ãƒ‘ã‚¹](https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus)ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’OpenJtalkã§ã‚«ãƒŠã«å¤‰æ›ã—ãŸã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

ã¾ãŸã€å®Ÿè¡Œå ´æ‰€ã¯ã€Google Colabã§ã™ã€‚
é‡è¦ãªéƒ¨åˆ†ã®ã¿è¨˜è¼‰ã™ã‚‹ã®ã§ã€è©³ç´°ãªå…¨ã¦ã®ã‚³ãƒ¼ãƒ‰ã¯ã€[Whisper](https://github.com/openai/whisper)ã®[Discussion: Finetuning/Training code ?](https://github.com/openai/whisper/discussions/64#discussioncomment-3765117)ã‚’ã¿ã¦ãã ã•ã„ã€‚


## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã€Whisperã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚ä»Šå›ã¯ã€`pytorch lightning`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚CERã®è¨ˆç®—ã®ãŸã‚`evaluate`ã‚‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ã€‚

```s
%%capture
! pip install git+https://github.com/openai/whisper.git
! pip install jiwer
! pip install pyopenjtalk==0.3.0
! pip install pytorch-lightning==1.7.7
! pip install -qqq evaluate==0.2.2
```

## audioã®èª­ã¿è¾¼ã¿

æœ€è¿‘ã€éŸ³éŸ¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¯ã€`torchaudio`ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã™ãŒã€ã‚ˆã‚Šæ—©ãèª­ã¿è¾¼ã‚“ã§ã€sample rateã‚’å¤‰æ›ã§ãã‚‹æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹äººã¯æ•™ãˆã¦æ¬²ã—ã„ã§ã™ã€‚

```python
def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:
    waveform, sr = torchaudio.load(wave_path, normalize=True)
    if sample_rate != sr:
        waveform = at.Resample(sr, sample_rate)(waveform)
    return waveform
```

## JVS ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

JVSãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’google driveã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```s
%%capture
import gdown
gdown.download('https://drive.google.com/u/0/uc?id=19oAw8wWn3Y7z6CKChRdAyGOB9yupL_Xt', 'jvs.zip', quiet=False)
!unzip jvs.zip -d ./jvs

import IPython.display
IPython.display.Audio("/content/jvs/jvs_ver1/jvs001/nonpara30/wav24kHz16bit/BASIC5000_0025.wav")
```

## ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€é–¢é€£

ã¾ãšã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã§ã™ã€‚

```python
def text_kana_convert(text):
    text = pyopenjtalk.g2p(text, kana=True)
    return text

class JvsSpeechDataset(torch.utils.data.Dataset):
    def __init__(self, audio_info_list, tokenizer, sample_rate) -> None:
        super().__init__()

        self.audio_info_list = audio_info_list # List[(audioã®ID, audioã®ãƒ‘ã‚¹, audioã®ãƒ†ã‚­ã‚¹ãƒˆ)]
        self.sample_rate = sample_rate # 16_000
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.audio_info_list)
    
    def __getitem__(self, id):
        audio_id, audio_path, text = self.audio_info_list[id]

        # audioã‚’èª­ã¿è¾¼ã¿ã‚ã‚‹ã‚¹ãƒšã‚¯ãƒ­ã‚°ãƒ©ãƒ ã«å¤‰æ›
        audio = load_wave(audio_path, sample_rate=self.sample_rate)
        audio = whisper.pad_or_trim(audio.flatten()) # 480000ã‚µãƒ³ãƒ—ãƒ«(30sec)ã«é•·ã•ã‚’æƒãˆã‚‹
        mel = whisper.log_mel_spectrogram(audio) # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒ­ã‚°ãƒ©ãƒ ã«å¤‰æ›

        text = text_kana_convert(text) # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚«ãƒŠã«å¤‰æ›

        # <|start of transcribe|><|ja|><|transcribe|><|notimestamps|>ã‚³ãƒ³ãƒ‹ãƒãƒ<|endoftext|>ãŒæœ€çµ‚çš„ã«å¾—ãŸã„å½¢å¼

        # ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ã¯ <|start of transcribe|><|ja|><|transcribe|><|notimestamps|>ã‚³ãƒ³ãƒ‹ãƒãƒ
        # <|endoftext|> ãªã—
        text = [*self.tokenizer.sot_sequence_including_notimestamps] + self.tokenizer.encode(text)

        # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ã€<|ja|><|transcribe|><|notimestamps|>ã‚³ãƒ³ãƒ‹ãƒãƒ<|endoftext|>
        # <|start of transcribe|> ãªã—
        labels = text[1:] + [self.tokenizer.eot]

        return {
            "input_ids": mel,
            "dec_input_ids": text,
            "labels": labels
        }
```

ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹éš›ã«ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚’åˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€`Collator`ã‚‚å®Ÿè£…ã—ã¾ã—ãŸã€‚
å®Ÿè£…ã¯ã€[sanchit-gandhi/whisper-medium-switchboard-5k](https://huggingface.co/sanchit-gandhi/whisper-medium-switchboard-5k/blob/main/run_speech_recognition_whisper.py#L444)ã‚‚å‚è€ƒã«ã•ã›ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚

```python
class WhisperDataCollatorWhithPadding:
    def __call__(sefl, features):
        input_ids, labels, dec_input_ids = [], [], []
        for f in features:
            input_ids.append(f["input_ids"])
            labels.append(f["labels"])
            dec_input_ids.append(f["dec_input_ids"])

        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])
        
        label_lengths = [len(lab) for lab in labels]
        dec_input_ids_length = [len(e) for e in dec_input_ids]
        max_label_len = max(label_lengths+dec_input_ids_length)

        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]
        dec_input_ids = [np.pad(e, (0, max_label_len - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, dec_input_ids_length)] # 50257 is eot token id

        batch = {
            "labels": labels,
            "dec_input_ids": dec_input_ids
        }

        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}
        batch["input_ids"] = input_ids

        return batch
```

ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã§ã€ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
woptions = whisper.DecodingOptions(language="ja", without_timestamps=True)
wmodel = whisper.load_model("base")
wtokenizer = whisper.tokenizer.get_tokenizer(True, language="ja", task=woptions.task)

dataset = JvsSpeechDataset(eval_audio_transcript_pair_list, wtokenizer, SAMPLE_RATE)
loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWhithPadding())
```

## å­¦ç¿’ã‚³ãƒ¼ãƒ‰

æœ€å¾Œã«å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚ã“ã‚Œã¯ã€pytorch lightningã«æ²¿ã£ã¦å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

ã¾ãšã¯ã€`Config`ã§ã™ã€‚

```python
class Config:
    learning_rate = 0.0005
    weight_decay = 0.01
    adam_epsilon = 1e-8
    warmup_steps = 2
    batch_size = 16
    num_worker = 2
    num_train_epochs = 10
    gradient_accumulation_steps = 1
    sample_rate = SAMPLE_RATE
```

æ¬¡ã«ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®šã§ã™ã€‚

```python
class WhisperModelModule(LightningModule):
    def __init__(self, cfg:Config, model_name="base", lang="ja", train_dataset=[], eval_dataset=[]) -> None:
        super().__init__()
        # ãƒ¢ãƒ‡ãƒ«ã‚„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨­å®šã§ã™ã€‚
        self.options = whisper.DecodingOptions(language=lang, without_timestamps=True)
        self.model = whisper.load_model(model_name)
        self.tokenizer = whisper.tokenizer.get_tokenizer(True, language="ja", task=self.options.task)

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã‚ˆã‚‹éŸ³éŸ¿ç‰¹å¾´é‡ã®æŠ½å‡ºéƒ¨åˆ†ã®å­¦ç¿’ã¯è¡Œã„ã¾ã›ã‚“ã€‚
        for p in self.model.encoder.parameters():
            p.requires_grad = False
        
        # Discussionã«æ›¸ã‹ã‚Œã¦ã¾ã—ãŸãŒã€CrossEntropyã‚’ä½¿ã£ã¦ã„ã‚‹ãã†ã§ã™
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)

        # WERã¨CERã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã§ã™ã€‚
        self.metrics_wer = evaluate.load("wer")
        self.metrics_cer = evaluate.load("cer")

        self.cfg = cfg
        self.__train_dataset = train_dataset # List[(audioã®ID, audioã®ãƒ‘ã‚¹, audioã®ãƒ†ã‚­ã‚¹ãƒˆ)]
        self.__eval_dataset = eval_dataset # List[(audioã®ID, audioã®ãƒ‘ã‚¹, audioã®ãƒ†ã‚­ã‚¹ãƒˆ)]
    
    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_id):
        input_ids = batch["input_ids"]
        labels = batch["labels"].long()
        dec_input_ids = batch["dec_input_ids"].long()

        with torch.no_grad():
            audio_features = self.model.encoder(input_ids) # ã“ã“ã¯å­¦ç¿’ã—ãªã„

        out = self.model.decoder(dec_input_ids, audio_features) # ãƒ‡ã‚³ãƒ¼ãƒ€ã®ã¿å­¦ç¿’
        loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))
        self.log("train/loss", loss, on_step=True, prog_bar=True, logger=True)
        return loss
    
    def validation_step(self, batch, batch_id):
        input_ids = batch["input_ids"]
        labels = batch["labels"].long()
        dec_input_ids = batch["dec_input_ids"].long()


        audio_features = self.model.encoder(input_ids)
        out = self.model.decoder(dec_input_ids, audio_features)

        loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))

        out[out == -100] = self.tokenizer.eot
        labels[labels == -100] = self.tokenizer.eot

        # ä»¥ä¸‹ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚«ãƒŠ(ãƒ†ã‚­ã‚¹ãƒˆ)ã«å¤‰æ›
        o_list, l_list = [], []
        for o, l in zip(out, labels):
            o = torch.argmax(o, dim=1)
            o_list.append(self.tokenizer.decode(o, skip_special_tokens=True)) 
            l_list.append(self.tokenizer.decode(l, skip_special_tokens=True))
        cer = self.metrics_cer.compute(references=l_list, predictions=o_list)
        wer = self.metrics_wer.compute(references=l_list, predictions=o_list)

        self.log("val/loss", loss, on_step=True, prog_bar=True, logger=True)
        self.log("val/cer", cer, on_step=True, prog_bar=True, logger=True)
        self.log("val/wer", wer, on_step=True, prog_bar=True, logger=True)

        return {
            "cer": cer,
            "wer": wer,
            "loss": loss
        }

    def configure_optimizers(self):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆã™ã‚‹"""
        model = self.model
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in model.named_parameters() 
                            if not any(nd in n for nd in no_decay)],
                "weight_decay": self.cfg.weight_decay,
            },
            {
                "params": [p for n, p in model.named_parameters() 
                            if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        optimizer = AdamW(optimizer_grouped_parameters, 
                          lr=self.cfg.learning_rate, 
                          eps=self.cfg.adam_epsilon)
        self.optimizer = optimizer

        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=self.cfg.warmup_steps, 
            num_training_steps=self.t_total
        )
        self.scheduler = scheduler

        return [optimizer], [{"scheduler": scheduler, "interval": "step", "frequency": 1}]
    
    def setup(self, stage=None):
        """åˆæœŸè¨­å®š"""

        if stage == 'fit' or stage is None:
            self.t_total = (
                (len(self.__train_dataset) // (self.cfg.batch_size))
                // self.cfg.gradient_accumulation_steps
                * float(self.cfg.num_train_epochs)
            )
    
    def train_dataloader(self):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆã™ã‚‹"""
        dataset = JvsSpeechDataset(self.__train_dataset, self.tokenizer, self.cfg.sample_rate)
        return torch.utils.data.DataLoader(dataset, 
                          batch_size=self.cfg.batch_size, 
                          drop_last=True, shuffle=True, num_workers=self.cfg.num_worker,
                          collate_fn=WhisperDataCollatorWhithPadding()
                          )

    def val_dataloader(self):
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆã™ã‚‹"""
        dataset = JvsSpeechDataset(self.__eval_dataset, self.tokenizer, self.cfg.sample_rate)
        return torch.utils.data.DataLoader(dataset, 
                          batch_size=self.cfg.batch_size, 
                          num_workers=self.cfg.num_worker,
                          collate_fn=WhisperDataCollatorWhithPadding()
                          )
```

è¨“ç·´ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

```python
log_output_dir = "/content/logs" # ãƒ­ã‚°ã®å‡ºåŠ›å…ˆ
check_output_dir = "/content/artifacts" # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‡ºåŠ›å…ˆ
train_name = "whisper"
train_id = "00001"
model_name = "base" # whisperã®pratrainã§ã™
lang = "ja" # æ—¥æœ¬èªï¼

cfg = Config()

# å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
Path(log_output_dir).mkdir(exist_ok=True)
Path(check_output_dir).mkdir(exist_ok=True)

# tensor boardã§ãƒ­ã‚®ãƒ³ã‚°ã—ã¾ã™
tflogger = TensorBoardLogger(
    save_dir=log_output_dir,
    name=train_name,
    version=train_id
)

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‡ºåŠ›è¨­å®šã§ã™
# ã¨ã‚Šã‚ãˆãšã€å…¨ã¦å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™
checkpoint_callback = ModelCheckpoint(
    dirpath=f"{check_output_dir}/checkpoint",
    filename="checkpoint-{epoch:04d}",
    save_top_k=-1 # all model save
)

callback_list = [checkpoint_callback, LearningRateMonitor(logging_interval="epoch")]

# ãƒ¢ãƒ‡ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä½œæˆã§ã™
model = WhisperModelModule(cfg, model_name, lang, train_audio_transcript_pair_list, eval_audio_transcript_pair_list)

# è¨“ç·´é–‹å§‹ã§ã™ã€‚fp16ã§å­¦ç¿’ã™ã‚‹ã‚ˆã†è¨­å®šã—ã¦ã„ã¾ã™ã€‚
trainer = Trainer(
    precision=16,
    accelerator="gpu",
    max_epochs=cfg.num_train_epochs,
    accumulate_grad_batches=cfg.gradient_accumulation_steps,
    logger=tflogger,
    callbacks=callback_list
)

trainer.fit(model)
```

ä»¥ä¸Šã€Fine Tuningã®ã‚³ãƒ¼ãƒ‰ã§ã—ãŸã€‚

# æ¨è«–

æ¬¡ã«ã€å®Ÿéš›ã«æ¨è«–ã—ã¦ã¿ã¦è©•ä¾¡ã—ã¾ã™ã€‚

```python
checkpoint_path = "/content/artifacts/checkpoint/checkpoint-epoch=0009.ckpt"

# state_dictã«ã¯ã€epochã‚„optimizerã®æƒ…å ±ã‚‚æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€state_dictã ã‘å–ã‚Šå‡ºã—ã¾ã™ã€‚
state_dict = torch.load(checkpoint_path)
print(state_dict.keys())
state_dict = state_dict['state_dict']

# ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
whisper_model = WhisperModelModule(cfg)
whisper_model.load_state_dict(state_dict)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ä½œæˆ
woptions = whisper.DecodingOptions(language="ja", without_timestamps=True)
wtokenizer = whisper.tokenizer.get_tokenizer(True, language="ja", task=woptions.task)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ä½œæˆ
dataset = JvsSpeechDataset(eval_audio_transcript_pair_list, wtokenizer, SAMPLE_RATE)
loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWhithPadding())

# æ¨è«–å‡¦ç†ã§ã™ï¼
refs = []
res = []
for b in tqdm(loader):
    input_ids = b["input_ids"].half().cuda()
    labels = b["labels"].long().cuda()
    with torch.no_grad():
        results = whisper_model.model.decode(input_ids, woptions)
        for r in results:
            res.append(r.text)
        
        for l in labels:
            l[l == -100] = wtokenizer.eot
            ref = wtokenizer.decode(l, skip_special_tokens=True)
            refs.append(ref)
```

å®Ÿéš›ã«ã€CERã§è©•ä¾¡ã—ã¦ã¿ã‚‹ã¨ã€CER: `0.0179`ã§ã€ã‹ãªã‚Šè‰¯ã„æ°—ãŒã—ã¾ã™ã€‚ã‚¿ã‚¹ã‚¯ãŒç°¡å˜ãªã®ã§ã€æ€§èƒ½ã®é™ç•Œã‚’è¦‹ã‚Œã¦ã„ã‚‹æ°—ã¯ã—ã¾ã›ã‚“ãŒã€å­¦ç¿’ã¯ã§ãã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚

# ã¾ã¨ã‚

æœ€æ–°ã®ã€ã‚ã£ã¡ã‚ƒæ€§èƒ½ãŒè‰¯ã„éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«Whisperã®Fine Tuningæ–¹æ³•ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚æ­£ç›´ãªã¨ã“ã‚ã€æ­£ç¢ºãªè¨“ç·´æ–¹æ³•ã‹æ€ªã—ã„ã§ã™ãŒã€ä¸€å¿œã€ã§ãã¦ãã†ãªæ°—ãŒã—ã¾ã™ã€‚ã‚‚ã—ã€æ°—ä»˜ã„ãŸç‚¹ãŒã‚ã‚Œã°ã€ã‚³ãƒ¡ãƒ³ãƒˆã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚

----

ç”»åƒãƒ»è‡ªç„¶è¨€èªãƒ»éŸ³å£°ã«é–¢ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ç ”ç©¶é–‹ç™ºã‚„MLOpsã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã€æ©Ÿæ¢°å­¦ç¿’ã«é–¢ã—ã¦ã€ã”ç›¸è«‡ãŒã‚ã‚Œã°ã€[@kwashizzz](https://twitter.com/kwashizzz)ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¾ã§DMã—ã¦ãã ã•ã„ï¼

<iframe class="meety-embed" src="https://meety.net/embed/matches/DtCWdBaoxEoe?service=meety&color=pink"
  style="border: 0; display: block; max-width: 99%; width: 498px; height: 228px; padding: 0px; margin: 0px; position: static; visibility: visible;"></iframe>


ã“ã‚Œã¾ã§ã®ã€[æ©Ÿæ¢°å­¦ç¿’è¨˜äº‹ã®ã¾ã¨ã‚](https://zenn.dev/kwashizzz/articles/my-ml-articles-summary)ã§ã™ã€‚
