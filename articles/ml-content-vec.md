---
title: "【論文読み】ContentVec: 話者情報の切り離しによるSpeech表現の自己教師あり学習の改善" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
publication_name: "fusic"
---

こんにちは！鷲崎([@kwashizzz](https://twitter.com/kwashizzz))です。最近、[so-vits-svc(SoftVC VITS Singing Voice Conversion)](https://github.com/svc-develop-team/so-vits-svc)という、性能の良いVoice Conversion (VC)が流行っている気がします。構造的には、普通のVCとほとんど変わらないと思いますが、4.0で、本記事で紹介するContentVecが使用されていたりと、実用的な工夫がおもしろいです。

そこで、私が開発中のVCにも、ContentVecを取り入れるべく、論文を読んでみたので、その内容を解説します。

[ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers](https://arxiv.org/abs/2204.09224)


# ざっくりまとめ

音声の自己教師学習(SSL)は、大規模な未注釈音声コーパスに対して音声表現ネットワークを学習させ、学習した表現を下流のタスクに適用するものである。SSL学習の下流タスクの大半は、音声の内容情報に主眼を置いているため、最も望ましい音声表現は、話者のバリエーションなどの不要なバリエーションを内容から切り離すことができるものでなければならない。しかし、話者の情報を取り除くことは、容易に内容も失うことになり、後者の損害は前者の利益をはるかに上回るのが普通であるため、話者の切り離しは非常に困難である。本論文では、コンテンツの深刻な損失なしに話者の分離を達成することができる新しいSSL手法を提案する。我々のアプローチはHuBERTフレームワークから採用され、教師（マスクされた予測ラベル）と生徒（学習された表現）の両方を正則化するディスエンタングリング機構を組み込んでいる。我々は、コンテンツに関連した一連の下流タスクで話者分離の利点を評価し、話者分離された表現が一貫して顕著なパフォーマンス上の利点を持つことを観察した。


近年、自己教師学習（SSL）は、注釈付きデータが比較的少ない多くの音声処理問題に対する最先端のソリューションとして浮上している。SSLの基本的な考え方は、大規模な未注釈コーパスを用いて音声表現ネットワークを訓練し、意味のある音声構造や情報を捕捉して引き出すことを目的としています。その結果得られた音声表現は、少数の注釈付きデータを用いて下流タスクの学習に適用される。音声表現はすでに十分に構造化されているため、大規模なデータセットに対する下流タスクのトレーニングの依存性を低減することができます。


音声SSLは驚くほど幅広いタスクで利点を発揮しますが、音声SSLの主な焦点の1つは、音声認識/電話分類、音声コンテンツ生成など、音声の内容を処理するタスクにあります。これらのタスクにとって、最も望ましい音声表現は、音声の内容情報と、話者のバリエーションなどの他の干渉するバリエーションを切り離すことができるものであるべきです。しかし、最も広く使われている既存の音声表現の中で、話者変動の合理的な切り分けを実現できるものはほとんどない。例えば、HUBERT表現（Hsu et al., 2021）は、SUPERBベンチマーク（Yang et al., 2021）で最大81.4％の話者識別精度を達成できる。この観察から、話者のディセンションに適切に対処すれば、コンテンツ関連の音声処理タスクにおけるSSLの性能向上の余地がまだある可能性が示唆されます。

しかし、話者の切り離しが非常に困難であることは広く認識されている。音声表現ネットワークのトレーニング中にテキスト注釈にアクセスできないため、音声表現から話者のバリエーションを取り除く試みは、容易にコンテンツ情報の損失につながる可能性がある（Choi et al.、2021）。ほとんどのコンテンツ関連の下流タスクでは、コンテンツ情報を失うコストは、話者を分離する際の利点をはるかに上回ります。

本論文では、以下の2つの研究課題を調査することを目的とする。第一に、SSLトレーニング中に、大きなコンテンツの損失を伴わずに話者のバリエーションを分離する方法はあるのだろうか？この目的のために、我々はHUBERTトレーニングパラダイムから適応されたSSLフレームワークであるCONTENTVECを提案する。HUBERTの重要なアイデアは、MFCCのような比較的貧弱な音声表現をマスクされた予測タスクの教師ラベルとして使用することで、コンテンツ保存を含む多くの側面で教師よりもはるかに優れた音声表現（生徒と呼ばれることもある）を導き出すことができるというものです。このことは、HUBERTの教師-生徒のフレームワークと話者の分離技術を組み合わせることで、後者によって引き起こされたコンテンツの損失を回復できる可能性があることを示唆しています。

![Hubert](https://storage.googleapis.com/zenn-user-upload/0c61aabacfb0-20230329.png)

[HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447)

そこで、HUBERTに3つの分離機構（教師分離、生徒分離、話者条件付け）を組み込んだCONTENTVECの設計に至った。具体的には、教師におけるdisentanglementとは、教師ラベルから話者情報を削除することである。生徒の分離とは、音声表現に直接話者不変性を強制する正則化ロスを導入することである。話者条件付けとは、話者情報をマスクされた予測タスクに入力することで、音声表現が話者情報をエンコードする必要性をなくすことである。これから示すように、3つのモジュールは、音声表現ネットワーク層を横断する話者情報の流れを形成するために不可欠であり、それにより、コンテンツ情報を維持したまま優れた分離品質を達成することができる。

私たちが探求したい2つ目の研究課題は、以下の通りです： SSL特徴量における話者の分離は、下流のタスクに貢献できるとしたら、どの程度の性能向上をもたらすことができるのか？我々の広範な評価により、話者分離はコンテンツ関連のアプリケーションにおいて、ベースライン音声表現に対して一貫した性能優位性を達成できることが示されました。この論文で得られた知見は、下流タスクにより的を絞った情報を供給し、より強力なコンテンツ処理を音声上で直接行うことができる次世代音声表現に光を当てることができる。

# 手法

HUBERT教師が貧弱（例えば、コンテンツを失う）であっても、マスクされた予測メカニズムのおかげで、生徒は教師よりもはるかに優れたコンテンツを保存できることが報告されています（Hsu et al.、2021）。この観察から、（コンテンツの損失を引き起こす可能性のある）話者の分離技術をマスクされた予測フレームワークと組み合わせることで、話者の分離アルゴリズムを単独で使用するよりもコンテンツをより忠実に保存できるという仮説を検証することを思いつきました。教師、生徒、予測者は、覆面予測の3大要素であるため、CONTENTVECでは、図1に示すように、3つの構成要素にそれぞれ取り組むために、教師におけるディスエンタングルメント、生徒におけるディスエンタングルメント、話者の条件付けという3つのディスエンタングルメント機構を導入しました。

![ContentVec](https://storage.googleapis.com/zenn-user-upload/8e414c36702a-20230329.png)

Teacherでは、一人の話者にVCしている。話者表現の分離。教師用音声表現はすでに話者の離散化を達成しているが、どのような音声変換システムでも、時には（一部の話者に対して）無視できないコンテンツロスを引き起こすため、そのコンテンツ保存は満足できるものではないことは注目に値する（Choi et al., 2021）。現代の音声変換のこの欠点を改善するために、我々は、その出力を下流のタスクに直接適用するのではなく、より良い学生を訓練するための教師として音声変換を使用します。

Studentは、Contrastive-learning-beased algorithmである、SIM-CLRを用いている。


対比的損失を適用する際の最大の課題は、他の側面の変化を最小限に抑えつつ、発話の話者アイデンティティのみを変化させるランダム変換をどのように設計するかということである。このため、Choiら（2021）が提案したランダム変換アルゴリズムを採用する。具体的には、このアルゴリズムは3段階の変換で構成される。まず、発話内のすべてのフォルマント周波数をρ1の係数でスケーリングし、次に、すべてのフレームのF0をρ2の係数でスケーリングし、最後に、チャネル効果に対応するためにランダムイコライザーを適用する。 ρ1およびρ2はいずれも一様分布U（[1, 1.4] ）からランダムに引き、確率 0.5 でそれらの逆数に反転される。音声情報の大部分はフォルマント周波数とF0周波数範囲に存在し（例えば、（Eide & Gish, 1996））、コンテンツ情報は相対フォルマント周波数比に存在する（Stevens, 1987）ので、すべてのフォルマントとF0を一様にスケーリングすると、コンテンツを保持しながら話者情報を変更する傾向があります。

[contentvec/data/audio/contentvec_dataset.py](https://github.com/auspicious3000/contentvec/blob/main/contentvec/data/audio/contentvec_dataset.py)

教師の分離は、教師ラベルから話者情報の大部分を削除することができますが、特定の話者情報は残ります。その結果、教師ラベルを合理的に予測するために、生徒の表現も教師と同じ量の話者情報を持たざるを得ないという望ましくない事態が発生する。生徒の話者情報と教師の話者情報の間のこの連関を解消するために、我々は話者埋め込みを予測器に供給する。話者埋め込みは、話者埋め込みネットワーク、我々の場合は事前に訓練されたGE2E（Wan et al.、2018）によって生成され、音声発話を入力として受け取り、発話中の話者情報を要約したベクトルを出力する。したがって、予測器を話者埋め込みに条件付けることで、マスク予測タスクに必要な話者情報を何でも供給することができ、学生が自ら話者情報を持ち運ぶ必要がないようにすることができます。

このように、CONTENTVECは話者情報を識別するために話者ラベルを必要としますが、話者ラベルは話者埋め込みネットワークの事前学習にのみ使用されます。CONTENTVEC自体の学習には、話者ラベルではなく、話者埋め込みのみが必要である。話者埋め込みネットワークは別のデータセットで事前学習され、未見の話者にもうまく汎化できるため、CONTENTVECの学習セットには話者ラベルが必要ない。

図2は、音声表現ネットワークf(-)と予測器p(-)の各レイヤーにおいて、話者情報量がどのように変化するかを示す概念曲線である。縦軸は話者情報量、横軸は層数を表す。白い部分が音声表現ネットワークの層で、灰色の部分が音声表現ネットワークの上にある予測層を表しています。左側は、話者情報が入力音声に含まれる完全な話者情報と同じである。右側では、話者情報は教師ラベルの話者情報とほぼ等しくなるはずで、入力の話者情報よりはるかに低いが、まだゼロではない。情報処理の不等式により、話者情報が再投入される予測層を除き、層が進むにつれて話者情報は単調に減少していきます。

このように、話者情報が急激に変化する場所が2箇所あることがわかる。1つ目は、コントラスト損失（式(2)）が課され、話者情報が大きく減少している場所である。もう一つは、話者情報が再投入され、話者情報がわずかに増加する箇所である。その結果、話者情報は音声表現ネットワークと予測器の交点で最小となるはずです。図2は、CONTENTVECのすべてのモジュールが、話者切り離しを成功させるために不可欠であることを示しています。