---
title: "【論文読み】ContentVec: 話者情報の切り離しによるSpeech表現の自己教師あり学習の改善" # 記事のタイトル
emoji: "😸" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["機械学習"] # タグ。["markdown", "rust", "aws"]のように指定する
published: false # 公開設定（falseにすると下書き）
publication_name: "fusic"
---

こんにちは！鷲崎([@kwashizzz](https://twitter.com/kwashizzz))です。最近、[so-vits-svc(SoftVC VITS Singing Voice Conversion)](https://github.com/svc-develop-team/so-vits-svc)という、性能の良いVoice Conversion (VC)が流行っている気がします。構造的には、普通のVCとほとんど変わらないと思いますが、4.0で、本記事で紹介するContentVecが使用されていたりと、実用的な工夫がおもしろいです。

そこで、私が開発中のVCにも、ContentVecを取り入れるべく、論文を読んでみたので、その内容を解説します。

[ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers](https://arxiv.org/abs/2204.09224)


# ざっくりまとめ

音声の自己教師学習(SSL)は、大規模な未注釈音声コーパスに対して音声表現ネットワークを学習させ、学習した表現を下流のタスクに適用するものである。SSL学習の下流タスクの大半は、音声の内容情報に主眼を置いているため、最も望ましい音声表現は、話者のバリエーションなどの不要なバリエーションを内容から切り離すことができるものでなければならない。しかし、話者の情報を取り除くことは、容易に内容も失うことになり、後者の損害は前者の利益をはるかに上回るのが普通であるため、話者の切り離しは非常に困難である。本論文では、コンテンツの深刻な損失なしに話者の分離を達成することができる新しいSSL手法を提案する。我々のアプローチはHuBERTフレームワークから採用され、教師（マスクされた予測ラベル）と生徒（学習された表現）の両方を正則化するディスエンタングリング機構を組み込んでいる。我々は、コンテンツに関連した一連の下流タスクで話者分離の利点を評価し、話者分離された表現が一貫して顕著なパフォーマンス上の利点を持つことを観察した。


近年、自己教師学習（SSL）は、注釈付きデータが比較的少ない多くの音声処理問題に対する最先端のソリューションとして浮上している。SSLの基本的な考え方は、大規模な未注釈コーパスを用いて音声表現ネットワークを訓練し、意味のある音声構造や情報を捕捉して引き出すことを目的としています。その結果得られた音声表現は、少数の注釈付きデータを用いて下流タスクの学習に適用される。音声表現はすでに十分に構造化されているため、大規模なデータセットに対する下流タスクのトレーニングの依存性を低減することができます。


音声SSLは驚くほど幅広いタスクで利点を発揮しますが、音声SSLの主な焦点の1つは、音声認識/電話分類、音声コンテンツ生成など、音声の内容を処理するタスクにあります。これらのタスクにとって、最も望ましい音声表現は、音声の内容情報と、話者のバリエーションなどの他の干渉するバリエーションを切り離すことができるものであるべきです。しかし、最も広く使われている既存の音声表現の中で、話者変動の合理的な切り分けを実現できるものはほとんどない。例えば、HUBERT表現（Hsu et al., 2021）は、SUPERBベンチマーク（Yang et al., 2021）で最大81.4％の話者識別精度を達成できる。この観察から、話者のディセンションに適切に対処すれば、コンテンツ関連の音声処理タスクにおけるSSLの性能向上の余地がまだある可能性が示唆されます。

しかし、話者の切り離しが非常に困難であることは広く認識されている。音声表現ネットワークのトレーニング中にテキスト注釈にアクセスできないため、音声表現から話者のバリエーションを取り除く試みは、容易にコンテンツ情報の損失につながる可能性がある（Choi et al.、2021）。ほとんどのコンテンツ関連の下流タスクでは、コンテンツ情報を失うコストは、話者を分離する際の利点をはるかに上回ります。

本論文では、以下の2つの研究課題を調査することを目的とする。第一に、SSLトレーニング中に、大きなコンテンツの損失を伴わずに話者のバリエーションを分離する方法はあるのだろうか？この目的のために、我々はHUBERTトレーニングパラダイムから適応されたSSLフレームワークであるCONTENTVECを提案する。HUBERTの重要なアイデアは、MFCCのような比較的貧弱な音声表現をマスクされた予測タスクの教師ラベルとして使用することで、コンテンツ保存を含む多くの側面で教師よりもはるかに優れた音声表現（生徒と呼ばれることもある）を導き出すことができるというものです。このことは、HUBERTの教師-生徒のフレームワークと話者の分離技術を組み合わせることで、後者によって引き起こされたコンテンツの損失を回復できる可能性があることを示唆しています。

![Hubert](https://storage.googleapis.com/zenn-user-upload/0c61aabacfb0-20230329.png)

[HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447)

そこで、HUBERTに3つの分離機構（教師分離、生徒分離、話者条件付け）を組み込んだCONTENTVECの設計に至った。具体的には、教師におけるdisentanglementとは、教師ラベルから話者情報を削除することである。生徒の分離とは、音声表現に直接話者不変性を強制する正則化ロスを導入することである。話者条件付けとは、話者情報をマスクされた予測タスクに入力することで、音声表現が話者情報をエンコードする必要性をなくすことである。これから示すように、3つのモジュールは、音声表現ネットワーク層を横断する話者情報の流れを形成するために不可欠であり、それにより、コンテンツ情報を維持したまま優れた分離品質を達成することができる。

私たちが探求したい2つ目の研究課題は、以下の通りです： SSL特徴量における話者の分離は、下流のタスクに貢献できるとしたら、どの程度の性能向上をもたらすことができるのか？我々の広範な評価により、話者分離はコンテンツ関連のアプリケーションにおいて、ベースライン音声表現に対して一貫した性能優位性を達成できることが示されました。この論文で得られた知見は、下流タスクにより的を絞った情報を供給し、より強力なコンテンツ処理を音声上で直接行うことができる次世代音声表現に光を当てることができる。

# 手法

HUBERT教師が貧弱（例えば、コンテンツを失う）であっても、マスクされた予測メカニズムのおかげで、生徒は教師よりもはるかに優れたコンテンツを保存できることが報告されています（Hsu et al.、2021）。この観察から、（コンテンツの損失を引き起こす可能性のある）話者の分離技術をマスクされた予測フレームワークと組み合わせることで、話者の分離アルゴリズムを単独で使用するよりもコンテンツをより忠実に保存できるという仮説を検証することを思いつきました。教師、生徒、予測者は、覆面予測の3大要素であるため、CONTENTVECでは、図1に示すように、3つの構成要素にそれぞれ取り組むために、教師におけるディスエンタングルメント、生徒におけるディスエンタングルメント、話者の条件付けという3つのディスエンタングルメント機構を導入しました。

![ContentVec](https://storage.googleapis.com/zenn-user-upload/8e414c36702a-20230329.png)

Teacherでは、一人の話者にVCしている。話者表現の分離。教師用音声表現はすでに話者の離散化を達成しているが、どのような音声変換システムでも、時には（一部の話者に対して）無視できないコンテンツロスを引き起こすため、そのコンテンツ保存は満足できるものではないことは注目に値する（Choi et al., 2021）。現代の音声変換のこの欠点を改善するために、我々は、その出力を下流のタスクに直接適用するのではなく、より良い学生を訓練するための教師として音声変換を使用します。

Studentは、Contrastive-learning-beased algorithmである、SIM-CLRを用いている。


対比的損失を適用する際の最大の課題は、他の側面の変化を最小限に抑えつつ、発話の話者アイデンティティのみを変化させるランダム変換をどのように設計するかということである。このため、Choiら（2021）が提案したランダム変換アルゴリズムを採用する。具体的には、このアルゴリズムは3段階の変換で構成される。まず、発話内のすべてのフォルマント周波数をρ1の係数でスケーリングし、次に、すべてのフレームのF0をρ2の係数でスケーリングし、最後に、チャネル効果に対応するためにランダムイコライザーを適用する。 ρ1およびρ2はいずれも一様分布U（[1, 1.4] ）からランダムに引き、確率 0.5 でそれらの逆数に反転される。音声情報の大部分はフォルマント周波数とF0周波数範囲に存在し（例えば、（Eide & Gish, 1996））、コンテンツ情報は相対フォルマント周波数比に存在する（Stevens, 1987）ので、すべてのフォルマントとF0を一様にスケーリングすると、コンテンツを保持しながら話者情報を変更する傾向があります。

[contentvec/data/audio/contentvec_dataset.py](https://github.com/auspicious3000/contentvec/blob/main/contentvec/data/audio/contentvec_dataset.py)

教師の分離は、教師ラベルから話者情報の大部分を削除することができますが、特定の話者情報は残ります。その結果、教師ラベルを合理的に予測するために、生徒の表現も教師と同じ量の話者情報を持たざるを得ないという望ましくない事態が発生する。生徒の話者情報と教師の話者情報の間のこの連関を解消するために、我々は話者埋め込みを予測器に供給する。話者埋め込みは、話者埋め込みネットワーク、我々の場合は事前に訓練されたGE2E（Wan et al.、2018）によって生成され、音声発話を入力として受け取り、発話中の話者情報を要約したベクトルを出力する。したがって、予測器を話者埋め込みに条件付けることで、マスク予測タスクに必要な話者情報を何でも供給することができ、学生が自ら話者情報を持ち運ぶ必要がないようにすることができます。

このように、CONTENTVECは話者情報を識別するために話者ラベルを必要としますが、話者ラベルは話者埋め込みネットワークの事前学習にのみ使用されます。CONTENTVEC自体の学習には、話者ラベルではなく、話者埋め込みのみが必要である。話者埋め込みネットワークは別のデータセットで事前学習され、未見の話者にもうまく汎化できるため、CONTENTVECの学習セットには話者ラベルが必要ない。

図2は、音声表現ネットワークf(-)と予測器p(-)の各レイヤーにおいて、話者情報量がどのように変化するかを示す概念曲線である。縦軸は話者情報量、横軸は層数を表す。白い部分が音声表現ネットワークの層で、灰色の部分が音声表現ネットワークの上にある予測層を表しています。左側は、話者情報が入力音声に含まれる完全な話者情報と同じである。右側では、話者情報は教師ラベルの話者情報とほぼ等しくなるはずで、入力の話者情報よりはるかに低いが、まだゼロではない。情報処理の不等式により、話者情報が再投入される予測層を除き、層が進むにつれて話者情報は単調に減少していきます。

このように、話者情報が急激に変化する場所が2箇所あることがわかる。1つ目は、コントラスト損失（式(2)）が課され、話者情報が大きく減少している場所である。もう一つは、話者情報が再投入され、話者情報がわずかに増加する箇所である。その結果、話者情報は音声表現ネットワークと予測器の交点で最小となるはずです。図2は、CONTENTVECのすべてのモジュールが、話者切り離しを成功させるために不可欠であることを示しています。

# 実験

話者識別は、話者分離の代理として機能する。さらに、我々は、話者分離アルゴリズムが、地域のアクセントは主に音素の内容によって伝達されるのか、それとも話者依存のスタイルによって伝達されるのか、という点にも関心をもっています。そこで、SUPERBベンチマークの話者識別タスク（SID）と、7つのアクセントグループを含むL2-ARCTICデータセットを用いたアクセント分類タスクで、話者分離の品質を評価した。 

図3は、2つの分類タスクの精度をプロットしたものである。このように、CONTENTVECは両タスクにおいて精度を大幅に低下させていることがわかる。SIDタスクでは、HUBERT-ITERと比較して36%も低下しており、CONTENTVECの話者切り離しメカニズムが非常に有効であることを示しています。HUBERT-ITERはHUBERTと比較してSIDの精度が若干低く、反復学習により話者情報量が減少することがわかる。アクセント分類タスクにおいても、その減少幅は大きく、話者情報のディスエンタングルによって、アクセント情報もある程度減少することが検証された。

既存の主流の音声変換システムは、エンコーダとデコーダのパラダイムに従っており、エンコーダは話者情報が分離された音声表現を導き出し、デコーダは話者の埋め込み/ラベルを条件として音声信号を合成する。Polyakら(2021)はさらに、自己教師付き音声表現の上に直接構築されたデコーダが、最先端の音声変換結果を生み出すのに十分であることを示す。CONTENTVECの話者分離の利点がより良い音声変換につながるかどうかを評価するために、Polyakら（2021）の音声変換モデルをCONTENTVECと他のベースラインの音声表現で実行し、変換された音声のターゲット話者との話者類似度を評価します。 Polyakら（2021）のように離散化された音声表現を使用する代わりに、連続音声表現を使用する、より困難な設定を考慮する。モデルはLibrispeechで学習・評価される。テストセットには、トレーニングセットと同じスピーカーが含まれている。評価の複雑さを軽減するために、ソーススピーカーはテストセット全体から、ターゲットスピーカーは20人のサブセットから、10人は「クリーン」サブセットから、10人は「その他」サブセットからである。このように、我々の評価は4つのシナリオに分けられる。クリーンからクリーン（C2C）、クリーンからその他（C2O）、その他からクリーン（O2C）、その他からその他（O2O）である。

表3は、変換された音声とターゲットスピーカーの間のdベクトルの平均コサイン類似度（Heigold et al.、2016）を示す。観察されるように、ニューラルデコーダの固有の話者切り離し能力のおかげで、HUBERTベースの音声変換モデルはすでにまともな話者類似度を有しており、これはPolyakら（2021）による発見と一致する。しかし、CONTENTVECはさらに性能を大きく前進させることができ、CONTENTVECの話者離散化品質の優位性をさらに検証しています。また、セクション4.5に従って、わずかに優れた話者分離特性を持つHUBERT-ITERも、このタスクにおける話者の類似性の点でHUBERTよりも向上することが確認された。

## Contribution of Each Disentanglement Module

3つの主要な分離機構（教師における分離、生徒における分離、予測因子の話者条件付け）が全体の性能にどれだけ寄与しているかを測定するために、それぞれの機構を取り除いたCONTENTVECの亜種を作り、それぞれNO-DTEACHERS, NO-DSTUDENTS, NO-SONDと名付けた。具体的には、NO-DTEACHERSは音声変換モジュールを導入しないことを意味し、NO-DSTUDENTSは学生モジュールで変換やコントラスト損失を課さないことを意味し、NO-CONDは予測器に話者埋め込みを与えないことを意味する。それぞれの性能は表4に報告されている。見てわかるように、3つのモデルはすべて、CONTENTVECより大幅に性能が悪い。3つのモジュールすべてがCONTENTVECに不可欠であると結論づけることができる。

## Speaker Information Flow 

図2の概念曲線が正しいかどうかを検証するために、CONTENTVECの各レイヤーでSID精度（話者情報の代理）を評価します。各実験において、10k回の繰り返しごとに、検証精度に基づいて最適なSIDモデルを選択し、統計的有意性を測るために、これらすべてのSIDモデルによって達成された精度を計算する。図4(a)は、ネットワーク層の関数としてSID精度をプロットしたものです。図2と図4(a)を比較すると、我々の主要な仮説が検証されたことがわかります。まず、SIDは第9層以前で単調に減少し、対照的な損失が課される第7層以前で有意に減少している3。この観察は、図2の急減と一致する。第二に、最後の層に向かってわずかな増加があり、これは図2の増加と一致するが、予想よりも低いレベルで起こっている。これは、我々のSIDが離散的な表現に基づいているのに対し、概念曲線が連続的な表現に基づいているためであると考えられる。しかしながら、これらの観察結果は、我々のモデル設計の根拠を裏付けるものである。

## Position of Contrastive Loss

コントラストロスの位置が話者情報の流れにどのような影響を与えるかを説明するために、CONTENTVECの3つのバリエーション（CONTENTVEC-N1, CONTENTVEC-N4, CONTENTVECN8と表記）を実行した。「N1"、"N4"、"N8 "は、それぞれ最後、最後から3番目、最後から7番目の層で対照損失を課すことを意味する（標準のCONTENTVECの最後から5番目ではなく、最後から7番目）。図4は、これらのバリエーションについて、層に対するSID精度を比較したものである。これらの曲線は、すべて同じように減少し、その後増加する傾向を観察することができます。さらに重要なことは、SID精度の低下位置とコントラストロスが課されるレイヤーとの間に興味深い相関関係があることです。CONTENTVEC-N8が最も早く低下し、CONTENTVEC-N8、CONTENTVECN1の順で低下しています。また、遅くドロップを開始したモデルは、より低いポイントにドロップすることが観察される。また、これらのモデルバリアントの性能を前述のメトリクスに関して評価し、その結果を表4に示します。観察されるように、すべてのバリエーションは非常に競争力のある結果を達成し、すべてのベースラインを上回った。これらの結果は、CONTENTVECの競争力が、このハイパーパラメータの特定の選択に依存しないことを示しています。

## Contrastive Loss Weight

また、異なるコントラスト損失の重み（式(5)のλ）を評価した。我々の標準的なCONTENTVECが1e-5の重み係数を使用していることを思い出してください。次の4つの重み係数、1e-6、5e-6、2e-5、5e-5をテストし、その結果を表4に示す。CONTENTVECは、すべてのモデルで競争力のある性能を発揮し、ハイパーパラメータ感度が低いことを確認することができます。

# Conclusion

本論文では、コンテンツ情報の損失を防ぎながら話者情報を除去することを目的とした音声表現学習ネットワークであるCONTENTVECを提案する。CONTENTVECはHUBERTフレームワークをベースに、3つの重要なディセンタングルメントコンポーネントを導入している：教師におけるディセンタングルメント、生徒におけるディセンタングルメント、予測因子の教師条件付け。我々の実証分析では、3つのモジュールすべてがCONTENTVECの成功に不可欠であることを確認しました。また、話者のディセンションが成功すれば、コンテンツに関連した幅広い音声処理タスクに役立つことも確認した。一方、CONTENTVECには、わずかなコンテンツロスやハイパーパラメータの選択方法の欠如など、まだいくつかの制限があります。これらの問題に対する解決策を見つけることが、私たちの将来の方向性です。